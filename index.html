<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Asuka的个人笔记</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Asuka的个人笔记">
<meta property="og:url" content="https://nfsp412.github.io/asuka.github.io/index.html">
<meta property="og:site_name" content="Asuka的个人笔记">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="asuka">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/asuka.github.io/atom.xml" title="Asuka的个人笔记" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/asuka.github.io/favicon.png">
  
  
  
<link rel="stylesheet" href="/asuka.github.io/css/style.css">

  
    
<link rel="stylesheet" href="/asuka.github.io/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/asuka.github.io/" id="logo">Asuka的个人笔记</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/asuka.github.io/">Home</a>
        
          <a class="main-nav-link" href="/asuka.github.io/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/asuka.github.io/atom.xml" title="RSS 订阅"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="搜索"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://nfsp412.github.io/asuka.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-bigdata005" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/asuka.github.io/2024/10/24/bigdata005/" class="article-date">
  <time class="dt-published" datetime="2024-10-24T03:24:38.000Z" itemprop="datePublished">2024-10-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/asuka.github.io/2024/10/24/bigdata005/">Kafka学习笔记</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>本文是自己在学习Kafka过程中整理的一些基础笔记,仅做记录</p>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><h5 id="消息队列是什么"><a href="#消息队列是什么" class="headerlink" title="消息队列是什么"></a>消息队列是什么</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">基于文件或者数据库都是离线计算,但是实际上可能走消息队列更多</span><br><span class="line">所以一般结合消息队列加flume实时读取加spark或者flink</span><br><span class="line"></span><br><span class="line">功能解耦合,限流削峰,异步处理,消息驱动(即来了消息才驱动后续代码功能执行)</span><br><span class="line"></span><br><span class="line">消息传递的两种模式</span><br><span class="line">点对点</span><br><span class="line">发布订阅</span><br></pre></td></tr></table></figure>

<h5 id="Kafka是什么"><a href="#Kafka是什么" class="headerlink" title="Kafka是什么"></a>Kafka是什么</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Kafka是一个分布式的基于发布/订阅模式的消息队列（MessageQueue），主要应用于大数据实时处理领域</span><br><span class="line">Kafka是 一个开源的 分 布式事件流平台 （Event StreamingPlatform），被数千家公司用于高性能数据管道、流分析、数据集成和关键任务应用</span><br></pre></td></tr></table></figure>

<h5 id="发布订阅的含义"><a href="#发布订阅的含义" class="headerlink" title="发布订阅的含义"></a>发布订阅的含义</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">发布/订阅：消息的发布者不会将消息直接发送给特定的订阅者，而是将发布的消息分为不同的类别，订阅者只接收感兴趣的消息</span><br></pre></td></tr></table></figure>

<h5 id="消息队列产品有哪些"><a href="#消息队列产品有哪些" class="headerlink" title="消息队列产品有哪些"></a>消息队列产品有哪些</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">目前企业中比较常见的消息队列产品主要有 Kafka、ActiveMQ 、RabbitMQ 、RocketMQ 等</span><br><span class="line">在大数据场景主要采用 Kafka 作为消息队列。在 JavaEE 开发中主要采用 ActiveMQ、RabbitMQ、RocketMQ</span><br></pre></td></tr></table></figure>

<h5 id="消息队列的应用场景有哪些"><a href="#消息队列的应用场景有哪些" class="headerlink" title="消息队列的应用场景有哪些"></a>消息队列的应用场景有哪些</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">缓存/消峰、解耦和异步通信。</span><br><span class="line">缓冲/消峰：有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况</span><br><span class="line">解耦：允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束</span><br><span class="line">异步通信：允许用户把一个消息放入队列，但并不立即处理它，然后在需要的时候再去处理它们</span><br></pre></td></tr></table></figure>

<h5 id="消息队列有哪些模式"><a href="#消息队列有哪些模式" class="headerlink" title="消息队列有哪些模式"></a>消息队列有哪些模式</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">点对点模式 消费者主动拉取数据，消息收到后清除消息</span><br><span class="line">发布/订阅模式 可以有多个topic主题（浏览、点赞、收藏、评论等） 消费者消费数据之后，不删除数据 每个消费者相互独立，都可以消费到数据</span><br><span class="line"></span><br><span class="line">主题的数量不同</span><br><span class="line">消费以后是否删除数据</span><br><span class="line">消费者的数量</span><br></pre></td></tr></table></figure>

<h1 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h1><h5 id="Kafka的重要组件有哪些"><a href="#Kafka的重要组件有哪些" class="headerlink" title="Kafka的重要组件有哪些"></a>Kafka的重要组件有哪些</h5><p>broker,partition,消费者组,replication&lt;&#x3D;broker-1,leader副本,follower副本,master broker就是controller,</p>
<p>生产者,消费者,broker三者的网络客户端client,broker额外包含服务端server</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">避免IO热点问题,分布式扩展,所以有多个broker</span><br><span class="line">同时topic也分布式扩展到多个broker上面</span><br><span class="line">所以产生了将一个topic切分成为多个partition</span><br><span class="line">所以生产者可以指定partition消费者也可以指定partition</span><br><span class="line">产生的消费者组,让多个消费者在一个组内,那么组内每个消费者可以只消费指定某个partition即可,整个组构成了完整整体去消费一个topic</span><br><span class="line">但是某个broker宕机,消费这个topic就不完全,所以产生了备份,并且备份放在其他broker上面去</span><br><span class="line">follower备份的数量应该&lt;=broker数量-1,即三个broker,备份2份</span><br><span class="line">这里备份称之为副本replication,</span><br><span class="line">副本只能有一个提供读取操作,其他的是用来备份的</span><br><span class="line">leader副本有读写能力,follower副本只用来备份</span><br><span class="line">为了监控broker的运行情况,选择某一个broker作为master(zk选举机制),也就是controller</span><br><span class="line">为了HA高可用,任何一个节点都可以作为standby备份的controller</span><br><span class="line">生产者,消费者,broker,都包含网络客户端,而broker还有socket server网络服务器,用来接收生产者和消费者的连接请求</span><br><span class="line">broker中还有replication manager,用来管理副本的,还有log manager用来管理日志,还有zk client用来访问zk的,还有事务协调器TransactionCoordinator,用来管理事务</span><br><span class="line"></span><br><span class="line">producer生产者,java语言编写</span><br><span class="line">consumer消费者,java语言编写</span><br><span class="line">topic主题 存储数据的地方</span><br><span class="line">partition分区 一个主题分为多个分区存储,负载均衡,提高并行度</span><br><span class="line">broker 代表服务器节点名称,scala语言编写</span><br><span class="line">消费者组 每一个消费者负责一个分区的数据的消费,共同组成消费者组</span><br><span class="line">	注意一个分区的数据只能由一个消费者进行消费</span><br><span class="line">HA 每个分区增加副本冗余存储,保证安全</span><br><span class="line">	和hdfs副本机制不同,Kafka存在leader和follower,消费者只能消费leader</span><br><span class="line">	当leader挂掉以后,follower可能成为leader</span><br><span class="line">zk 存储Kafka的服务器的存货情况,存储leader的信息</span><br><span class="line">	2.8.0版本以后可以不使用zk</span><br></pre></td></tr></table></figure>

<h5 id="命令行有哪些"><a href="#命令行有哪些" class="headerlink" title="命令行有哪些"></a>命令行有哪些</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">启动:</span><br><span class="line">bin/kafka-server-start.sh -daemon config/server.properties</span><br><span class="line">停止:</span><br><span class="line">bin/kafka-server-stop.sh</span><br><span class="line">创建topic:</span><br><span class="line">bin/kafka-topics.sh --bootstrap-server ip:port --topic first --create --partitions 1 --replication-factor 3</span><br><span class="line">删除topic:</span><br><span class="line">bin/kafka-topics.sh --bootstrap-server ip:port --topic first --delete</span><br><span class="line">查看topic详细信息:</span><br><span class="line">bin/kafka-topics.sh --bootstrap-server ip:port --topic first --describe</span><br><span class="line">修改分区数量:只能增大分区</span><br><span class="line">bin/kafka-topics.sh --bootstrap-server ip:port --topic first --alter --partitions 3</span><br><span class="line">查询所有主题:</span><br><span class="line">bin/kafka-topics.sh --bootstrap-server ip:port --list</span><br><span class="line"></span><br><span class="line">bin/kafka-topics.sh </span><br><span class="line">	--bootstrap-server hadoop101:9092,hadoop102:9092,hadoop103:9092</span><br><span class="line">		测试环境连接一个即可,生产环境都连接</span><br><span class="line">	--topic</span><br><span class="line">	--partitions 指定分区</span><br><span class="line">	--replication-factor 指定副本数</span><br><span class="line">	--create</span><br><span class="line">	--delete</span><br><span class="line">	--alter</span><br><span class="line">	--describe</span><br><span class="line">	--list</span><br><span class="line"></span><br><span class="line">生产者生产数据:</span><br><span class="line">bin/kafka-console-producer.sh --bootstrap-server ip:port --topic first</span><br><span class="line">消费者消费数据:</span><br><span class="line">bin/kafka-console-consumer.sh --bootstrap-server ip:port --topic first</span><br><span class="line">历史数据一次性消费:</span><br><span class="line">bin/kafka-console-consumer.sh --bootstrap-server ip:port --topic first --from-beginning</span><br></pre></td></tr></table></figure>

<h5 id="生产者发送原理是"><a href="#生产者发送原理是" class="headerlink" title="生产者发送原理是"></a>生产者发送原理是</h5><p>main-send-kv的序列化-分区器进行数据分区-缓冲区-sender线程发送数据到broker集群-反向ack校验</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">简单来说:</span><br><span class="line">main线程</span><br><span class="line">send方法发送数据</span><br><span class="line">拦截器列表</span><br><span class="line">key序列化器</span><br><span class="line">value序列化器</span><br><span class="line">分区器</span><br><span class="line">元数据缓存MetadataCache</span><br><span class="line">数据缓冲区RecordAccumulat</span><br><span class="line">Sender发送数据线程</span><br><span class="line">网络客户端,在途请求缓冲区,默认5</span><br><span class="line"></span><br><span class="line">结合源码来说:</span><br><span class="line">生产者KafkaProducer类调用send方法发送数据</span><br><span class="line">数据是封装为ProducerRecord</span><br><span class="line">底层先调用拦截器ProducerInterceptors对象的onSend方法</span><br><span class="line">	onSend方法里面是将拦截器这个列表循环遍历,去执行每一个拦截器ProducerInterceptor接口的onSend方法的具体实现</span><br><span class="line">最终将一个ProducerRecord转换成另一个ProducerRecord</span><br><span class="line">然后再调用doSend发送</span><br><span class="line">该方法里面会使用key序列化器和value序列化器进行序列化,必须传入,否则这里直接抛出异常了</span><br><span class="line">然后执行分区器的方法partition,返回int类型分区编号</span><br><span class="line">	先判断是否在ProducerRecord中设置了分区编号,设置了直接使用,不会做任何正确性判断</span><br><span class="line">	再判断是否有自定义分区器</span><br><span class="line">	再判断key不为空而且不忽略key即参数partitioner.ignore,keys默认false,就根据key进行分区</span><br><span class="line">	再判断key为空,最终会根据分区负载情况粘性分区,即尽可能选择同一个分区,阈值batch.size=16kb</span><br><span class="line">数据校验</span><br><span class="line">	max.request.size=1mb</span><br><span class="line">	buffer.memory=32mb</span><br><span class="line">数据缓冲区,即参数buffer.memory=32mb对应的数据缓冲区</span><br><span class="line">	当batch.size满了或者是开启了一个新的batch,就唤醒了sender线程</span><br><span class="line">以broker的id作为key,以拉取数据的请求request作为value,构建一个拉取数据的队列;这个队列最多默认5个请求</span><br><span class="line"></span><br><span class="line">主线程main</span><br><span class="line">send方法,发送数据</span><br><span class="line">拦截器</span><br><span class="line">序列化器,自己实现的序列化方式</span><br><span class="line">分区器,进行数据分区</span><br><span class="line">	一个分区会创建一个队列</span><br><span class="line">	在内存中完成,队列大小32MB</span><br><span class="line">	传输数据的批次大小16KB</span><br><span class="line">sender线程,从队列读取数据发往kafka集群</span><br><span class="line">	batch.size 满16KB一批次,sender就拉取一次</span><br><span class="line">	linger.ms 默认0毫秒,即不等待16KB批次参数</span><br><span class="line">	以broker的id作为key,以拉取数据的请求request作为value,构建一个拉取数据的队列</span><br><span class="line">	这个队列最多默认5个请求</span><br><span class="line">selector作为链路打通,还要接收kafka集群接收数据的ack应答</span><br><span class="line">	0 生产者发送的数据,不需要等待数据落盘</span><br><span class="line">	1 生产者发送的数据,需要leader收到数据后应答</span><br><span class="line">	-1 生产者发送的数据,需要leader和isr队列所有节点收齐全数据后应答</span><br><span class="line">反馈成功:清理请求队列的请求,清理队列中数据</span><br><span class="line">反馈失败:默认一直重试</span><br></pre></td></tr></table></figure>

<h5 id="分区器源码"><a href="#分区器源码" class="headerlink" title="分区器源码"></a>分区器源码</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">执行分区器的方法partition,返回int类型分区编号</span><br><span class="line">	先判断是否在ProducerRecord中设置了分区编号,设置了直接使用,不会做任何正确性判断</span><br><span class="line">	再判断是否有自定义分区器</span><br><span class="line">	再判断key不为空而且不忽略key即参数partitioner.ignore,keys默认false,就根据key进行分区</span><br><span class="line">	再判断key为空,最终会根据分区负载情况粘性分区,即尽可能选择同一个分区,阈值batch.size=16kb</span><br><span class="line">	</span><br><span class="line">ProducerRecord类,使用不同的构造器</span><br><span class="line">设置分区就发送到对应分区</span><br><span class="line">没设置分区,如果有key,就按照key的哈希code值和分区数取余</span><br><span class="line">如果也没有key,使用粘性分区,指的是随机选择分区后,尽量一直使用这个分区,等batch.size满,再更换分区</span><br><span class="line"></span><br><span class="line">DefaultPartitioner类</span><br><span class="line"></span><br><span class="line">生产中可以把topic名字即表名字设置为key,所有表的数据可以进入相同分区</span><br></pre></td></tr></table></figure>

<h5 id="zk中存储了哪些信息"><a href="#zk中存储了哪些信息" class="headerlink" title="zk中存储了哪些信息"></a>zk中存储了哪些信息</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">有brokers节点,包含了id,topic等</span><br><span class="line">还有cluster节点,包含controller等</span><br><span class="line"></span><br><span class="line">例如</span><br><span class="line">/brokers/ids</span><br><span class="line">/brokers/topics</span><br><span class="line">/controller</span><br><span class="line"></span><br><span class="line">ls / 查看节点信息</span><br><span class="line">ls /kafka 查看kafka信息</span><br><span class="line"></span><br><span class="line">ls /kafka/brokers/ids 存储了节点信息</span><br><span class="line">ls /kafka/brokers/topics/主题名称/partitions/分区编号/state 查看主题分区下的leader和ISR</span><br><span class="line">offset保存在kafka集群中自己的主题中,不再保存在zk,避免网络传输</span><br><span class="line">ls /kafka/controller 注册该节点,选举leader</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="异步发送"><a href="#异步发送" class="headerlink" title="异步发送"></a>异步发送</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">指的是数据发送到ProducerAccumulator中,无需等确认发送到kafka broker 集群就返回</span><br><span class="line">代码中使用send就是异步发送,还能够使用callback回调</span><br></pre></td></tr></table></figure>

<h5 id="选举流程"><a href="#选举流程" class="headerlink" title="选举流程"></a>选举流程</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">简单来说</span><br><span class="line">所有的brokers启动后,去zk抢注临时节点,抢注成功的成为controller节点</span><br><span class="line">其他剩余brokers设置监听</span><br><span class="line">当controller宕机,节点消失,剩余brokers继续抢注</span><br><span class="line"></span><br><span class="line">详细来说</span><br><span class="line">broker创建的时候会做几件事情</span><br><span class="line">1 创建/brokers/ids/xxx</span><br><span class="line">2 监听/controller节点</span><br><span class="line">3 创建/controller节点</span><br><span class="line">4 创建成功就选举成功成为controller,监听/brokers/ids/的变化,来判断其余brokers的增减</span><br><span class="line">创建失败原因(zk无法创建相同节点)</span><br><span class="line">成为controller后</span><br><span class="line">1 controller由于监听了/brokers/ids/的变化,会收到通知brokers发生了变化</span><br><span class="line">2 controller会连接所有的集群broker,传递消息包含集群的topic,controller的id等内容</span><br><span class="line">如果controller发生宕机</span><br><span class="line">1 controller发生宕机时,所有brokers都会收到通知,是因为broker都会监听/controller</span><br><span class="line">2 所有brokers抢注/controller</span><br><span class="line">3 创建成功的成为controller,增加监听/brokers/ids/的变化</span><br><span class="line">4 此时会发生一次集群内容同步,controller会连接所有的集群broker,传递消息包含集群的topic,controller的id等内容</span><br><span class="line"></span><br><span class="line">broker启动以后,在zk注册启动成功,在/kafka/brokers/ids记录信息</span><br><span class="line">broker的Controller类去zk抢注册/kafka/controller节点,成为leader</span><br><span class="line">Controller选出以后,监听/kafka/brokers/ids节点</span><br><span class="line">Controller选举Leader:</span><br><span class="line">	ISR中存活为前提,按照AR中顺序靠前优先</span><br><span class="line">Controller将Leader信息上传zk,即/kafka/brokers/topics/主题名称/partitions/分区编号/state</span><br><span class="line">其他的Controller节点会同步这个节点信息</span><br><span class="line"></span><br><span class="line">数据按照Segment存储,默认1GB,存在.log文件和.index索引文件</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="如何进行节点的服役和退役"><a href="#如何进行节点的服役和退役" class="headerlink" title="如何进行节点的服役和退役"></a>如何进行节点的服役和退役</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">节点服役,直接安装启动即可,但是需要进行主题的负载均衡操作</span><br><span class="line"></span><br><span class="line">退役节点,先进行负载均衡,但是--broker-list指定的是不退役的节点id</span><br></pre></td></tr></table></figure>

<h5 id="副本的知识"><a href="#副本的知识" class="headerlink" title="副本的知识"></a>副本的知识</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">默认1,生产一般配置2</span><br><span class="line">分为Leader和follower,</span><br><span class="line">分区中所有的副本统称为AR</span><br><span class="line">即AR=ISR+OSR</span><br><span class="line">ISR表示和Leader进行同步的follower集合</span><br><span class="line">	参数表示follower超时踢出ISR的时间默认30秒</span><br><span class="line">OSR表示延迟过多的副本follower</span><br></pre></td></tr></table></figure>

<h5 id="Leader的选举规则是"><a href="#Leader的选举规则是" class="headerlink" title="Leader的选举规则是"></a>Leader的选举规则是</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">选举规则:ISR中存活,AR中排序靠前优先,按照AR中顺序轮询</span><br></pre></td></tr></table></figure>

<h5 id="Follower挂掉后的处理流程"><a href="#Follower挂掉后的处理流程" class="headerlink" title="Follower挂掉后的处理流程?"></a>Follower挂掉后的处理流程?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Leader和Follower的数据没有对齐,是因为Leader先获取数据,所以数据量大于Follower</span><br><span class="line">LEO,指的是每个副本的最后的一个offset,实际上是最新的offset+1</span><br><span class="line">HW,指的是所有的副本中,最小的那个LEO</span><br><span class="line">消费者能看到的最大的offset,是HW减去1的那个数据</span><br><span class="line"></span><br><span class="line">挂掉后:</span><br><span class="line">踢出ISR</span><br><span class="line">期间正常接收数据</span><br><span class="line">Follower恢复后,读取本地磁盘上一次的HW,将log中高于HW的数据截取,从HW开始同步Leader的数据,然后等到该Follower的LEO大于等于该分区的HW时,就可以重新加入ISR</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="Leader挂掉后的处理流程"><a href="#Leader挂掉后的处理流程" class="headerlink" title="Leader挂掉后的处理流程"></a>Leader挂掉后的处理流程</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">从ISR中选举一个新的Leader</span><br><span class="line">其余Follower将高出Leader的数据截取掉,所以能保证数据一致性,但是不能保证数据不丢失或者不重复</span><br></pre></td></tr></table></figure>

<h5 id="分区副本如何进行分配"><a href="#分区副本如何进行分配" class="headerlink" title="分区副本如何进行分配"></a>分区副本如何进行分配</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">分区数量大于broker节点数量:尽量均匀的方式进行分配</span><br><span class="line"></span><br><span class="line">0123,1230,2301这样分配</span><br><span class="line"></span><br><span class="line">可以手动的调整</span><br></pre></td></tr></table></figure>

<h5 id="文件存储机制"><a href="#文件存储机制" class="headerlink" title="文件存储机制"></a>文件存储机制</h5><p>底层存储按照topic+分区号设置文件目录,包含index,log等</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">broker-&gt;topic-&gt;partition-&gt;log.dirs-&gt;segment</span><br><span class="line"></span><br><span class="line">Partition是一个物理上的概念</span><br><span class="line">	一个分区对应一个log文件</span><br><span class="line">	生产的数据会不断追加到log文件末尾</span><br><span class="line">	每个Partition分成多个segment文件存储,每个segment包含log文件,index文件,timeindex文件等</span><br><span class="line">	以当前segment第一条消息的offset命名文件名</span><br><span class="line">topic是一个逻辑上的概念,一个topic分成多个分区</span><br><span class="line">	文件夹命名是主题名称+分区编号</span><br><span class="line"></span><br><span class="line">所以真正磁盘上这样存储的:</span><br><span class="line">	first-0</span><br><span class="line">		xxx.index</span><br><span class="line">		xxx.log</span><br><span class="line">		xxx.timeindex</span><br><span class="line"></span><br><span class="line">使用系统工具kafka-run-class.sh kafka.tools.DumpLogSegments --files xxx.index查看</span><br><span class="line"></span><br><span class="line">index文件:</span><br><span class="line">index是稀疏索引,每4KB约数据,写入一条索引,参数log.index.interval.bytes</span><br><span class="line">格式是:</span><br><span class="line">相对offset,position位置</span><br><span class="line">注意position位置指的是log文件中的position位置</span><br><span class="line"></span><br><span class="line">log文件:</span><br><span class="line">格式是 :</span><br><span class="line">baseoffset,lastoffset,position等等内容</span><br><span class="line"></span><br><span class="line">默认7天会删除主题topic数据,可以设置</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><h5 id="生产者如何提高吞吐量"><a href="#生产者如何提高吞吐量" class="headerlink" title="生产者如何提高吞吐量?"></a>生产者如何提高吞吐量?</h5><p>sender线程发送数据到broker集群时,涉及到缓冲区大小buffer,数据可以设置压缩,linger.ms等待时间,默认不等待batch.size时间,batch,size增大</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">四个参数</span><br><span class="line">增加linger.ms等待时间,5-100毫秒</span><br><span class="line">batch.size可以修改16KB到32KB</span><br><span class="line">数据压缩,compression.type例如使用snappy</span><br><span class="line">缓冲区大小默认32MB可以修改为64MB</span><br><span class="line"></span><br><span class="line">properties.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 1024 * 1024 * 64);</span><br><span class="line">properties.put(ProducerConfig.BATCH_SIZE_CONFIG, 1024 * 32);</span><br><span class="line">properties.put(ProducerConfig.LINGER_MS_CONFIG, 5);</span><br><span class="line">properties.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, &quot;snappy&quot;);</span><br></pre></td></tr></table></figure>

<h5 id="生产者如何保证数据不丢不重"><a href="#生产者如何保证数据不丢不重" class="headerlink" title="生产者如何保证数据不丢不重?"></a>生产者如何保证数据不丢不重?</h5><p>如何保证数据不丢失?</p>
<p>ack设置-1,分区partition的副本replication数量&gt;&#x3D;2,并且isr队列中最小副本数也要&gt;&#x3D;2</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">replica.lag.time.max.ms参数默认30秒,超时会将follower踢出isr队列</span><br><span class="line">	这个参数的意思是,在acks=-1时,如果某个follower迟迟没给响应,那么延迟性会很高,此时我们设置这个参数,把这个follower剔除isr队列,不再校验这个存储的情况,所以isr中必须得保证最小的副本数&gt;=2才能保证数据不丢失,即最少得有一个follower在isr中,</span><br><span class="line"></span><br><span class="line">数据可靠性=</span><br><span class="line">    ack应答设置-1 +</span><br><span class="line">	分区副本数&gt;=2 + 即最少一个follower</span><br><span class="line">	isr中最小副本数&gt;=2 即最少一个follower,min.insync.replicas参数默认1,min.insync.replicas怎么设置的?</span><br><span class="line">	一般还把重试次数设置小一点</span><br><span class="line"></span><br><span class="line">properties.put(ProducerConfig.ACKS_CONFIG,&quot;all&quot;);</span><br><span class="line">properties.put(ProducerConfig.RETRIES_CONFIG, 3);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>如何保证数据不重复?</p>
<p>数据不丢失的保证下,开启幂等性,然后设置事务id</p>
<p>能保证单分区数据有序,并且不重复</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">ack=-1时,可能产生数据重复问题,在ack应答时leader挂掉,所以会重复接受数据,但是概率低,但是有概率</span><br><span class="line"></span><br><span class="line">保证精确一次,或者说保证数据不重复</span><br><span class="line">幂等性+上面的数据可靠性</span><br><span class="line">或者事务+上面的数据可靠性</span><br><span class="line"></span><br><span class="line">开启幂等性:enable.idempotence开启true,默认true</span><br><span class="line">幂等性还能保证单分区内有序</span><br><span class="line"></span><br><span class="line">使用事务:先开启幂等性,再操作代码</span><br><span class="line"></span><br><span class="line">出现重复的情况是数据已经保存,但是ack响应的时候没成功响应导致</span><br><span class="line">乱序问题是因为一次性可以发送5个batch数据但是某个发送失败后会重试所以排到后面去了</span><br><span class="line">但是注意如果生产者重启,那么分区编号的那个值会改变,这样还是可能造成重复数据</span><br><span class="line">这个时候再使用事务保证,添加配置transactional.id设置事务id</span><br><span class="line"></span><br><span class="line">properties.put(ProducerConfig.ACKS_CONFIG, &quot;all&quot;);</span><br><span class="line">properties.put(ProducerConfig.RETRIES_CONFIG, 3);</span><br><span class="line">properties.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);</span><br><span class="line">properties.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, &quot;trans1&quot;);</span><br></pre></td></tr></table></figure>

<h5 id="幂等性的判断条件是"><a href="#幂等性的判断条件是" class="headerlink" title="幂等性的判断条件是?"></a>幂等性的判断条件是?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;PID,partition,seqnumber&gt;</span><br><span class="line">PID是kafka集群每次重启都会分配一个新的,所以不能保证多会话不重复</span><br><span class="line">partition表示分区号,所以只能保证单分区内不重复</span><br><span class="line">seqnumber是单调增序列</span><br><span class="line">给每个数据增加一个唯一性标识,即seqnumber,还有一个分区编号标识</span><br></pre></td></tr></table></figure>

<h5 id="事务的原理"><a href="#事务的原理" class="headerlink" title="事务的原理?"></a>事务的原理?</h5><p>通过事务协调器实现,</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">每一个broker节点还包含了:</span><br><span class="line">事务协调器</span><br><span class="line">存储事务信息的特殊主题topic</span><br><span class="line"></span><br><span class="line">多个broker有多个事务协调器,那么具体使用哪一个呢?</span><br><span class="line">根据事务id(手动指定,全局唯一值)的哈希code和50取余,计算该事务属于哪个broker和事务协调器</span><br><span class="line"></span><br><span class="line">1生产者请求pid</span><br><span class="line">2事务协调器返回pid</span><br><span class="line">3发数据</span><br><span class="line">4发数据以后,提交commit到事务协调器</span><br><span class="line">5这个请求消息,持久化到存储事务信息的特殊主题topic中,方便回滚之类</span><br><span class="line">6返回持久化成功</span><br><span class="line">7事务协调器向分区leader发送commit提交请求</span><br><span class="line">8返回数据已经处理成功消息</span><br><span class="line">9事务成功的信息持久化到存储事务信息的特殊主题topic中</span><br></pre></td></tr></table></figure>

<h5 id="如何保证数据单分区内有序"><a href="#如何保证数据单分区内有序" class="headerlink" title="如何保证数据单分区内有序?"></a>如何保证数据单分区内有序?</h5><p>开启幂等性,然后设置发送数据的队列个数&lt;&#x3D;5</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">假设kafka集群有两个broker节点</span><br><span class="line">那么生产者端,可以每个broker缓存5个请求,即sender线程发送数据到集群的请求队列(见上文)</span><br><span class="line"></span><br><span class="line">1版本保证数据单分区有序:设置max.in.flight.requests.per.connection=1</span><br><span class="line">1版本以后保证数据单分区有序:未开启幂等性,设置参数为1,</span><br><span class="line">	开启幂等性:设置小于等于5,因为最多缓存五个,会在内存进行重新排序</span><br><span class="line">乱序问题是因为一次性可以发送5个batch数据但是某个发送失败后会重试所以排到后面去了</span><br><span class="line"></span><br><span class="line">properties.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);</span><br><span class="line">properties.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 5);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="如何对主题进行负载均衡"><a href="#如何对主题进行负载均衡" class="headerlink" title="如何对主题进行负载均衡?"></a>如何对主题进行负载均衡?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">创建json文件</span><br><span class="line">&#123;</span><br><span class="line">	&quot;topics&quot;:[</span><br><span class="line">		&#123;&quot;topic&quot;:&quot;first&quot;&#125;,xxx</span><br><span class="line">	],</span><br><span class="line">	&quot;version&quot;:1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">进行负载均衡计划</span><br><span class="line">kafka-reassign-partitions.sh --bootstrap-server xxx --topics-to-move-json-file xxx --broker-list &quot;0,1,2,3&quot; --generate</span><br><span class="line"></span><br><span class="line">创建副本存储计划的json</span><br><span class="line">将刚才新的执行计划的内容复制</span><br><span class="line"></span><br><span class="line">执行计划</span><br><span class="line">kafka-reassign-partitions.sh --bootstrap-server xxx --reassignment-json-file xxx --execute</span><br><span class="line"></span><br><span class="line">验证是否成功</span><br><span class="line">kafka-reassign-partitions.sh --bootstrap-server xxx --reassignment-json-file xxx --verify</span><br></pre></td></tr></table></figure>

<h5 id="如何手动调整分区副本存储的策略"><a href="#如何手动调整分区副本存储的策略" class="headerlink" title="如何手动调整分区副本存储的策略?"></a>如何手动调整分区副本存储的策略?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">例如将所有副本都存储在broker0和1节点:</span><br><span class="line">创建json</span><br><span class="line">&#123;</span><br><span class="line">	&quot;version&quot;:1,</span><br><span class="line">	&quot;partition&quot;:[</span><br><span class="line">		&#123;&quot;topic&quot;:主题名称a,&quot;partition&quot;:分区号0,&quot;replicas&quot;:[0,1]&#125;,</span><br><span class="line">        &#123;&quot;topic&quot;:主题名称a,&quot;partition&quot;:分区号1,&quot;replicas&quot;:[0,1]&#125;,</span><br><span class="line">        &#123;&quot;topic&quot;:主题名称a,&quot;partition&quot;:分区号2,&quot;replicas&quot;:[1,0]&#125;,</span><br><span class="line">        &#123;&quot;topic&quot;:主题名称a,&quot;partition&quot;:分区号3,&quot;replicas&quot;:[1,0]&#125;</span><br><span class="line">	]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">执行计划</span><br><span class="line">执行计划</span><br><span class="line">kafka-reassign-partitions.sh --bootstrap-server xxx --reassignment-json-file xxx --execute</span><br><span class="line"></span><br><span class="line">验证是否成功</span><br><span class="line">kafka-reassign-partitions.sh --bootstrap-server xxx --reassignment-json-file xxx --verify</span><br></pre></td></tr></table></figure>

<h5 id="Leader-Partition自动平衡"><a href="#Leader-Partition自动平衡" class="headerlink" title="Leader Partition自动平衡"></a>Leader Partition自动平衡</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">本身会进行自动平衡,但是broker节点宕机以后,可能会导致不平衡</span><br><span class="line">使用自动再平衡机制参数,auto.leader.rebalance.enable默认true</span><br><span class="line">比例阈值参数,leader.imbalance,per.broker.percentage默认10%</span><br><span class="line">检查时间间隔参数leader.imbalance.check.interval.seconds默认300秒</span><br><span class="line"></span><br><span class="line">实际生产上,不建议开启自动再平衡</span><br></pre></td></tr></table></figure>

<h5 id="如何增加副本数量"><a href="#如何增加副本数量" class="headerlink" title="如何增加副本数量?"></a>如何增加副本数量?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">即需要后续增加副本数量</span><br><span class="line">不能通过命令行方式修改副本数</span><br><span class="line">创建json文件</span><br><span class="line">&#123;</span><br><span class="line">	&quot;version&quot;:1,</span><br><span class="line">	&quot;partitions&quot;:[</span><br><span class="line">		&#123;&quot;topic&quot;:主题名称a,&quot;partition&quot;:分区号0,&quot;replicas&quot;:[0,1,2]&#125;,</span><br><span class="line">        &#123;&quot;topic&quot;:主题名称a,&quot;partition&quot;:分区号1,&quot;replicas&quot;:[0,1,2]&#125;,</span><br><span class="line">        &#123;&quot;topic&quot;:主题名称a,&quot;partition&quot;:分区号2,&quot;replicas&quot;:[0,1,2]&#125;</span><br><span class="line">	]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">执行计划</span><br><span class="line">kafka-reassign-partitions.sh --bootstrap-server xxx --reassignment-json-file xxx --execute</span><br><span class="line"></span><br><span class="line">验证是否成功</span><br><span class="line">kafka-reassign-partitions.sh --bootstrap-server xxx --reassignment-json-file xxx --verify</span><br></pre></td></tr></table></figure>

<h5 id="文件清除策略"><a href="#文件清除策略" class="headerlink" title="文件清除策略"></a>文件清除策略</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">log.retention.hours默认7天清除数据</span><br><span class="line">log.retention.minutes默认没设置,设置了就使用该参数,分钟级别</span><br><span class="line">log.retention.ms默认没设置,设置了就使用该参数,毫秒级别</span><br><span class="line">检测是否到达时间的参数log.retention.check.intervel.ms,默认5分钟</span><br><span class="line"></span><br><span class="line">建议3天清除,也有保存7小时的</span><br><span class="line"></span><br><span class="line">清除的策略:</span><br><span class="line">删除delete</span><br><span class="line">	log.cleanup.policy默认使用该策略,delete</span><br><span class="line">		按照时间判断,默认方式,即按照segment中所有数据的最大时间戳作为该segment时间戳来判断是否该删除,这样即使一个segment中有一部分没有超时,也不会有问题,因为是按照最大时间戳,也就是最后一条数据的时间戳来判断的</span><br><span class="line">		按照大小判断,参数log.retention.bytes默认-1代表无穷大,代表关闭,一般不会打开的,</span><br><span class="line">压缩compact</span><br><span class="line">	log.cleanup.policy为compact</span><br><span class="line">	针对kv类型数据可以使用,即对于相同key的不同value的数据,只保留最后一个版本</span><br><span class="line">	使用较少</span><br></pre></td></tr></table></figure>

<h5 id="kafka如何高效读写数据"><a href="#kafka如何高效读写数据" class="headerlink" title="kafka如何高效读写数据?"></a>kafka如何高效读写数据?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">分布式,使用分区方式并行读写</span><br><span class="line">读取数据使用稀疏索引,快速定位数据</span><br><span class="line">顺序写入磁盘,追加数据到log末尾</span><br><span class="line">使用页缓存+零拷贝技术</span><br><span class="line">	页缓存,使用底层操作系统的页缓存,就相当于内存缓存加速的</span><br><span class="line">	零拷贝,指的是数据处理不经过集群,不走应用层,直接走网卡传输数据,效率高</span><br></pre></td></tr></table></figure>

<h1 id="消费"><a href="#消费" class="headerlink" title="消费"></a>消费</h1><h5 id="kafka的消费方式是"><a href="#kafka的消费方式是" class="headerlink" title="kafka的消费方式是?"></a>kafka的消费方式是?</h5><p>主动拉取</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">拉取pull</span><br><span class="line">推送push</span><br><span class="line">kafka使用主动拉取的方式进行消费</span><br></pre></td></tr></table></figure>

<h5 id="消费者的工作流程"><a href="#消费者的工作流程" class="headerlink" title="消费者的工作流程"></a>消费者的工作流程</h5><p>创建客户端进行网络交互-主动拉取-存入queue队列-kv反序列化-处理数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">一个消费者可以消费多个分区数据</span><br><span class="line">多个消费者可以消费同一个分区,因为消费者之间是相互隔离的</span><br><span class="line">但是存在消费者组的概念,一个分区只能由一个消费者组的一个消费者进行消费</span><br><span class="line">offset存储在__consumer_offsets中</span><br><span class="line"></span><br><span class="line">前置是消费者组和coordinator</span><br><span class="line"></span><br><span class="line">消费者创建consumer network client</span><br><span class="line">消费者发送消费请求,send fetches</span><br><span class="line">	相关参数:fetch.min.bytes每个批次最小拉取大小,默认1字节</span><br><span class="line">	fetch.max.wait.ms一批数据未达到最小拉取值的超时时间默认500毫秒</span><br><span class="line">	fetch.max.bytes最大每批次抓取大小,默认50MB</span><br><span class="line">拉取来的数据,存入消息队列queue,</span><br><span class="line">具体消费者,从该queue队列获取数据,max.poll.records最大条数,默认500</span><br><span class="line">	先进行反序列化,</span><br><span class="line">	经过拦截器,</span><br><span class="line">	处理数据</span><br></pre></td></tr></table></figure>

<h5 id="消费者组的概念"><a href="#消费者组的概念" class="headerlink" title="消费者组的概念?"></a>消费者组的概念?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">由多个消费者组成,所有消费者的groupid相同则属于同一组</span><br><span class="line">组内消费者消费不同分区数据</span><br><span class="line">消费者组互相之间隔离</span><br><span class="line"></span><br><span class="line">如何消费者组内的消费者数量大于分区数量?会存在闲置的消费者</span><br><span class="line"></span><br><span class="line">coordinator,每个broker节点都有这个组件,消费者组的groupid的哈希code值和50取余,因为__consumer_offsets的分区数量是50</span><br><span class="line">就得到了这个消费者组所对应的broker节点</span><br><span class="line">组内的所有消费者,发送join group请求,到coordinator</span><br><span class="line">coordinator随机选择一个消费者作为leader,把需要消费的topic都发给这个leader</span><br><span class="line">然后这个leader,或者说consumer消费者,会制定一个消费的计划,谁来消费哪个分区的计划</span><br><span class="line">把这个计划发送给coordinator,然后分发给各个consumer</span><br><span class="line">这个coordinator和所有消费者之间会保持一个心跳默认3s,一旦超时,session.timeout.ms=45s,会移除该消费者,并进行重平衡,或者消费者处理时间超时,max.poll.interval.ms=5分钟,也会同样移除,重平衡</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="消费者和分区对应关系"><a href="#消费者和分区对应关系" class="headerlink" title="消费者和分区对应关系?"></a>消费者和分区对应关系?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">即消费者组内的消费者,具体应该消费哪个分区的数据呢?</span><br><span class="line">参数partition.assignment.strategy</span><br><span class="line">默认策略是Range+CooperativeSticky</span><br></pre></td></tr></table></figure>

<h5 id="Range分配策略"><a href="#Range分配策略" class="headerlink" title="Range分配策略"></a>Range分配策略</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">同一个topic的分区,按照分区编号排序,并且对消费者按照字母顺序排序</span><br><span class="line">分区数量/消费者数量,决定每一个消费者应该消费几个分区的数据,除不尽的按顺序多分配消费者上,然后每个消费者消费的分区数量知道以后,再进行分区的分配</span><br><span class="line"></span><br><span class="line">如果topic很多,很容易产生数据倾斜问题</span><br><span class="line"></span><br><span class="line">如果某个消费者挂掉,会整体分配给别人,</span><br><span class="line"></span><br><span class="line">再平衡:会把宕机数据分配其他节点,全部给其中一个消费者</span><br></pre></td></tr></table></figure>

<h5 id="RoundRobin分配策略"><a href="#RoundRobin分配策略" class="headerlink" title="RoundRobin分配策略"></a>RoundRobin分配策略</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">针对所有的topic来说</span><br><span class="line">使用轮询的方式,把所有主题的所有的分区放在一起,把所有的消费者放在一起,然后按照哈希code排序,然后轮询分</span><br><span class="line"></span><br><span class="line">再平衡:轮询分给其他所有节点</span><br></pre></td></tr></table></figure>

<h5 id="Sticky分配策略"><a href="#Sticky分配策略" class="headerlink" title="Sticky分配策略"></a>Sticky分配策略</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">和Range区别在于,是随机的,所以每一次分配的情况都不同</span><br><span class="line"></span><br><span class="line">再平衡:打散给到其余所有消费者</span><br></pre></td></tr></table></figure>

<h5 id="offset维护位置"><a href="#offset维护位置" class="headerlink" title="offset维护位置"></a>offset维护位置</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">维护在__consumer_offsets主题中,0.9版本以前存储在zk中</span><br><span class="line">使用kv格式存储数据,key是消费者组id+topic+分区编号,value是当前offset的值</span><br><span class="line">会间隔一段时间进行compact压缩,即按照key进行压缩保留最新的</span><br><span class="line"></span><br><span class="line">如何查看?</span><br><span class="line">consumer.properties文件</span><br><span class="line">exclude.internal.topics默认false,设置为true才能查看系统主题</span><br><span class="line">不需要重启服务</span><br><span class="line"></span><br><span class="line">命令行查看:</span><br><span class="line">kafka-console-consumer.sh --topic __consumer_offsets --bootstrap-server hadoop101:9092 --consumer.config config/consumer.properties --formatter &quot;kafka.coordinator.group.GroupMetadataManager\$OffsetsMessageFormatter&quot; --from-beginning</span><br></pre></td></tr></table></figure>

<h5 id="提交offset功能"><a href="#提交offset功能" class="headerlink" title="提交offset功能"></a>提交offset功能</h5><p>生产中使用异步提交,手动设置提交</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">自动提交</span><br><span class="line">希望使用者专注于业务实现</span><br><span class="line">enable.auto.commit是否自动提交offset,默认true</span><br><span class="line">auto.commit.interval.ms自动提交的时间间隔,默认5s</span><br><span class="line"></span><br><span class="line">手动提交</span><br><span class="line">分为同步提交和异步提交</span><br><span class="line">同步提交kafkaConsumer.commitSync();</span><br><span class="line">异步提交kafkaConsumer.commitAsync();</span><br><span class="line"></span><br><span class="line">实际生产中,使用异步提交方式</span><br></pre></td></tr></table></figure>

<h5 id="指定offset消费的模式"><a href="#指定offset消费的模式" class="headerlink" title="指定offset消费的模式"></a>指定offset消费的模式</h5><p>lastest最新,earliest最早</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">参数是auto.offset.reset</span><br><span class="line">默认latest,从最新的偏移量消费</span><br><span class="line">earliest相当于--from-beginning,从头消费</span><br><span class="line">none未找到消费者组的先前偏移量,则抛出异常</span><br><span class="line"></span><br><span class="line">还能从指定offset进行消费</span><br><span class="line">需要指定哪个分区从哪个offset进行消费,所以要先获取分区的信息</span><br><span class="line">可能没能消费到数据,为什么?因为从订阅主题到获取分区分配的信息可能延迟了,导致获取的分区信息为空</span><br><span class="line">所以还需要进行一个为空判断</span><br><span class="line">每次消费完,需要更改组id</span><br><span class="line"></span><br><span class="line">指定时间段进行重新消费</span><br><span class="line">即把时间段转换成offset,然后还是使用指定offset的方式消费</span><br></pre></td></tr></table></figure>

<h5 id="offset如何使用"><a href="#offset如何使用" class="headerlink" title="offset如何使用"></a>offset如何使用</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">auto.offset.reset=earliest/latest/none</span><br><span class="line">默认是latest即从最新数据开始消费</span><br><span class="line">earliest即命令行的--from-begining从头开始消费</span><br><span class="line"></span><br><span class="line">properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,&quot;latest&quot;);</span><br></pre></td></tr></table></figure>

<h5 id="消费者如何保证不漏不重"><a href="#消费者如何保证不漏不重" class="headerlink" title="消费者如何保证不漏不重?"></a>消费者如何保证不漏不重?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">重复消费:自动提交offset引起,自动提交offset间隔中,消费者挂掉,重启consumer后从上一次offset消费,导致重复消费</span><br><span class="line">漏消费:设置offset手动提交引起,数据还没有落盘时,消费者挂掉,那么offset已经提交但是数据没有处理,</span><br><span class="line"></span><br><span class="line">解决方式:使用事务,即将消费过程和提交offset绑定事务,下游系统必须支持事务(例如下游还是kafka或者rdbms)</span><br></pre></td></tr></table></figure>

<h5 id="消费者如何提高吞吐量"><a href="#消费者如何提高吞吐量" class="headerlink" title="消费者如何提高吞吐量"></a>消费者如何提高吞吐量</h5><p>增加分区,同时增加消费者数据量,二者相等</p>
<p>每批次拉取数据量的条数和大小,增加</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">即如何提高消费者的吞吐量</span><br><span class="line">如果是kafka消费能力不足,可以增加分区,同时增加消费者数量,让消费者数量=分区数量</span><br><span class="line">	增加linger.ms等待时间,5-100毫秒</span><br><span class="line">	batch.size可以修改16KB到32KB</span><br><span class="line">	数据压缩,compression.type例如使用snappy</span><br><span class="line">	缓冲区大小默认32MB可以修改为64MB</span><br><span class="line">如果是下游处理不足,提高每批次拉取数据量,以及上限值,50MB和500条</span><br><span class="line">	max.poll.records最大条数,默认500</span><br><span class="line">	fetch.max.bytes最大每批次抓取大小,默认50MB</span><br><span class="line">	</span><br><span class="line">properties.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 1000);</span><br><span class="line">properties.put(ConsumerConfig.FETCH_MAX_BYTES_CONFIG, 1024 * 1024 * 100);</span><br></pre></td></tr></table></figure>

<h1 id="Kraft模式"><a href="#Kraft模式" class="headerlink" title="Kraft模式"></a>Kraft模式</h1><h5 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">不依赖zk,设置三台controller进行集群管理</span><br><span class="line">元数据存储在controller中</span><br></pre></td></tr></table></figure>

<h1 id="调优"><a href="#调优" class="headerlink" title="调优"></a>调优</h1><h5 id="硬件配置"><a href="#硬件配置" class="headerlink" title="硬件配置"></a>硬件配置</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">假设场景</span><br><span class="line">每天数据条数:100万日活*100条=1亿条(中型公司)</span><br><span class="line">每秒速度:1亿/(24*3600)=1150条/秒</span><br><span class="line">1条数据:0.5k-2k,取值1k</span><br><span class="line">所以每秒平均数据量:1150*1=1MB/秒</span><br><span class="line">每秒峰值数据量:假设20倍,所以20MB/秒</span><br><span class="line"></span><br><span class="line">评估服务器数量:</span><br><span class="line">数量=2*(生产者峰值*副本数/100)+1</span><br><span class="line">=2*(20*2/100)+1</span><br><span class="line">除不尽往上进位</span><br><span class="line">=3台</span><br><span class="line"></span><br><span class="line">磁盘选择:</span><br><span class="line">按照顺序读写,所以机械硬盘即可</span><br><span class="line">1亿*1k=100GB/天</span><br><span class="line">100GB*2副本*保存3天/0.7(预留0.3)=1TB</span><br><span class="line">所以三台总计1TB即可</span><br><span class="line"></span><br><span class="line">内存选择:</span><br><span class="line">内存=kafka堆内存+页缓存</span><br><span class="line">堆内存=10-15GB即可,修改start脚本</span><br><span class="line">页缓存=一般将segment的1GB的25%放置页缓存即可*分区总数中leader总数量(假设3台服务器对应3分区)/集群broker台数=256MB</span><br><span class="line">所以一台服务器=10GB+256MB即可</span><br><span class="line"></span><br><span class="line">如何查看内存使用情况:</span><br><span class="line">jstat -gc 进程号 ls 10</span><br><span class="line">主要查看YGC次数</span><br><span class="line">jmap -heap 进程号</span><br><span class="line"></span><br><span class="line">CPU核数</span><br><span class="line">num.io.threads=8写磁盘的线程数,需要占比总C的50%</span><br><span class="line">num.replica.fetchers=1副本拉取线程,占比总C的50%的三分之一</span><br><span class="line">num.network.threads=3数据传输线程数,占比总C的50%的三分之二</span><br><span class="line">建议32C,分成24+8</span><br><span class="line">参数分别是12,4,8</span><br><span class="line">剩余8留给系统本身</span><br><span class="line"></span><br><span class="line">网络</span><br><span class="line">网络带宽=峰值吞吐量=20MB/s</span><br><span class="line">千兆网卡,1000Mbps,即1000/8=125MB/s</span><br></pre></td></tr></table></figure>

<h5 id="生产者配置"><a href="#生产者配置" class="headerlink" title="生产者配置"></a>生产者配置</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">提高吞吐量:</span><br><span class="line">	32MB-&gt;64MB</span><br><span class="line">	16KB-&gt;32KB</span><br><span class="line">	0-&gt;5到100ms</span><br><span class="line">	压缩snappy</span><br><span class="line"></span><br><span class="line">数据可靠:</span><br><span class="line">	ack=-1</span><br><span class="line">	副本&gt;=2 --replicate-factor=3 </span><br><span class="line">	ISR&gt;=2(默认1) min.insync.replicas=2</span><br><span class="line"></span><br><span class="line">数据去重:</span><br><span class="line">	幂等性开启</span><br><span class="line">	代码使用事务</span><br><span class="line"></span><br><span class="line">数据乱序解决:</span><br><span class="line">	幂等性开启</span><br><span class="line">	ack次数默认5</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="broker配置"><a href="#broker配置" class="headerlink" title="broker配置"></a>broker配置</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">自动再平衡</span><br><span class="line">	建议关闭</span><br><span class="line">	auto.leader.rebalance.enable 建议false</span><br><span class="line"></span><br><span class="line">数据存储时间:</span><br><span class="line">	建议7天-&gt;3天</span><br><span class="line"></span><br><span class="line">线程三个参数设置:</span><br><span class="line">	查看上面的硬件配置</span><br><span class="line"></span><br><span class="line">自动创建主题:</span><br><span class="line">	建议关闭</span><br><span class="line">	auto.create.topics.enable=false</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="消费者配置"><a href="#消费者配置" class="headerlink" title="消费者配置"></a>消费者配置</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">消费者事务:</span><br><span class="line">	使用事务</span><br><span class="line">消费者提高吞吐量:</span><br><span class="line">	增加分区数量--partitions</span><br><span class="line">	max.poll.records一次性拉取数据条数500增加</span><br><span class="line">	fetch.max.bytes默认50MB增加</span><br><span class="line">    </span><br></pre></td></tr></table></figure>

<h5 id="精确一次配置"><a href="#精确一次配置" class="headerlink" title="精确一次配置"></a>精确一次配置</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">ack=-1</span><br><span class="line">分区&gt;=2,建议3,--replication-factor=3</span><br><span class="line">ISR&gt;=2,建议2,min.insync.replicas=2</span><br><span class="line"></span><br><span class="line">幂等性</span><br><span class="line">事务代码</span><br><span class="line">手动提交offset,enable.auto.commit=false,异步提交</span><br><span class="line">输出目的地支持事务</span><br><span class="line"></span><br><span class="line">手动提交offset+消费过程绑定事务,详见后续</span><br></pre></td></tr></table></figure>

<h5 id="吞吐量配置"><a href="#吞吐量配置" class="headerlink" title="吞吐量配置"></a>吞吐量配置</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">32MB-&gt;64MB</span><br><span class="line">16KB-&gt;32KB</span><br><span class="line">0-&gt;5到100ms</span><br><span class="line">压缩snappy</span><br><span class="line"></span><br><span class="line">增加分区数量--partitions</span><br><span class="line">max.poll.records一次性拉取数据条数500增加</span><br><span class="line">fetch.max.bytes默认50MB增加</span><br></pre></td></tr></table></figure>

<h5 id="分区数规划"><a href="#分区数规划" class="headerlink" title="分区数规划"></a>分区数规划</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">假设想要的吞吐量是100MB/s</span><br><span class="line">上面说生产者20MB/s吞吐</span><br><span class="line">那么分区数量设置100/20=5</span><br><span class="line"></span><br><span class="line">做压测</span><br><span class="line"></span><br><span class="line">一般3-10分区数</span><br><span class="line"></span><br><span class="line">注意分区数量可以大于broker数量</span><br></pre></td></tr></table></figure>

<h5 id="单条日志大于1MB"><a href="#单条日志大于1MB" class="headerlink" title="单条日志大于1MB"></a>单条日志大于1MB</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">该问题的解决:</span><br><span class="line">message.max.bytes默认1MB,调大,针对broker</span><br><span class="line">max.request.size,默认1MB,调大,针对topic</span><br><span class="line">replica.fetch.max.bytes,默认1MB,调大</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="服务器宕机"><a href="#服务器宕机" class="headerlink" title="服务器宕机"></a>服务器宕机</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">尝试重启</span><br><span class="line">增加资源</span><br><span class="line">服役新节点并负载均衡</span><br></pre></td></tr></table></figure>

<h5 id="压测"><a href="#压测" class="headerlink" title="压测"></a>压测</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">生产者压测,kafka-producer-perf-test.sh</span><br><span class="line">	创建压测主题</span><br><span class="line">	kafka-producer-perf-test.sh --topic test --record-size 1024 --num-records 1000000 --throughput 10000 --producer-props.bootstrap.servers=xxx batch.size=xxx linger.ms=xxx</span><br><span class="line">	record-size 一条数据的大小,单位字节</span><br><span class="line">	num-records 总共测试多少条数据</span><br><span class="line">	--throughput 每秒10000条这里</span><br><span class="line"></span><br><span class="line">消费者压测,kafka-consumer-perf-test.sh</span><br><span class="line">    kafka-consumer-perf-test.sh --bootstrap-server xxx --topic xxx --message 1000000 --consumer.config xxx</span><br><span class="line">    --message总共需要消费的数据条数</span><br><span class="line">    max.poll.records一次性拉取数据条数500增加</span><br><span class="line">	fetch.max.bytes默认50MB增加</span><br><span class="line">	压测上面两个参数</span><br><span class="line">	</span><br></pre></td></tr></table></figure>

<h1 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h1><h5 id="producer源码"><a href="#producer源码" class="headerlink" title="producer源码"></a>producer源码</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">整体流程:</span><br><span class="line"></span><br><span class="line">A 创建生产者对象过程,创建了生产者对象,并开启了sender线程</span><br><span class="line"></span><br><span class="line">B 发送数据过程:</span><br><span class="line">1 main中的send方法</span><br><span class="line">	生产者KafkaProducer类调用send方法发送数据</span><br><span class="line">	数据是封装为ProducerRecord</span><br><span class="line">2 Interceptor拦截器</span><br><span class="line">	底层先调用拦截器ProducerInterceptors对象的onSend方法</span><br><span class="line">		onSend方法里面是将拦截器这个列表循环遍历,去执行每一个拦截器ProducerInterceptor接口的onSend方法的具体实现</span><br><span class="line">		最终将一个ProducerRecord转换成另一个ProducerRecord</span><br><span class="line">		然后再调用doSend发送</span><br><span class="line">3 Serializer序列化器</span><br><span class="line">	该方法里面会使用key序列化器和value序列化器进行序列化,必须传入,否则这里直接抛出异常了</span><br><span class="line">4 Partition分区器</span><br><span class="line">	执行分区器的方法partition,返回int类型分区编号</span><br><span class="line">		先判断是否在ProducerRecord中设置了分区编号,设置了直接使用,不会做任何正确性判断</span><br><span class="line">		再判断是否有自定义分区器</span><br><span class="line">		再判断key不为空而且不忽略key即参数partitioner.ignore,keys默认false,就根据key进行分区</span><br><span class="line">		再判断key为空,最终会根据分区负载情况粘性分区,即尽可能选择同一个分区,阈值batch.size=16kb</span><br><span class="line">5 RecordAccumulate数据缓冲区</span><br><span class="line">	大小 buffer.memory=32mb</span><br><span class="line">	max.request.size=1mb</span><br><span class="line">6 Sender线程</span><br><span class="line">	两个参数:batch.size=16kb 达到linger.ms默认0</span><br><span class="line">	sendProducerData发送数据</span><br><span class="line">	poll拉取发送后的结果</span><br><span class="line">7 NetworkClient</span><br><span class="line">	sender线程中客户端最终调用send方法发送数据,这个客户端的具体实现类就是NetworkClient</span><br><span class="line">	max.in.flight.requests.per.connection 默认5</span><br><span class="line">8 Selector</span><br><span class="line">	selector最终调用send方法发送数据</span><br><span class="line">	失败重试retries 默认int最大值</span><br><span class="line">	成功后清理数据缓冲区对应数据</span><br><span class="line">9 ACKS应答</span><br></pre></td></tr></table></figure>

<h5 id="consumer源码"><a href="#consumer源码" class="headerlink" title="consumer源码"></a>consumer源码</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">整体流程</span><br><span class="line">A 初始化流程,即创建KafkaConsumer对象</span><br><span class="line">1 创建客户端对象ConsumerNetworkClient</span><br><span class="line">	初始化相关参数</span><br><span class="line">		连接重试时间reconnect.backoff.ms 50ms</span><br><span class="line">		最大连接重试时间reconnect.backoff.max.ms 1s</span><br><span class="line">		发送缓存send.buffer 128kb</span><br><span class="line">		接收缓存receive.buffer 64kb</span><br><span class="line">		客户端请求服务器超时时间request.timeout.ms 30s</span><br><span class="line">2 消费者分区分配策略 三种</span><br><span class="line">	range</span><br><span class="line">	round robin</span><br><span class="line">	sticky</span><br><span class="line">3 创建消费者组coordinator</span><br><span class="line">    消费者和coordinator会保持3s心跳</span><br><span class="line">        这两个参数触发,会触发消费者再平衡</span><br><span class="line">        session.timeout.ms=45s 心跳超时</span><br><span class="line">        max.poll.interval.ms=5分钟 消费者处理时间超时</span><br><span class="line">	ConsumerCoordinator对象</span><br><span class="line">	自动提交offset时间auto.commit.interval.ms 默认5s</span><br><span class="line">4 创建Fetcher对象,抓取数据</span><br><span class="line">    fetch.min.bytes每批次最小抓取字节数默认1字节</span><br><span class="line">	fetch.max.wait.ms一批次的数据最小值都未达到的超时时间默认500ms</span><br><span class="line">	fetch.max.bytes最大50mb</span><br><span class="line">	max.partition.fetch.bytes 默认1mb即单条日志最大上限</span><br><span class="line">	max.poll.records默认500条</span><br><span class="line"></span><br><span class="line">B 订阅主题,即main线程调用subscribe方法</span><br><span class="line">1 注册监听器,实现负载均衡</span><br><span class="line">	registerRebalanceListener</span><br><span class="line">2 订阅主题</span><br><span class="line"></span><br><span class="line">拉取数据,即main线程调用到了poll方法</span><br><span class="line">1 消费者组选择coordinator,即消费者组初始化</span><br><span class="line">	updateAssignmentMetadataIfNeeded</span><br><span class="line">	组id的hashcode对50取模(__consumer_offsets的分区数量是50默认)</span><br><span class="line">2 调用pollForFetches抓取数据</span><br><span class="line">3 发送消费请求sendFetches该方法比较核心</span><br><span class="line">    fetch.min.bytes每批次最小抓取字节数默认1字节</span><br><span class="line">	fetch.max.wait.ms一批次的数据最小值都未达到的超时时间默认500ms</span><br><span class="line">	fetch.max.bytes最大50mb</span><br><span class="line">3 客户端发送调用send</span><br><span class="line">4 发送结果设置了监听器,成功的回调函数onSuccess</span><br><span class="line">	成功的数据batchs和offset都放在completedFetches队列中</span><br><span class="line">5 消费者调用fetchRecords再次拉取数据</span><br><span class="line">	每批次从completedFetches队列中拉取的</span><br><span class="line">	参数max.poll.records默认500条</span><br><span class="line">	调用completedFetches.poll真正拉取数据</span><br><span class="line">6 反序列化</span><br><span class="line">7 拦截器intercepts调用onConsume处理</span><br><span class="line">	拦截器链</span><br><span class="line">8 用户处理数据阶段</span><br><span class="line"></span><br><span class="line">C 提交offset,即main调用commitAsync</span><br><span class="line">1 协调器coordinator调用commitOffsetsAsync</span><br><span class="line">	(异步提交)</span><br><span class="line">	sendOffsetCommitRequest发送提交的请求</span><br><span class="line">	结果注册监听器addListener</span><br><span class="line">		onSuccess回调函数进行处理</span><br></pre></td></tr></table></figure>

<h5 id="broker源码"><a href="#broker源码" class="headerlink" title="broker源码"></a>broker源码</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">broker源码</span><br><span class="line">KafkaServer类</span><br><span class="line">	startup方法</span><br><span class="line">		执行initZkClient启动</span><br><span class="line">		执行logManager.startup启动	</span><br><span class="line">		执行clientToControllerChannelManager.start()启动 网络客户端,可以接收请求</span><br><span class="line">		new SocketServer,创建网络服务器,可以发送请求,默认9092</span><br><span class="line">		调用replicaManager.startup()启动 用来管理底层文件和对象映射</span><br><span class="line">		调用kafkaController.startup()启动</span><br><span class="line">		创建AutoTopicCreationManager,即自动创建topic如果不存在</span><br><span class="line">		new KafkaApis对象,将请求的不同类型转换成具体要执行的功能</span><br><span class="line">			</span><br></pre></td></tr></table></figure>



      
    </div>
    <footer class="article-footer">
      <a data-url="https://nfsp412.github.io/asuka.github.io/2024/10/24/bigdata005/" data-id="cm3ae93sn00096cur5x7l80ya" data-title="Kafka学习笔记" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/asuka.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag">大数据</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata006" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/asuka.github.io/2024/10/23/bigdata006/" class="article-date">
  <time class="dt-published" datetime="2024-10-23T03:24:38.000Z" itemprop="datePublished">2024-10-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/asuka.github.io/2024/10/23/bigdata006/">HBase学习笔记</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>本文是自己在学习HBase过程中整理的一些基础笔记,仅做记录</p>
<h5 id="hbase是什么"><a href="#hbase是什么" class="headerlink" title="hbase是什么"></a>hbase是什么</h5><p>以hdfs存储数据,分布式,nosql数据库</p>
<p>数据存储稀疏,多维度</p>
<p>整体按照rowkey字典序排序</p>
<p>不同行的数据有不同的列</p>
<p>物理存储结构为rowkey+列族+字段名key+字段值value+时间戳+类型</p>
<h5 id="hbase的一些概念"><a href="#hbase的一些概念" class="headerlink" title="hbase的一些概念"></a>hbase的一些概念</h5><p>namespace,类比数据库,默认default</p>
<p>table,类比数据表,但是只需要声明列族,不需要具体字段名和字段值,因为可以动态按需</p>
<p>rowkey,字典序排序,检索索引,十分重要</p>
<p>时间戳,记录了数据的不同版本,是写入hbase的时间</p>
<p>列限定符,即具体的字段名,key</p>
<p>cell,一行就是一个cell</p>
<p>region,其实就是分目录加快查询,按照region id分目录,会进行拆分,可以系统默认可以自定义预分区</p>
<p>hfile,具体体现的存储的文件,包含了索引,布隆过滤,kv键值对数据等,会进行小合并和大合并</p>
<p>mem store一个mem store对应一个store file(hfile),有刷写的参数控制,单文件有序</p>
<h5 id="hbase架构"><a href="#hbase架构" class="headerlink" title="hbase架构"></a>hbase架构</h5><p>基于zookeeper</p>
<p>HMaster:负载均衡,元数据hbase:meta表管理,WAL预写日志</p>
<p>HRegionServer:WAL保证数据有序,block cache读取缓存,优化读取效率,mem store写出缓存,每一个store有对应的单独的一个mem store</p>
<p>支持高可用</p>
<p>默认端口16010</p>
<h5 id="hbase存储数据路径"><a href="#hbase存储数据路径" class="headerlink" title="hbase存储数据路径"></a>hbase存储数据路径</h5><p>namespace对应目录</p>
<p>table表对应目录</p>
<p>下分多个region</p>
<p>下分store目录(一长串随机字符串,实际上是region id)</p>
<p>hfile存储在store,内部就是一个个 cell</p>
<h5 id="命令行操作"><a href="#命令行操作" class="headerlink" title="命令行操作"></a>命令行操作</h5><p>命令行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hbase shell</span><br></pre></td></tr></table></figure>

<p>创建表语法:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create &#x27;bigdata:student&#x27;, &#123;NAME =&gt; &#x27;info&#x27;, VERSIONS =&gt;5&#125;, &#123;NAME =&gt; &#x27;msg&#x27;&#125;  不写默认1version</span><br></pre></td></tr></table></figure>

<p>创建表语法(设置预分区)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">create &#x27;staff1&#x27;,&#x27;info&#x27;, SPLITS =&gt; [&#x27;1000&#x27;,&#x27;2000&#x27;,&#x27;3000&#x27;,&#x27;4000&#x27;]</span><br><span class="line">create &#x27;staff3&#x27;, &#x27;info&#x27;,SPLITS_FILE =&gt; &#x27;splits.txt&#x27;</span><br><span class="line">create &#x27;staff2&#x27;,&#x27;info&#x27;,&#123;NUMREGIONS =&gt; 15, SPLITALGO =&gt; &#x27;HexStringSplit&#x27;&#125;</span><br></pre></td></tr></table></figure>

<p>查询表语法:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">describe &#x27;student1&#x27;  / list</span><br></pre></td></tr></table></figure>

<p>新增&#x2F;修改表的列族信息:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter &#x27;student1&#x27;, &#123;NAME =&gt; &#x27;f1&#x27;, VERSIONS =&gt; 3&#125;  </span><br></pre></td></tr></table></figure>

<p>删除表的列族信息:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter &#x27;bigdata:student&#x27;, &#123;NAME =&gt; &#x27;msg&#x27;, METHOD =&gt; &#x27;delete&#x27;&#125;</span><br></pre></td></tr></table></figure>

<p>删除表:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">先disable &#x27;student1&#x27;  再drop &#x27;student1&#x27;  </span><br></pre></td></tr></table></figure>

<p>写入数据:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">put &#x27;bigdata:student&#x27;,&#x27;1001&#x27;,&#x27;info:age&#x27;,&#x27;18&#x27;  重复写入按照rowkey+列族+字段名进行覆盖</span><br></pre></td></tr></table></figure>

<p>读取数据:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get &#x27;bigdata:student&#x27;,&#x27;1001&#x27; , &#123;COLUMN =&gt; &#x27;info:name&#x27;,VERSIONS =&gt; 5&#125;  后面可以不指定列族信息和版本信息</span><br></pre></td></tr></table></figure>

<p>读取多行数据:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scan &#x27;bigdata:student&#x27;,&#123;STARTROW =&gt; &#x27;1001&#x27;,STOPROW =&gt;&#x27;1002&#x27;&#125;  </span><br></pre></td></tr></table></figure>

<p>删除数据:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">delete &#x27;bigdata:student&#x27;,&#x27;1001&#x27;,&#x27;info:name&#x27;  / </span><br><span class="line">deleteall &#x27;bigdata:student&#x27;,&#x27;1001&#x27;,&#x27;info:name&#x27;  后面是删除所有version</span><br></pre></td></tr></table></figure>

<p>元数据读取</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/opt/module/hbase/bin/hbase hfile -m -f /hbase/data/bigdata/student/xxx/info/xxx</span><br></pre></td></tr></table></figure>

<h5 id="phoenix结合使用"><a href="#phoenix结合使用" class="headerlink" title="phoenix结合使用"></a>phoenix结合使用</h5><p>Phoenix 中建表，会在 HBase 中创建一张对应的表  ,Phoenix 默认会对 HBase 中的列名(字段名称)做编码处理  </p>
<p>表名自动转换大写,使用双引号变成小写</p>
<p>插入数据upsert,查询select,删除delete,删除表drop</p>
<p>hbase的表使用phoenix进行映射,一种是视图映射create view,一种是表映射create table</p>
<h5 id="phoenix视图映射和表映射的区别"><a href="#phoenix视图映射和表映射的区别" class="headerlink" title="phoenix视图映射和表映射的区别"></a>phoenix视图映射和表映射的区别</h5><p>视图映射只能查询,表映射可以增删改查,不能使用列名称编码注意</p>
<h5 id="rowkey设计的思路"><a href="#rowkey设计的思路" class="headerlink" title="rowkey设计的思路"></a>rowkey设计的思路</h5><p>让数据均匀分布,防止数据倾斜</p>
<p>例如随机数,hash,散列值</p>
<p>时间戳,业务含义主键字符串反转,例如客户号字符串反转</p>
<p>字符串拼接,多个业务需求中的业务主键进行拼接</p>
<p>在rowkey最前面加上预分区的分区号</p>
<p>适用性强 泛用性差  </p>
<h5 id="hbase的预分区是什么"><a href="#hbase的预分区是什么" class="headerlink" title="hbase的预分区是什么"></a>hbase的预分区是什么</h5><p>每一个 region 维护着 startRow 与 endRowKey，如果加入的数据符合某个 region 维护的rowKey 范围，则该数据交给这个 region 维护。那么依照这个原则，我们可以将数据所要投放的分区提前大致的规划好  </p>
<p>自定义预分区,或者系统默认自动分区</p>
<h5 id="region拆分参数"><a href="#region拆分参数" class="headerlink" title="region拆分参数"></a>region拆分参数</h5><p>2版本后,当前region server当前表只有一个region,则按照2*128MB切分,否则按照配置的值切分,推荐配置20GB</p>
<h5 id="hbase参数调优"><a href="#hbase参数调优" class="headerlink" title="hbase参数调优"></a>hbase参数调优</h5><p>参考的网上文档,region设置20GB,刷写大小128MB</p>
<p>一个表尽量一个列族,列族名称尽量短,例如f1</p>
<h5 id="hbase写入流程"><a href="#hbase写入流程" class="headerlink" title="hbase写入流程"></a>hbase写入流程</h5><p>向zk发送连接请求</p>
<p>获取meta表存储的region server位置,然后读取</p>
<p>读取meta表整个加载到meta cache连接缓存中,重操作</p>
<p>如果meta表发生变化则重新读取</p>
<p>发送put命令,表名,rowkey找到region,列族找到store</p>
<p>先写入WAL落盘,再写入mem store,此时返回ACK校验,排序,单文件store file有序</p>
<p>刷写flush到对应的store中,即hfile&#x2F;store file</p>
<h5 id="hbase读取流程"><a href="#hbase读取流程" class="headerlink" title="hbase读取流程"></a>hbase读取流程</h5><p>向zk发送连接请求</p>
<p>获取meta表存储的region server位置,然后读取</p>
<p>读取meta表整个加载到meta cache连接缓存中,重操作</p>
<p>如果meta表发生变化则重新读取</p>
<p>发送get命令,可能会读三个位置</p>
<p>1 block cache读缓存,为了提高效率,没变化就不会再次读取</p>
<p>2 mem store写缓存数据也要读取</p>
<p>3 store file(hfile)文件的数据,该文件包含了索引,布隆过滤器,key值,64kb大小存储数据本身</p>
<p>最终合并,高版本覆盖低版本,读取</p>
<h5 id="mem-store-刷写flush条件"><a href="#mem-store-刷写flush条件" class="headerlink" title="mem store 刷写flush条件"></a>mem store 刷写flush条件</h5><p>刷写由多个线程控制,每个线程有自己的判断条件,互相独立</p>
<p>1 mem store达到128MB,或者128MB*4(此时会组织继续写入mem store并刷写)</p>
<p>2 低水位线&#x3D;堆内存×0.4×0.95,按照mem store大小顺序刷写</p>
<p>高水位线&#x3D;堆内存×0.4,阻止数据写入mem store</p>
<p>3 周期性监控5min,是否达到1h还没有刷写,基本不会出现</p>
<p>4 WAL文件数量超过32还没刷写,基本不会出现</p>
<h5 id="文件合并的概念"><a href="#文件合并的概念" class="headerlink" title="文件合并的概念"></a>文件合并的概念</h5><p>分为小合并和大合并</p>
<p>mem store每次刷写,都会生产一个hfile,所以要进行文件合并</p>
<p>小合并将临近的若干小文件进行合并,清理过期和删除数据</p>
<p>大合并默认7天进行一次</p>
<h5 id="如何结合hive"><a href="#如何结合hive" class="headerlink" title="如何结合hive"></a>如何结合hive</h5><p>第一种,创建hive表,同时会创建hbase表,然后写入数据到hive表,hbase表自动同步</p>
<p>第二种,针对已有的hbase表,创建hive外部表映射hbase表,然后使用hivesql进行表数据分析</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://nfsp412.github.io/asuka.github.io/2024/10/23/bigdata006/" data-id="cm3ae93sl00046curfcfu2f9i" data-title="HBase学习笔记" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/asuka.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag">大数据</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata007" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/asuka.github.io/2024/10/21/bigdata007/" class="article-date">
  <time class="dt-published" datetime="2024-10-21T03:24:38.000Z" itemprop="datePublished">2024-10-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/asuka.github.io/2024/10/21/bigdata007/">Clickhouse学习笔记</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>本文是自己在学习Clickhouse过程中整理的一些基础笔记,仅做记录</p>
<h5 id="clickhouse是什么"><a href="#clickhouse是什么" class="headerlink" title="clickhouse是什么"></a>clickhouse是什么</h5><p>基于c++编写,列式存储数据库,用于olap在线分析</p>
<p>50mb-200mb每秒的写入速度</p>
<p>每行数据100bytes估算,即50w-200w每秒的数据条数写入</p>
<p>单表聚合查询速度特别快,测试比presto,impala,spark,hive,都要快</p>
<p>并行处理能力强,单条查询语句利用整机cpu</p>
<p>关联查询慢</p>
<h5 id="clickhouse有哪些表引擎"><a href="#clickhouse有哪些表引擎" class="headerlink" title="clickhouse有哪些表引擎"></a>clickhouse有哪些表引擎</h5><p>TinyLog,不支持索引,没有并发控制,平时练习使用,用得少</p>
<p>Memory,未压缩数据存储内存,不支持索引,数据量尽量限制在1亿行内,用得少</p>
<p>MergeTree  ,以及衍生系列</p>
<p>ReplacingMergeTree  </p>
<p>SummingMergeTree  </p>
<h5 id="分区的作用"><a href="#分区的作用" class="headerlink" title="分区的作用"></a>分区的作用</h5><p>partition by分区,目的主要是降低扫描的范围，优化查询速度  ,可选</p>
<p>涉及跨分区的查询统计， ClickHouse 会以分区为单位并行处理  </p>
<p>任何一个批次的数据写入都会产生一个临时分区，不会纳入任何一个已有的分区。写入<br>后的某个时刻（大概 10-15 分钟后）， ClickHouse 会自动执行合并操作（等不及也可以手动<br>通过 optimize 执行）  </p>
<p>是可选项</p>
<h5 id="主键的作用"><a href="#主键的作用" class="headerlink" title="主键的作用"></a>主键的作用</h5><p>提供一级索引,但是不是唯一性约束,一般根据where条件设置</p>
<p>根据条件通过对主键进行某种形式的二分查找，能够定位到对应的 index granularity,避<br>免了全表扫描</p>
<p>主键必须是 order by 字段的前缀字段  比如 order by 字段是 (id,sku_id) 那么主键必须是 id 或者(id,sku_id)    </p>
<p>是可选项</p>
<h5 id="排序的作用"><a href="#排序的作用" class="headerlink" title="排序的作用"></a>排序的作用</h5><p>最重要,必选项</p>
<p>order by 设定了分区内的数据按照哪些字段顺序进行有序保存  </p>
<h5 id="二级索引"><a href="#二级索引" class="headerlink" title="二级索引"></a>二级索引</h5><p>21版本支持二级索引,二级索引能够为非主键字段的查询发挥作用  </p>
<p>sql写法INDEX a total_amount TYPE minmax GRANULARITY 5  </p>
<p>GRANULARITY N 是设定二级索引对于一级索引粒度的粒度  </p>
<p>查询where根据total_amount过滤时能触发二级索引</p>
<h5 id="ttl"><a href="#ttl" class="headerlink" title="ttl"></a>ttl</h5><p>有列级别的ttl设置和表级别的ttl设置</p>
<p>表级别的设置建表时指定,推荐使用分区的日期字段  </p>
<p>例如 TTL toDate(create_time) + toIntervalDay(2)</p>
<h5 id="ReplacingMergeTree"><a href="#ReplacingMergeTree" class="headerlink" title="ReplacingMergeTree"></a>ReplacingMergeTree</h5><p>多一个去重功能,例如根据分区时间日期字段去重</p>
<p>数据的去重只会在合并的过程中出现  ,或者在insert插入时去重</p>
<p>如果表经过了分区，去重只会在分区内部进行去重，不能执行跨分区的去重  </p>
<p>适用于在后台清除重复的数据以节省空间，但是它不保证没有重复的数据出现  </p>
<p>实际上按照order by指定的字段去重,相同的则按照插入顺序去重</p>
<h5 id="SummingMergeTree"><a href="#SummingMergeTree" class="headerlink" title="SummingMergeTree"></a>SummingMergeTree</h5><p>按照指定字段汇总,必须是数值列,</p>
<p>order by字段作为维度列</p>
<p>汇总后其他字段保留第一条字段值</p>
<p>不在一个分区的数据不会被聚合  </p>
<p>只有在同一批次插入(新版本)或分片合并时才会进行聚合  </p>
<p>设计时,唯一键值、流水号可以去掉，所有字段全部是维度、度量或者时间戳  </p>
<h5 id="ReplicatedMergeTree"><a href="#ReplicatedMergeTree" class="headerlink" title="ReplicatedMergeTree"></a>ReplicatedMergeTree</h5><p>创建副本表,需要在不同节点分别建表,这样就相当于创建了副本表,数据能够同步,查询任意都可以查询到</p>
<p>参数1写法:&#x2F;clickhouse&#x2F;tables&#x2F;库名&#x2F;表名&#x2F;{uuid}&#x2F;{shard},分片</p>
<p>参数2写法:{replica},副本</p>
<h5 id="Distributed"><a href="#Distributed" class="headerlink" title="Distributed"></a>Distributed</h5><p>通过分片把一份完整的数据进行切分，不同的分片分布到不同的节点上，再通过 Distributed 表引擎把数据拼接起来一同使用  ,本身不存储数据  </p>
<p>分片键,即参数4,常用hiveHash(),可以设置为order by的字段</p>
<h5 id="如何实现高性能update和delete"><a href="#如何实现高性能update和delete" class="headerlink" title="如何实现高性能update和delete"></a>如何实现高性能update和delete</h5><p>update:设置_version字段,插入时该字段自增1,查询时过滤该字段最大值,直接使用replacing merge tree更好??</p>
<p>delete:设置_sign字段,不删除设置0,删除设置1,想要删除时设置1并且设置version字段自增1,然后过滤最大version和删除标记为0的即可</p>
<h5 id="rollup-cube-total的区别"><a href="#rollup-cube-total的区别" class="headerlink" title="rollup,cube,total的区别"></a>rollup,cube,total的区别</h5><p>rollup从右到左去掉维度小计</p>
<p>cube,先从右到左再从左到右,小计</p>
<p>total直接总计</p>
<h5 id="如何导出数据"><a href="#如何导出数据" class="headerlink" title="如何导出数据"></a>如何导出数据</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clickhouse-client --query &quot;select * from t_order_mt where create_time=&#x27;2020-06-01 12:00:00&#x27;&quot; --format CSVWithNames &gt; /opt/module/data/rs1.csv  </span><br></pre></td></tr></table></figure>

<h5 id="clickhouse创建集群表使用uuid"><a href="#clickhouse创建集群表使用uuid" class="headerlink" title="clickhouse创建集群表使用uuid"></a>clickhouse创建集群表使用uuid</h5><p>目的是删除后可以立即创建同样的表名,因为zk节点信息没有那么快去及时删除</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://nfsp412.github.io/asuka.github.io/2024/10/21/bigdata007/" data-id="cm3ae93sm00066curdpwn3t2t" data-title="Clickhouse学习笔记" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/asuka.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag">大数据</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata004" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/asuka.github.io/2024/10/21/bigdata004/" class="article-date">
  <time class="dt-published" datetime="2024-10-21T03:24:38.000Z" itemprop="datePublished">2024-10-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/asuka.github.io/2024/10/21/bigdata004/">Flink学习笔记</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>本文是自己在学习Flink过程中整理的一些基础笔记,仅做记录</p>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><h5 id="flink是什么"><a href="#flink是什么" class="headerlink" title="flink是什么"></a>flink是什么</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">大数据流式处理引擎</span><br><span class="line">有状态的流式处理</span><br><span class="line">低延迟,高吞吐,结果准确,容错性良好,精确一次,高可用,能解决乱序问题</span><br><span class="line">能做到每秒百万条数据,毫秒级延迟</span><br></pre></td></tr></table></figure>

<h5 id="flink的分层api"><a href="#flink的分层api" class="headerlink" title="flink的分层api"></a>flink的分层api</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">flink sql</span><br><span class="line">flink cep</span><br><span class="line">datastream(注意区分spark的dataset和dataframe)</span><br><span class="line">有状态的流处理processfunction</span><br></pre></td></tr></table></figure>

<h5 id="Flink的流批API分别是"><a href="#Flink的流批API分别是" class="headerlink" title="Flink的流批API分别是"></a>Flink的流批API分别是</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">流DataStream</span><br><span class="line">批DataSet</span><br><span class="line">官方推荐流批一体,所以建议都使用流DataStream,可以在提交任务时单独指定批处理</span><br><span class="line">bin/flink run -Dexecution.runtime-mode=BATCH xxx.jar</span><br></pre></td></tr></table></figure>

<h5 id="flink模拟读取无界流数据"><a href="#flink模拟读取无界流数据" class="headerlink" title="flink模拟读取无界流数据"></a>flink模拟读取无界流数据</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">使用nc -lk 7777</span><br><span class="line">使用socketTextStream</span><br></pre></td></tr></table></figure>

<h5 id="如何解释DataFrame的WordCount的结果"><a href="#如何解释DataFrame的WordCount的结果" class="headerlink" title="如何解释DataFrame的WordCount的结果?"></a>如何解释DataFrame的WordCount的结果?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">7&gt; (world,1)</span><br><span class="line">4&gt; (hello,1)</span><br><span class="line">2&gt; (java,1)</span><br><span class="line">4&gt; (hello,2)</span><br><span class="line">4&gt; (hello,3)</span><br><span class="line">10&gt; (flink,1)</span><br></pre></td></tr></table></figure>

<p>体现了流处理,例如hello正是来一条处理一条</p>
<p>前面的编号是?并行度,这里使用IDEA环境下,对应的并行度就是电脑线程数,我这里是12</p>
<h1 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h1><h5 id="集群部署-on-yarn"><a href="#集群部署-on-yarn" class="headerlink" title="集群部署 on yarn"></a>集群部署 on yarn</h5><ul>
<li><p>概述:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">客户端将应用提交给rm,然后rm向nm申请容器资源,会在容器上部署jobmanager和taskmanager的实例,从而启动集群,会根据作业所需要的slot数量动态的分配taskmanager资源</span><br></pre></td></tr></table></figure>
</li>
<li><p>部署流程</p>
<p>略</p>
</li>
<li><p>三种模式的应用</p>
<ul>
<li><p>会话模式</p>
<p>启动集群</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">会话模式,yarn-session.sh</span><br><span class="line">-d 后台运行</span><br><span class="line">-nm 名称</span><br><span class="line">该模式下,yaml配置文件会自动被覆盖,所以可以用standalone模式的安装包</span><br><span class="line">此时8081不再是webUI端口,是yarn随机分配的</span><br><span class="line">启动集群以后如果没有提交作业,没有taskmanager和slot资源,当提交作业以后动态分配资源</span><br><span class="line">相关参数</span><br><span class="line">-jm jm内存</span><br><span class="line">-tm tm内存</span><br><span class="line">-qu yarn队列</span><br><span class="line">-s slots</span><br><span class="line">-j 指定job manager</span><br></pre></td></tr></table></figure>

<p>提交作业</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">提交作业的方式可以在webUI提交,或者命令行提交</span><br><span class="line">作业执行完成后,会资源回收</span><br><span class="line">此时的flink集群就是yarn的一个应用进程,类似于thriftserver服务</span><br><span class="line"></span><br><span class="line">命令行提交作业:bin/flink run -d -c xxx全类名 xxx.jar</span><br><span class="line"></span><br><span class="line">如果此模式下还是想指定到standalone集群,必须在提交作业时使用-m指定集群</span><br><span class="line"></span><br><span class="line">如果提交作业后不能分配到slot资源,可能考虑更换flink-conf.yaml配置文件使用最初默认配置文件,大概率配置了slot但是分配不到资源导致连接超时</span><br></pre></td></tr></table></figure>

<p>取消作业</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">flink list 查看id</span><br><span class="line">flink cancel id</span><br></pre></td></tr></table></figure>

<p>关闭集群</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/tmp/.yarn-properties-root该文件记录了flink作业在yarn集群的id</span><br><span class="line"></span><br><span class="line">关闭session集群的命令行:可以通过webUI关闭集群,或者使用echo &quot;stop&quot; | bin/yarn-session.sh -id application_1700465028835_0002</span><br></pre></td></tr></table></figure>
</li>
<li><p>单作业模式</p>
<p>提交作业</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">单作业模式:直接提交作业,无需预先启动集群资源</span><br><span class="line"></span><br><span class="line">命令行:多一个参数-t yarn-per-job</span><br><span class="line">bin/flink run -d -c xxx全类名 -t yarn-per-job xxx.jar</span><br></pre></td></tr></table></figure>

<p>取消作业</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">如何停止:webUI取消作业,或者命令行使用cancel,停止作业就相当于停止集群资源了</span><br><span class="line"></span><br><span class="line">flink cancel</span><br><span class="line">	bin/flink cancel -t yarn-per-job -Dyarn.application.id=application_1700465028835_0003 f80034defa96f709116e733364ce71b9</span><br><span class="line">	最后要有一个jobid,因为你有可能是yarn session模式存在多个job所以要指定</span><br><span class="line"></span><br><span class="line">如何找到jobid</span><br><span class="line">flink list 比较适用于standalone</span><br><span class="line">	bin/flink list -t yarn-per-job -Dyarn.application.id=application_1700465028835_0003</span><br></pre></td></tr></table></figure>
</li>
<li><p>应用模式</p>
<p>提交作业</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">应用模式,无需启动集群,直接提交作业,并且代码解析放在jobmanager端进行</span><br><span class="line">bin/flink run-application -t yarn-application -c xxx全类名 xxx.jar</span><br><span class="line">直接到后台运行</span><br></pre></td></tr></table></figure>

<p>取消作业</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">如果应用模式:flink cancel -t yarn-application -Dyarn.application.id=xxx jobidxxx</span><br><span class="line"></span><br><span class="line">如何找到jobid</span><br><span class="line">如果应用模式:flink list -t yarn-application -Dyarn.application.id=xxx </span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>异常解决</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">异常:Exception in thread &quot;Thread-5&quot; java.lang.IllegalStateException: Trying to access closed classloader</span><br><span class="line">使用-d会出现,不影响使用</span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="集群部署-on-standalone"><a href="#集群部署-on-standalone" class="headerlink" title="集群部署 on standalone"></a>集群部署 on standalone</h5><ul>
<li><p>概述</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">一般用于开发测试场景,或者作业非常少的情况</span><br></pre></td></tr></table></figure>
</li>
<li><p>部署流程</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">需要配置workers和masters文件</span><br><span class="line">还需要配置flink-conf.yaml</span><br><span class="line">注意:taskmanager.host: hadoop102,该参数需要对应主机进行修改</span><br><span class="line">启动时使用start-cluster,需要到指定job manager节点启动</span><br></pre></td></tr></table></figure>
</li>
<li><p>三种模式的应用</p>
<ul>
<li><p>会话模式</p>
<p>提交作业</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">flink run -m rest_ip:8081 -c xxx全类名 xxx.jar</span><br><span class="line"></span><br><span class="line">注意:</span><br><span class="line">提交成功,但是输出结果还是得去UI界面查看taskmanager</span><br><span class="line">此时ctrl+c不会取消作业,还是得去UI界面取消</span><br><span class="line">Job has been submitted with JobID xxx</span><br></pre></td></tr></table></figure>

<p>取消作业</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">flink list查看id</span><br><span class="line">flink cancel id</span><br></pre></td></tr></table></figure>
</li>
<li><p>单作业模式:standalone不支持</p>
</li>
<li><p>应用模式</p>
<p>提交作业</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">bin/standalone-job.sh start --job-classname xxx全类名</span><br><span class="line"></span><br><span class="line">注意:</span><br><span class="line">jar包必须在lib目录,不然找不到</span><br><span class="line">提交以后到后台执行,进程名称StandaloneApplicationClusterEntryPoint</span><br><span class="line">还需要taskmanager,自己启动:bin/taskmanager.sh start</span><br><span class="line">此时web ui页面能查看,但是你在哪里启动的,对应ip就是哪里,并且此时还没有分配task manager,所以也没有slot显示</span><br><span class="line">注意应该根据你flink-conf.yaml中配置的job manager的地址去对应服务器启动job,因为应用模式启动的就是job manager,所以要去对应的地方启动,此时再启动task manager才能找到对应job manager</span><br></pre></td></tr></table></figure>

<p>取消作业和停止集群</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./bin/standalone-job.sh stop</span><br><span class="line">./bin/taskmanager.sh stop</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h5 id="flink三种部署模式是什么"><a href="#flink三种部署模式是什么" class="headerlink" title="flink三种部署模式是什么?"></a>flink三种部署模式是什么?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">分为会话模式,单作业模式,应用模式</span><br><span class="line">区别在于集群的生命周期以及资源分配的方式,以及main方法在哪里执行,是Client还是jobmanager</span><br><span class="line"></span><br><span class="line">会话模式:</span><br><span class="line">需要先启动集群,保持会话,通过客户端提交作业,所有的作业会竞争集群的资源</span><br><span class="line">适用于单个规模小,执行时间短的大量作业</span><br><span class="line"></span><br><span class="line">单作业模式:</span><br><span class="line">为了更好的隔离资源,使用单作业模式</span><br><span class="line">每个提交的作业,启动一个集群;在提交作业的时候才去启动集群资源;作业执行完成后就会关闭集群释放资源</span><br><span class="line">是实际生产上面的首选模式</span><br><span class="line">代价就是资源必须充沛</span><br><span class="line">需要借助k8s或者yarn执行</span><br><span class="line">perjob在17版本标记过时</span><br><span class="line"></span><br><span class="line">应用模式:</span><br><span class="line">前面两种模式的代码都是在客户端执行,由客户端提交jobmanager,但是需要占用大量网络带宽,也会加重客户端所在节点的资源消耗</span><br><span class="line">不需要客户端,直接把应用提交到jobmanager,为每一个应用创建一个jobmanager,执行结束以后jobmanager就关闭</span><br><span class="line">是新出的用法</span><br><span class="line"></span><br><span class="line">集群生命周期来看:会话模式先启动集群,再提交作业,集群资源不会释放,另外两个提交作业时启动集群,执行完成后关闭集群</span><br><span class="line">main方法执行:会话模式和单作业模式,都是在client解析代码再提交集群,另外的那个是在jobmanager解析代码执行</span><br><span class="line">资源分配:会话模式所有的job去共享资源,另外两个是每个作业单独一份资源</span><br></pre></td></tr></table></figure>

<h5 id="Flink组件架构"><a href="#Flink组件架构" class="headerlink" title="Flink组件架构"></a>Flink组件架构</h5><p>以standalone说明架构图</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">组件:</span><br><span class="line">Client:</span><br><span class="line">	获取代码,解析命令行参数</span><br><span class="line">	生成逻辑流图,即StreamGraph,就是把算子标注出来,并行度标注出来,算子之间的关系标注出来</span><br><span class="line">	转换为作业图,即JobGraph,即合并了算子链</span><br><span class="line">	封装数据,提交给JobManager(通过Actor)</span><br><span class="line">JobManager:</span><br><span class="line">	JobMaster:</span><br><span class="line">		处理单独作业job(一个作业一个)</span><br><span class="line">		从客户端获取任务信息包含jar包,逻辑流图(stream graph 或 data flow),作业图(job graph 进行算子链合并)</span><br><span class="line">		作业图转换成执行图(execution graph 即并行化版本的job graph),即包含了并行度的详细版本</span><br><span class="line">		向RM申请资源</span><br><span class="line">		分发给taskmanager</span><br><span class="line">		检查点工作</span><br><span class="line">	ResourceManager:</span><br><span class="line">		请求slots</span><br><span class="line">		指的是flink的RM而不是YARN的RM(一个集群只有一个)</span><br><span class="line">		资源是TaskManager的任务槽slot</span><br><span class="line">		一个任务task对应一个slot</span><br><span class="line">	Dispatcher:</span><br><span class="line">		启动webui,提交应用</span><br><span class="line">		启动JobMaster</span><br><span class="line">		可以没有</span><br><span class="line">TaskManager:</span><br><span class="line">	数据处理,一个taskmanager包含多个slot,同时一个集群包含多个taskmanager</span><br><span class="line">	向RM进行注册</span><br><span class="line">	提供slot给JobMaster</span><br><span class="line">	和其他TaskManager交换数据</span><br></pre></td></tr></table></figure>

<h5 id="POM文件注意事项"><a href="#POM文件注意事项" class="headerlink" title="POM文件注意事项"></a>POM文件注意事项</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1 建议针对dependence依赖,使用provided</span><br><span class="line">需要在IDEA的run选项的Edit Configuration选项,勾选include dependence with provided scope</span><br><span class="line">或者defaults模板,选择Application,勾选</span><br><span class="line"></span><br><span class="line">2 注意写法:</span><br><span class="line">&lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ServicesResourceTransformer&quot;&gt;</span><br><span class="line">&lt;/transformer&gt;</span><br><span class="line"></span><br><span class="line">3 shade打包注意,需要先clean,再package</span><br></pre></td></tr></table></figure>

<h5 id="历史服务器"><a href="#历史服务器" class="headerlink" title="历史服务器"></a>历史服务器</h5><ul>
<li><p>部署流程</p>
<p>略</p>
</li>
<li><p>注意事项</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">需要在对应服务器启动历史服务</span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="webUI端口是"><a href="#webUI端口是" class="headerlink" title="webUI端口是?"></a>webUI端口是?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">默认端口号8081,可以上传jar包执行,需要配置入口类</span><br></pre></td></tr></table></figure>

<h5 id="并行度有几种设置方式"><a href="#并行度有几种设置方式" class="headerlink" title="并行度有几种设置方式?"></a>并行度有几种设置方式?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">指定并行度有四种方式</span><br><span class="line">优先级从高到低</span><br><span class="line">代码算子指定setParallelism</span><br><span class="line">&gt;</span><br><span class="line">代码全局指定env.setParallelism</span><br><span class="line">&gt;</span><br><span class="line">提交任务指定-p</span><br><span class="line">&gt;</span><br><span class="line">配置文件指定flink-conf.yaml的parallelism.default</span><br></pre></td></tr></table></figure>

<h1 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h1><h5 id="四种流图"><a href="#四种流图" class="headerlink" title="四种流图"></a>四种流图</h5><p>数据流图,类似DAG,web ui可以查看</p>
<p>作业图,经过了算子链合并,</p>
<p>执行图,并行度进行拆分,</p>
<p>物理图,taskmanager根据物理图进行任务执行</p>
<h5 id="算子链"><a href="#算子链" class="headerlink" title="算子链"></a>算子链</h5><p>类比spark,算子种类分为一对一和重分区,类比spark的窄依赖和shuffle</p>
<p>并行度相同而且是一对一,可以连接形成一个大任务,一个大任务使用一个task执行</p>
<p>使用场景:定位问题算子,或者分开两个任务重的算子</p>
<p>大部分场景无需禁用算子链</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">env.disableOperatorChaining();//全局禁用算子链</span><br><span class="line">.disableChaining()//和前后算子断开</span><br><span class="line">.startNewChain()//开启新的算子链,和前面算子断开</span><br></pre></td></tr></table></figure>

<h5 id="共享组"><a href="#共享组" class="headerlink" title="共享组"></a>共享组</h5><p>3 注意,同一个job,不同算子的子任务是可以在同一个slot中运行的,前提是属于同一个slot共享组,默认都是default组,例如下图中map算子的子任务和聚合算子的子任务就可以在同一个slot中运行<br>4 代码中.slotSharingGroup(“AAA”)可以设置指定当前之后的所有算子的共享组</p>
<h5 id="作业提交流程-yarn-单作业模式"><a href="#作业提交流程-yarn-单作业模式" class="headerlink" title="作业提交流程(yarn 单作业模式)"></a>作业提交流程(yarn 单作业模式)</h5><p>on yarn,单作业模式</p>
<p>这种生产常用,以此举例说明</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1 客户端提交任务到yarn的rm,同时上传jar包,配置信息,到hdfs</span><br><span class="line">2 yarn的rm分配container资源,启动job manager,启动的是带有job manager的AM</span><br><span class="line">3 job manager里面的job master组件接收任务,向自身的资源管理器请求资源,</span><br><span class="line">4 自身的资源管理器向yarn集群请求容器资源</span><br><span class="line">5 yarn集群启动带有task manager的容器资源</span><br><span class="line">6 TaskManager 启动之后，向 自身的资源管理器 注册自己的可用任务槽（slots）</span><br><span class="line">7 自身的资源管理器通知 TaskManager 为新的作业提供 slots</span><br><span class="line">8 TaskManager 连接到对应的 JobMaster，提供 slots</span><br><span class="line">9 JobMaster 将需要执行的任务分发给 TaskManager</span><br><span class="line">10 TaskManager 执行任务，互相之间可以交换数据</span><br></pre></td></tr></table></figure>

<h5 id="yarn动态申请资源时申请TM数量如何计算"><a href="#yarn动态申请资源时申请TM数量如何计算" class="headerlink" title="yarn动态申请资源时申请TM数量如何计算?"></a>yarn动态申请资源时申请TM数量如何计算?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">申请的TM数量=job并行度/每个TM的slot数量,向上取整</span><br></pre></td></tr></table></figure>

<h5 id="slot和并行度的关系"><a href="#slot和并行度的关系" class="headerlink" title="slot和并行度的关系"></a>slot和并行度的关系</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">指的是一个TaskManager进程划分为多少个slot使用</span><br><span class="line">1 由于slot均分隔离内存,但是不隔离CPU,所以一般可以配置为CPU核心数</span><br><span class="line">2 配置方式: 在yaml文件中,taskmanager.numberOfTaskSlots参数配置,默认1,即一个taskmanager只有一个slot,这里指的是单个tm配置而不是总数,所以每个服务器的配置都是可以不同的</span><br><span class="line">3 注意,同一个job,不同算子的子任务是可以在同一个slot中运行的,前提是属于同一个slot共享组,默认都是default组,例如下图中map算子的子任务和聚合算子的子任务就可以在同一个slot中运行</span><br><span class="line">4 代码中.slotSharingGroup(&quot;AAA&quot;)可以设置指定当前之后的所有算子的共享组</span><br><span class="line">5 注意一个slot中运行至少一个子任务(或者是算子链形成的一个大的子任务)</span><br><span class="line">6 注意,一个slot中的多个子任务,都是在同时运行的,即使这些子任务之间看起来有先后的顺序,也并不是顺序执行的,如下图map算子子任务和聚合算子子任务实际上并不是先后执行,而是在同一个slot中同时运行的,这也符合flink的代码不动数据流动的思想</span><br><span class="line">7 slot的数量可以配置为cpu核心数,并行度的指定如果对接Kafka可以指定为Kafka分区partition数 </span><br><span class="line"></span><br><span class="line">任务槽slot是静态概念,指的是taskmanager并发的能力,使用taskmanager.numberOfTaskSlots设置</span><br><span class="line">并行度是动态的概念,也就是taskmanager实际使用时的并发能力,通过参数parallelism.default设置</span><br><span class="line"></span><br><span class="line">假设一个三个taskmanager,每个taskmanager有3个slot,则一共9个slot</span><br><span class="line">表示集群最多能并行执行9个同一算子的子任务</span><br><span class="line">我们的wordcount代码,使用了4个算子,而合并算子链以后是3个任务</span><br><span class="line">如果我们默认值,则并行度是1,则流程序总的并行度即为最大算子并行度,也是1,所以只有3个任务,使用一个slot即可</span><br><span class="line"></span><br><span class="line">如果程序并行度大于slot数量,则程序不能运行</span><br><span class="line"></span><br><span class="line">如果是yarn模式,是动态申请资源的,申请的TM数量=job并行度/每个TM的slot数量,向上取整</span><br><span class="line"></span><br><span class="line">页面展示的slot少了几个即并行度指定了几个</span><br><span class="line"></span><br><span class="line">并行度:一个任务拆分成多个子任务分发不同节点进行分布式并行计算,那么子任务的个数就是并行度;一个流的并行度就是所有算子最大的并行度</span><br><span class="line"></span><br><span class="line">任务并行和数据并行</span><br><span class="line"></span><br><span class="line">并行度优先级:算子&gt;env环境&gt;命令行参数-p&gt;配置文件yaml</span><br></pre></td></tr></table></figure>

<h5 id="背压是什么意思"><a href="#背压是什么意思" class="headerlink" title="背压是什么意思?"></a>背压是什么意思?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数据量过大、处理能力不足，短时间堆了大量数据处理不完，产生“背压”（back pressure）。</span><br></pre></td></tr></table></figure>

<h5 id="事件时间是什么意思"><a href="#事件时间是什么意思" class="headerlink" title="事件时间是什么意思?"></a>事件时间是什么意思?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">事件时间，是指每个事件在对应的设备上发生的时间，也就是数据生成的时间。</span><br><span class="line">数据一旦产生，这个时间自然就确定了，所以它可以作为一个属性嵌入到数据中。这其实</span><br><span class="line">就是这条数据记录的“时间戳”（Timestamp）。</span><br></pre></td></tr></table></figure>

<h5 id="水位线是什么意思"><a href="#水位线是什么意思" class="headerlink" title="水位线是什么意思?"></a>水位线是什么意思?</h5><p>水位线 &#x3D; 观察到的最大事件时间 – 最大延迟时间 – 1 毫秒  </p>
<ul>
<li><p>水位线是插入到数据流中的一个标记，可以认为是一个特殊的数据</p>
</li>
<li><p>水位线主要的内容是一个时间戳，用来表示当前事件时间的进展</p>
</li>
<li><p>水位线是基于数据的时间戳生成的</p>
</li>
<li><p>水位线的时间戳必须单调递增，以确保任务的事件时间时钟一直向前推进</p>
</li>
<li><p>水位线可以通过设置延迟，来保证正确处理乱序数据</p>
</li>
<li><p>一个水位线 Watermark(t)，表示在当前流中事件时间已经达到了时间戳 t, 这代表 t 之 前的所有数据都到齐了，之后流中不会出现时间戳 t’ ≤ t 的数据</p>
</li>
<li><p>广播给下游所有并行任务</p>
<p>如果上游有多个并行任务,下游应该选择其中最小的水位线时间戳,即上游多个并行任务都发来自己的水位线,然后下游任务就选择最小的,一旦上游有更新,就判断是否需要更新水位线,如果需要,就发送下游</p>
<p>合流操作类似,也是选择最小的</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">水位线，就是基于事件时间提出的概念</span><br><span class="line">一个数据产生的时刻，就是流处理中事件触发的时间点，这就是“事件时间”</span><br><span class="line">如果我们想要统计一段时间内的数据，需要划分时间窗口，这时只要判断一下时间戳就可以知道数据属于哪个窗口了。</span><br><span class="line">明确了一个数据的所属窗口，还不能直接进行计算。因为窗口处理的是有界数据，我们需要等窗口的数据都到齐了，才能计算出最终的统计结果</span><br><span class="line">我们直接用数据的时间戳来指示当前的时间进展，窗口的关闭自然也是以数据的时间戳等于窗口结束时间为准，这就相当于可以不受网络传输延迟的影响了</span><br><span class="line">但在分布式系统中，这种驱动方式又会有一些问题。因为数据本身在处理转换的过程中会变化，如果遇到窗口聚合这样的操作，其实是要攒一批数据才会输出一个结果，那么下游的数据就会变少，时间进度的控制就不够精细了</span><br><span class="line">另外，数据向下游任务传递时，一般只能传输给一个子任务（除广播外），这样其他的并行子任务的时钟就无法推进了</span><br><span class="line">所以我们应该把时钟也以数据的形式传递出去，告诉下游任务当前时间的进展；而且这个时钟的传递不会因为窗口聚合之类的运算而停滞</span><br><span class="line">这种用来衡量事件时间（Event Time）进展的标记，就被称作“水位线”（Watermark）。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">具体实现上，水位线可以看作一条特殊的数据记录，它是插入到数据流中的一个标记点，</span><br><span class="line">主要内容就是一个时间戳，用来指示当前的事件时间。而它插入流中的位置，就应该是在某个</span><br><span class="line">数据到来之后；这样就可以从这个数据中提取时间戳，作为当前水位线的时间戳了</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">实际应用中，如果当前数据量非常大，可能会有很多数据的时间戳是相同的，这时每来一</span><br><span class="line">条数据就提取时间戳、插入水位线就做了大量的无用功。而且即使时间戳不同，同时涌来的数</span><br><span class="line">据时间差会非常小（比如几毫秒），往往对处理计算也没什么影响。所以为了提高效率，一般</span><br><span class="line">会每隔一段时间生成一个水位线</span><br><span class="line">如何判断?就是判断最近一次数据的事件时间是多少</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">水位线的周期性生成，周期时间是指处理时间（系统时间），而不是事件时间。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">一个 7 秒时产生的数据，生成时间自然要比 9 秒的数据早；但</span><br><span class="line">是经过数据缓存和传输之后，处理任务可能先收到了 9 秒的数据，之后 7 秒的数据才姗姗来迟。</span><br><span class="line">这时如果我们希望插入水位线，来指示当前的事件时间进展，又该怎么做呢</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">只有数据的时间戳比当前时钟大，才能推动时钟前进，这时才插入水位线</span><br><span class="line">如果考虑到大量数据同时到来的处理效率，我们同样可以周期性地生成水位线。这时只需要保存一下之前所有数据中的最大时间戳，需要插入水位线时，就直接以它作为时间戳生成新的水位线，</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这样做尽管可以定义出一个事件时钟，却也会带来一个非常大的问题：我们无法正确处理“迟到”的数据</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">试着多等几秒，也就是把时钟调得更慢一些。最终的目的，就是要让窗口能够把所有迟到数据都收进来，得到正确的计算结果</span><br><span class="line">例如:为了让窗口能够正确收集到迟到的数据，我们也可以等上 2 秒；也就是用当前已有数据的最大时间戳减去 2 秒，就是要插入的水位线的时间戳，这样的话，9 秒的数据到来之后，事件时钟不会直接推进到 9 秒，而是进展到了 7 秒</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">另外需要注意的是，这里一个窗口所收集的数据，并不是之前所有已经到达的数据。因为数据属于哪个窗口，是由数据本身的时间戳决定的，一个窗口只会收集真正属于它的那些数据。也就是说，尽管水位线 W(20)之前有时间戳为 22 的数据到来，10~20 秒的窗口中也不会收集这个数据，进行计算依然可以得到正确的结果</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">水位线看起来比时间戳延迟一些,是因为水位线也是一个时间戳,也是事件驱动的,当第一条数据携带时间戳1000到来时,此时如果打印查看水位线,发现还没到200毫秒触发水位线的时间点,所以还是最小值</span><br><span class="line">当过了200毫秒,此时水位线就变成999了,但是我们看不到这个数字</span><br><span class="line">只有当第二条数据携带时间戳11000到来时,此时打印水位线才能看到999</span><br><span class="line">但是此时还是没有触发第二次水位线的更改,因为水位线都是在数据到来以后根据数据里的时间戳进行触发更改的</span><br><span class="line">所以过了200毫秒,水位线更改为10999,但是我们还是看不到这个数字</span><br><span class="line">只有下一条到来时才能继续看到</span><br></pre></td></tr></table></figure>

<h5 id="窗口是什么"><a href="#窗口是什么" class="headerlink" title="窗口是什么?"></a>窗口是什么?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">窗口,是将无限数据切割成有限的“数据块”进行处理</span><br><span class="line"></span><br><span class="line">窗口,应该理解为桶,来一条数据判断在哪个窗口,然后进入对应的桶</span><br><span class="line"></span><br><span class="line">事件时间窗口的关闭按照水位线的进展来判断</span><br><span class="line"></span><br><span class="line">窗口结束时,触发计算和关闭窗口,这两个操作是可以分开的</span><br><span class="line"></span><br><span class="line">窗口是动态创建的,有对应数据进桶时,进行创建</span><br><span class="line"></span><br><span class="line">窗口同一时间可以有多个,还能够重叠</span><br></pre></td></tr></table></figure>

<h5 id="什么是状态"><a href="#什么是状态" class="headerlink" title="什么是状态"></a>什么是状态</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">像聚合算子,窗口算子,实际上都是有状态的</span><br><span class="line">将这些数据存储在内存中,有容错性,按照key可以进行隔离,避免一个slot多个key之间的状态混乱</span><br><span class="line">可以分布式扩展,状态的重组调整</span><br><span class="line">分为按键分区状态和算子状态</span><br><span class="line">算子状态,只针对当前子任务,和其他并行子任务隔离,需要实现CheckpointedFunction接口,和本地变量一样去使用</span><br><span class="line">按键分区状态,按照key进行隔离+每个子任务和其他子任务隔离</span><br><span class="line">例如value状态,list状态,map状态,聚合状态等等</span><br></pre></td></tr></table></figure>

<h5 id="状态后端是什么"><a href="#状态后端是什么" class="headerlink" title="状态后端是什么"></a>状态后端是什么</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">状态的存储,访问,维护,都是由状态后端这个组件负责</span><br><span class="line">1负责本地状态管理</span><br><span class="line">2负责检查点写入远程,持久化存储</span><br></pre></td></tr></table></figure>

<h5 id="检查点是什么"><a href="#检查点是什么" class="headerlink" title="检查点是什么?"></a>检查点是什么?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">对状态进行持久化保存的快照机制叫作“检查点”（Checkpoint）  </span><br><span class="line"></span><br><span class="line">Flink 对状态进行持久化的方式，就是将当前所有分布式状态进行“快照”保存，写入一个“检查点”（checkpoint）或者保存点（savepoint）保存到外部存储系统中。具体的存储介质，一般是分布式文件系统（distributed file system）</span><br></pre></td></tr></table></figure>

<h1 id="DataStream"><a href="#DataStream" class="headerlink" title="DataStream"></a>DataStream</h1><h2 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h2><h5 id="Source有哪些类型"><a href="#Source有哪些类型" class="headerlink" title="Source有哪些类型"></a>Source有哪些类型</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">集合读取,elements,collection,sequence</span><br><span class="line">文件读取,textFile</span><br><span class="line">从文件读取,使用FileSource(新)</span><br><span class="line">socket读取</span><br><span class="line">kafka读取,即flink作为消费者消费kafka数据</span><br><span class="line">自定义数据源,并行数据源,实现SourceFunction接口,实现run和cancel方法.实现ParallelSourceFunction还能指定并行度</span><br></pre></td></tr></table></figure>

<h5 id="flink的kafkasource的offset消费策略"><a href="#flink的kafkasource的offset消费策略" class="headerlink" title="flink的kafkasource的offset消费策略?"></a>flink的kafkasource的offset消费策略?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">消费策略:OffsetsInitializer类</span><br><span class="line">earliest:一定从最早开始消费</span><br><span class="line">latest:一定从最新开始消费</span><br><span class="line">timestamp:指定消费时间</span><br><span class="line">默认是earliest</span><br><span class="line"></span><br><span class="line">和kafka消费策略不同</span><br><span class="line">earliest:如果有offset,从offset消费,如果没有,从最早消费</span><br><span class="line">latest:如果有offset,从offset消费,如果没有,从最新消费</span><br></pre></td></tr></table></figure>

<h2 id="transfer"><a href="#transfer" class="headerlink" title="transfer"></a>transfer</h2><h5 id="flatmap算子如何使用"><a href="#flatmap算子如何使用" class="headerlink" title="flatmap算子如何使用"></a>flatmap算子如何使用</h5><p>flatmap算子可以返回一条,多条数据,可以实现过滤转换等丰富功能,使用collect收集器向下游发送数据</p>
<p>所以使用场景多</p>
<p>和spark的flatmap或者scala的是不同的</p>
<h5 id="分区算子"><a href="#分区算子" class="headerlink" title="分区算子"></a>分区算子</h5><p>flink使用keyby将数据进行逻辑分区,相同key的数据发送到同一个分区,即同一个slot进行处理,</p>
<p>由于不需要等待所有数据都到齐在计算,所以就不需要使用shuffle</p>
<p>通过key的hash对分区数量取模得到</p>
<h5 id="min和minBy的区别"><a href="#min和minBy的区别" class="headerlink" title="min和minBy的区别?"></a>min和minBy的区别?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">minBy()：与 min()类似，在输入流上针对指定字段求最小值。不同的是，min()只计算指定字段的最小值，其他字段会保留最初第一个数据的值；而 minBy()则会返回包含字段最小值的整条数据。</span><br></pre></td></tr></table></figure>

<h5 id="如何实现全局分区"><a href="#如何实现全局分区" class="headerlink" title="如何实现全局分区"></a>如何实现全局分区</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keyby(x-&gt;true)</span><br></pre></td></tr></table></figure>

<h5 id="flink的富函数类有哪些方法"><a href="#flink的富函数类有哪些方法" class="headerlink" title="flink的富函数类有哪些方法"></a>flink的富函数类有哪些方法</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">使用RichxxxFunction类</span><br><span class="line">获取运行时上下文环境,上下文里面可以获取索引号,状态,任务名称等</span><br><span class="line">有生命周期方法open,调用一次,注意是每个算子子任务执行一次,即并行度设置多个,那么会执行多次的,按照slot区分的,</span><br><span class="line">close方法,做一些清理工作,调用一次,注意是每个算子子任务执行一次,在cancel时执行,异常退出不会执行</span><br></pre></td></tr></table></figure>

<h5 id="flink的物理分区方法有哪些"><a href="#flink的物理分区方法有哪些" class="headerlink" title="flink的物理分区方法有哪些"></a>flink的物理分区方法有哪些</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">物理分区指的是数据真正分配去哪里的意思,逻辑分区指的是按照key进行逻辑划分,是可能数据倾斜的</span><br><span class="line">当数据前后并行度不同的时候实际上就是已经发生了物理分区的</span><br><span class="line"></span><br><span class="line">默认forward,并行度不改变时默认使用</span><br><span class="line">rebalance,轮询分区,按照先后顺序分发数据,nextChannelToSendTo=(nextChannelToSendTo+1)%下游算子并行度</span><br><span class="line">	能够解决数据源的数据倾斜问题</span><br><span class="line">	如果前后的并行度不同,默认调用rebalance方法</span><br><span class="line">shuffle,随机分区,random.nextInt(下游算子并行度)</span><br><span class="line">rescale,类似于rebalance,区别在于分成几个小组,在组内进行轮询发送数据</span><br><span class="line">	应用在数据源存在多个并行数据源的情况下</span><br><span class="line">	注意和rebalance区别</span><br><span class="line">broadcast,数据发送给下游所有子任务</span><br><span class="line">global,全部发送到第一个子任务,并行度的设置就没用了</span><br><span class="line">自定义分区partitionCustom</span><br><span class="line">KeyGroupStreamPartitioner根据key进行两次hash来获取分发的位置</span><br></pre></td></tr></table></figure>

<h5 id="如何上传hdfs提交作业"><a href="#如何上传hdfs提交作业" class="headerlink" title="如何上传hdfs提交作业?"></a>如何上传hdfs提交作业?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">将flink相关依赖事先上传hdfs,这样无需每一次提交作业时,AM即jobmanager都去获取上传一次依赖,jar包也需要上传给jobmanager</span><br><span class="line"></span><br><span class="line">上传lib和plugins到hdfs</span><br><span class="line">[root@hadoop101 ~]# hadoop fs -mkdir /flink-dist</span><br><span class="line">[root@hadoop101 ~]# hadoop fs -put /opt/module/flink-1.17.1/plugins/ /flink-dist</span><br><span class="line">[root@hadoop101 ~]# hadoop fs -put /opt/module/flink-1.17.1/lib/ /flink-dist</span><br><span class="line"></span><br><span class="line">上传jar包到hdfs</span><br><span class="line">[root@hadoop101 ~]# hadoop fs -mkdir /flink-jars</span><br><span class="line">[root@hadoop101 ~]# hadoop fs -put /opt/datas/FlinkModule-1.0-SNAPSHOT.jar /flink-jars</span><br><span class="line"></span><br><span class="line">指定位置提交作业</span><br><span class="line">[root@hadoop101 ~]# flink run-application -t yarn-application -Dyarn.provided.lib.dirs=&quot;hdfs://hadoop101:8020/flink-dist&quot; -c 全类名 hdfs://hadoop101:8020/jar包路径</span><br></pre></td></tr></table></figure>

<h5 id="一个main方法可以生成几个job"><a href="#一个main方法可以生成几个job" class="headerlink" title="一个main方法可以生成几个job?"></a>一个main方法可以生成几个job?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">env.execute();</span><br><span class="line">该方法可以执行多次</span><br><span class="line">但是执行第一个以后就会阻塞</span><br><span class="line">但是如果使用</span><br><span class="line">env.executeAsync();</span><br><span class="line">一个main方法就可以生成多个job</span><br><span class="line"></span><br><span class="line">适用于同一个Source,但是多套不同处理逻辑,还要求写在一起</span><br></pre></td></tr></table></figure>

<h5 id="flink如何实现批处理"><a href="#flink如何实现批处理" class="headerlink" title="flink如何实现批处理"></a>flink如何实现批处理</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">使用datastream开发,执行job时,参数指定BATCH模式</span><br><span class="line"> -Dexecution.runtime-mode=BATCH</span><br></pre></td></tr></table></figure>

<h5 id="flink如何使用POJO类"><a href="#flink如何使用POJO类" class="headerlink" title="flink如何使用POJO类?"></a>flink如何使用POJO类?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">要求是:类是public,属性是public(或者提供getset方法),无参构造器,属性可以序列化;没有非静态的内部类;字段或者属性是非final的;</span><br></pre></td></tr></table></figure>

<h5 id="IDEA如何创建带web界面任务"><a href="#IDEA如何创建带web界面任务" class="headerlink" title="IDEA如何创建带web界面任务"></a>IDEA如何创建带web界面任务</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">常用于测试</span><br><span class="line">无需打包提交集群运行</span><br><span class="line">引入依赖</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-runtime-web&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">使用createLocalEnvironmentWithWebUI</span><br><span class="line"></span><br><span class="line">使用localhost:8081查看</span><br></pre></td></tr></table></figure>

<h5 id="如何解决泛型擦除问题"><a href="#如何解决泛型擦除问题" class="headerlink" title="如何解决泛型擦除问题?"></a>如何解决泛型擦除问题?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// InvalidTypesException 泛型擦除</span><br><span class="line">.returns(Types.TUPLE(Types.STRING,Types.INT))</span><br></pre></td></tr></table></figure>

<h5 id="PVUV指的是什么"><a href="#PVUV指的是什么" class="headerlink" title="PVUV指的是什么"></a>PVUV指的是什么</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PV指的是页面浏览量,即点击次数,不进行去重,就是求count</span><br><span class="line">UV指的是独立访客数,要针对用户去重,再统计次数</span><br></pre></td></tr></table></figure>

<h2 id="常见思路"><a href="#常见思路" class="headerlink" title="常见思路"></a>常见思路</h2><h5 id="如何统计所有用户中访问频次最高的那个"><a href="#如何统计所有用户中访问频次最高的那个" class="headerlink" title="如何统计所有用户中访问频次最高的那个"></a>如何统计所有用户中访问频次最高的那个</h5><p>就是当前访问量最大的用户是谁  </p>
<p>先keyby按照用户分区,然后使用reduce统计每个用户访问频次,然后按照true全局分区,使用reduce统计最大的pv即可</p>
<h5 id="统计-10-秒钟的-url-浏览量，每-5-秒钟更新一次"><a href="#统计-10-秒钟的-url-浏览量，每-5-秒钟更新一次" class="headerlink" title="统计 10 秒钟的 url 浏览量，每 5 秒钟更新一次"></a>统计 10 秒钟的 url 浏览量，每 5 秒钟更新一次</h5><p>使用滑动窗口,窗口大小10秒,步长5秒,(和业务确认是否是这种需求场景)</p>
<p>增量聚合来一条统计一条,全窗口展示包装结果</p>
<h5 id="统计每个用户的-pv，隔一段时间（10s）输出一次结果"><a href="#统计每个用户的-pv，隔一段时间（10s）输出一次结果" class="headerlink" title="统计每个用户的 pv，隔一段时间（10s）输出一次结果"></a>统计每个用户的 pv，隔一段时间（10s）输出一次结果</h5><p>注意不带窗口,所以使用状态实现</p>
<p>使用值状态,一个保存当前的pv值,一个保存定时器的时间戳</p>
<p>首先pv值状态每一条数据就累计一次</p>
<p>然后每次来一条数据就判断定时器时间戳状态是否为空,为空就注册10秒钟定时器,然后更新状态的值变成定时器时间戳</p>
<p>在onTimer方法中,输出结果,并且清空定时器时间戳状态,注意pv值状态不能清空</p>
<h5 id="计算了每个-url-在-10-秒滚动窗口的-pv-指标，每隔-1-秒钟触发一次窗口的计算"><a href="#计算了每个-url-在-10-秒滚动窗口的-pv-指标，每隔-1-秒钟触发一次窗口的计算" class="headerlink" title="计算了每个 url 在 10 秒滚动窗口的 pv 指标，每隔 1 秒钟触发一次窗口的计算"></a>计算了每个 url 在 10 秒滚动窗口的 pv 指标，每隔 1 秒钟触发一次窗口的计算</h5><p>设置10秒滚动窗口,然后设置定时器算子,获取一条数据就先判断状态,如果为空就设置定时器</p>
<p>按照窗口的大小10秒,按照1秒间隔设置定时器</p>
<p>然后在事件时间触发的方法里面进行触发</p>
<p>这样只要数据将事件时间推进到能够触发的时候就会进行触发</p>
<p>当窗口关闭时进行销毁</p>
<h5 id="统计最近10-秒钟内最热门的2个-url-链接，并且每-5-秒钟更新一次"><a href="#统计最近10-秒钟内最热门的2个-url-链接，并且每-5-秒钟更新一次" class="headerlink" title="统计最近10 秒钟内最热门的2个 url 链接，并且每 5 秒钟更新一次"></a>统计最近10 秒钟内最热门的2个 url 链接，并且每 5 秒钟更新一次</h5><p>就是topn场景</p>
<p>这里使用滑动窗口,10秒开窗,并且间隔5秒更新窗口</p>
<p>先正常按照窗口计算,将url,窗口的开始结束时间,统计值,包装pojo类汇总输出,</p>
<p>然后按照true进行分区,再接一个process处理</p>
<p>按照窗口结束时间设置定时器,将来到的数据存入列表状态里面</p>
<p>然后触发定时器的时候,就收集到了窗口内部的所有数据,再排序topn即可</p>
<p>定时器针对相同key相同时间戳去重,</p>
<h5 id="app的支付和第三方的支付进行5秒内的匹配"><a href="#app的支付和第三方的支付进行5秒内的匹配" class="headerlink" title="app的支付和第三方的支付进行5秒内的匹配"></a>app的支付和第三方的支付进行5秒内的匹配</h5><p>用cep</p>
<h5 id="如何实现两条流inner-join-不分窗口"><a href="#如何实现两条流inner-join-不分窗口" class="headerlink" title="如何实现两条流inner join,不分窗口"></a>如何实现两条流inner join,不分窗口</h5><p>两条流按照关联字段key进行分区</p>
<p>使用connect连接到一起</p>
<p>process中,定义两个列表状态,分别存储的是两条流的各自数据</p>
<p>来一条数据就加入列表状态,然后遍历另外一个列表状态进行匹配输出,另外一个处理方法一样</p>
<h5 id="动态规则配置的实现"><a href="#动态规则配置的实现" class="headerlink" title="动态规则配置的实现"></a>动态规则配置的实现</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">在connect时,另一个流是一个广播流,从而实现动态规则配置需求</span><br><span class="line">场景就是,实时变动的规则,用单独的流获取,然后广播下游,下游收到数据会保存为状态,即广播状态</span><br><span class="line">广播状态和广播流的使用</span><br><span class="line">广播状态是一个映射kv类型</span><br><span class="line"></span><br><span class="line">DataStream调用broadcast方法得到广播流,传入描述器,然后将要处理的数据流和广播流进行connect连接,得到广播连接流BroadcastConnectedStream,在这里可以使用广播状态</span><br><span class="line"></span><br><span class="line">注意得到广播流传入的描述器,和connect连接后内部实现类中使用的广播状态,是同一个名字</span><br></pre></td></tr></table></figure>

<h5 id="如何实现10秒滚动窗口统计每个url的pv值"><a href="#如何实现10秒滚动窗口统计每个url的pv值" class="headerlink" title="如何实现10秒滚动窗口统计每个url的pv值"></a>如何实现10秒滚动窗口统计每个url的pv值</h5><p>使用状态</p>
<p>按照url进行keyby</p>
<p>然后在process中,定义map状态</p>
<p>key是窗口的开始时间,value是对应的pv值</p>
<p>每次来一条数据,,注册这个开始时间+10秒-1的定时器,即窗口结束时间的定时器</p>
<p>然后判断是哪个窗口开始时间的,更新pv值+1</p>
<p>定时器触发后onTimer中,输出内容,并且移除该窗口开始时间key对应的值,相当于销毁窗口</p>
<h5 id="算子状态的应用案例"><a href="#算子状态的应用案例" class="headerlink" title="算子状态的应用案例"></a>算子状态的应用案例</h5><p>我们数据源输出到下游系统时,为了能够实现检查点,</p>
<p>,对状态进行持久化保存的快照机制叫作“检查点”（Checkpoint）  </p>
<p>实现了checkpointed Function接口,在snapshotState  方法中,将缓存的数据写入列表状态,即检查点钟</p>
<p>,在initializeState  方法中,如果是判断是故障恢复,即isRestored  ,将状态的数据又写入到缓存数据的集合中</p>
<h2 id="Sink"><a href="#Sink" class="headerlink" title="Sink"></a>Sink</h2><h5 id="sink有哪些类型"><a href="#sink有哪些类型" class="headerlink" title="sink有哪些类型"></a>sink有哪些类型</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">自定义sink,实现SinkFunction类,实现invoke方法</span><br><span class="line">redis,依赖添加bahir,实现RedisSink</span><br><span class="line">es,ElasticsearchSink类</span><br><span class="line">文件系统,使用StreamingFileSink,无需添加额外依赖,可以指定行类型文件或者列存储类型,还可以指定文件的滚动策略,</span><br><span class="line">hbase,需要自定义</span><br><span class="line">kakfa,此时flink作为生产者,该kafka实现了两阶段提交,实现了事务性保证,实现精确一次生产</span><br><span class="line">输出mysql,即jdbc,使用JdbcSink</span><br></pre></td></tr></table></figure>

<h2 id="时间和窗口"><a href="#时间和窗口" class="headerlink" title="时间和窗口"></a>时间和窗口</h2><h5 id="水位线的周期时间指的是"><a href="#水位线的周期时间指的是" class="headerlink" title="水位线的周期时间指的是?"></a>水位线的周期时间指的是?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">指的是生成水位线本身的周期触发时间,是处理时间,默认200毫秒</span><br><span class="line">以对于水位线的周期性生成，周期时间是指处理时间（系统时间），而不是事件时间。默认200毫秒</span><br><span class="line">env.getConfig().setAutoWatermarkInterval(100);单位毫秒</span><br><span class="line">也有基于数据时间或者数据内容生成水位线的策略,断点式</span><br></pre></td></tr></table></figure>

<h5 id="水位线生成的算子和位置"><a href="#水位线生成的算子和位置" class="headerlink" title="水位线生成的算子和位置"></a>水位线生成的算子和位置</h5><p>推荐在数据源中生成水位线,离数据源越近越好</p>
<p>数据源DataStreamSource调用assignTimestampsAndWatermarks设置水位线</p>
<p>也可以在数据源本身当中发送水位线</p>
<p>注意如果在自定义数据源发送水位线,就不能再调用该方法</p>
<h5 id="窗口有哪些分类"><a href="#窗口有哪些分类" class="headerlink" title="窗口有哪些分类?"></a>窗口有哪些分类?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">按照驱动类型分类</span><br><span class="line">时间窗口:按照时间进行截取数据</span><br><span class="line">	事件时间窗口</span><br><span class="line">	系统时间窗口</span><br><span class="line">countWindow计数窗口:按照数据个数进行截取数据</span><br><span class="line"></span><br><span class="line">按照窗口中数据分配的原则进行分类</span><br><span class="line">TumblingEventTimeWindows滚动窗口:将数据均匀的切片划分窗口,参数只有一个,就是定义窗口大小的参数</span><br><span class="line">	这个参数可以是时间,也可以是数据个数</span><br><span class="line">	不存在重叠</span><br><span class="line">	是一种特殊的滑动窗口</span><br><span class="line">SlidingEventTimeWindows滑动窗口:和滚动窗口一样,大小是固定的,所以这是一个参数;但是还有一个参数代表滑动的步长,即下一个窗口开始的时间间隔即为步长</span><br><span class="line">	如果步长等于窗口大小,就变成滚动窗口了</span><br><span class="line">	如果步长大于窗口大小,就变成不相连的窗口了</span><br><span class="line">	一个数据可能属于多个窗口,有重叠的情况下</span><br><span class="line">EventTimeSessionWindows会话窗口:指的是数据间隔多久时间没到,就关闭窗口,参数就是这个间隔时间</span><br><span class="line">	一旦有数据超过这个间隔时间,那么就应该属于下一个会话窗口了,前一个窗口也应该关闭,这意味着会话窗口是不会重叠或者连在一起的</span><br><span class="line">	这个间隔可以静态设置,也可以设置提取器动态设置</span><br><span class="line">	实际上的间隔应该是大于等于我们设置的间隔的,因为就是有可能两个数据的间隔大于这个间隔size</span><br><span class="line">	长度不确定,窗口的起始结束时间不确定</span><br><span class="line">	对于乱序数据,flink是针对每一个数据开启一个会话窗口,然后判断两个数据的窗口之间的距离,是不是小于size,是的话要合并merge</span><br><span class="line">GlobalWindows全局窗口:指的是分组聚合的分组条件key,将相同key的所有数据都划分到一个窗口中去计算</span><br><span class="line">	问题是没有结束,如何计算?需要使用触发器</span><br><span class="line">	计数窗口,实际上就是全局窗口实现的,触发器就规定结束时的数据个数</span><br><span class="line">	</span><br><span class="line">按照key分类</span><br><span class="line">按键分区窗口:指的是先分组keyby,再开窗计算,这样就并行计算,也是符合大多数计算场景的,window</span><br><span class="line">非按键分区窗口:所有数据到同一个task执行,并行度是1,不推荐,windowAll</span><br></pre></td></tr></table></figure>

<h5 id="窗口聚合函数有哪些"><a href="#窗口聚合函数有哪些" class="headerlink" title="窗口聚合函数有哪些"></a>窗口聚合函数有哪些</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">reduce方法和不进行窗口聚合的reduce方法一样,传入ReduceFunction实现类,重写reduce方法,针对两个数据进行聚合,得到的结果再和下一条数据进行聚合,问题在于输入和输出的数据类型一样</span><br><span class="line">aggregate,传入AggregateFunction重写四个方法,三个参数类型都可以不同,输入,累加器,输出都可以不同,第一个方法初始化累加器,第二个方法进行累加器的更新,第三个方法输出结果,第四个方法用来累加器合并(一般在会话窗口时使用),核心就是有状态的流处理</span><br><span class="line"></span><br><span class="line">apply,要被弃用了,用来处理全窗口的数据的,批处理思路,没有累加器,是因为所有数据都必须攒齐了再计算,</span><br><span class="line">process方法,代替apply,传入ProcessWindowFunction,泛型1输入,泛型2输出,泛型3是key,泛型4是窗口信息,重写process方法,</span><br><span class="line">该方法参数1是key,参数2是上下文环境对象,参数3是所有数据的迭代器,参数4是out写出对象</span><br><span class="line">能够获取更丰富的窗口信息,有上下文环境对象</span><br><span class="line"></span><br><span class="line">使用aggregate方法时,传参AggregateFunction和ProcessWindowFunction,这样就相当于结合二者了,代表着还是增量聚合而不是全窗口聚合,但是还能够获取窗口的上下文环境对象;操作的方法是,在AggregateFunction中,将累加器结果通过getResult方法传入ProcessWindowFunction的process方法的element迭代器对象参数,然后使用上下文环境对象对这个结果进行更丰富的操作</span><br></pre></td></tr></table></figure>

<h5 id="flink如何处理迟到数据"><a href="#flink如何处理迟到数据" class="headerlink" title="flink如何处理迟到数据?"></a>flink如何处理迟到数据?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">水位线,一般设置毫秒级别</span><br><span class="line"></span><br><span class="line">允许延迟allowedLateness算子,指的是窗口不关闭,数据可以继续进入窗口触发计算,只有水位线推进到窗口结束时间加延迟时间在关闭,会至少先触发一次窗口计算,然后后面继续更新结果,这证明窗口的触发计算和清除销毁是可以分开的</span><br><span class="line">注意迟到数据每一次到来一条就会触发一次计算的,因为本身已经满足触发条件了</span><br><span class="line">这就是Lambda架构的思想</span><br><span class="line">分钟级别</span><br><span class="line"></span><br><span class="line">侧输出流,窗口关闭处理结果已经统计,但是额外再针对侧输出流数据进行计算再进行合并,就回归正确了</span><br></pre></td></tr></table></figure>

<h5 id="聚合函数有哪些"><a href="#聚合函数有哪些" class="headerlink" title="聚合函数有哪些"></a>聚合函数有哪些</h5><p>如果是不开窗,直接分组聚合,那么可以使用min,minBy,max,maxBy,sum,reduce,process</p>
<p>如果进行了开窗,可以使用额外的apply,aggregate</p>
<h5 id="窗口时间和水位线时间"><a href="#窗口时间和水位线时间" class="headerlink" title="窗口时间和水位线时间"></a>窗口时间和水位线时间</h5><p>水位线&#x3D;观察的最大事件时间-延迟-1毫秒</p>
<p>窗口时间,左开右闭的</p>
<p>如果窗口5秒,那么5秒数据到来时可以触发窗口计算和关闭,此时的水位线是4999,而窗口的数据不包含这条五秒钟的数据</p>
<p>窗口内最大时间戳,指的也不是水位线,指的就是单纯的窗口结束时间-1毫秒</p>
<p>水位线和事件时间相关,指的是当前事件时间-延迟-1毫秒</p>
<h5 id="全局窗口和非按键分区窗口"><a href="#全局窗口和非按键分区窗口" class="headerlink" title="全局窗口和非按键分区窗口"></a>全局窗口和非按键分区窗口</h5><p>全局窗口指的是GlobalWindows  ,调用的window算子中传入的窗口函数的一种,按照key将数据全部发送的各自窗口中,没有窗口的结束,所以要结合触发器算子使用,例如计数窗口就是这样实现的</p>
<p>非按键分区窗口:所有数据到同一个task执行,并行度是1,不推荐,windowAll算子</p>
<h1 id="Process-Function"><a href="#Process-Function" class="headerlink" title="Process Function"></a>Process Function</h1><h5 id="ProcessFunction有哪些分类"><a href="#ProcessFunction有哪些分类" class="headerlink" title="ProcessFunction有哪些分类?"></a>ProcessFunction有哪些分类?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">基于DataStream调用ProcessFunction 常用</span><br><span class="line">基于KeyedStream调用KeyedProcessFunction 常用</span><br><span class="line">基于WindowedStream调用ProcessWindowFunction,他也是我们提到的全窗口函数 常用</span><br><span class="line">基于DataStream直接开窗WindowAll得到AllWindowedStream,调用ProcessAllWindowFunction</span><br><span class="line">connect合流以后得到ConnectedStreams调用  CoProcessFunction</span><br><span class="line">interval join间隔连接以后得到IntervalJoined调用  ProcessJoinFunction</span><br><span class="line">基于BroadcastConnectedStream调用  BroadcastProcessFunction</span><br><span class="line">	指的是未进行KeyBy的DataStream和广播流BroadcastStream做连接,得到广播连接流</span><br><span class="line">基于BroadcastConnectedStream调用 KeyedBroadcastProcessFunction</span><br><span class="line">	指的是进行KeyBy的KeyedStream和广播流BroadcastStream做连接,得到广播连接流</span><br></pre></td></tr></table></figure>

<h5 id="定时服务是什么"><a href="#定时服务是什么" class="headerlink" title="定时服务是什么?"></a>定时服务是什么?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">在KeyedProcessFunction中,可以使用定时器和定时服务</span><br><span class="line">使用定时服务timer service来注册或者删除定时器,</span><br><span class="line">定时器分为事件时间和处理时间两种语义</span><br><span class="line">当定时器设置的时间触发时,在onTimer中可以收到并进行处理</span><br><span class="line">定时器会按照key和设置的时间戳进行去除,所以重复设置也只会在onTimer中触发一次</span><br><span class="line">时间戳按照毫秒进行区分,</span><br><span class="line">由于onTimer和processElement方法存在同步,所以不用担心出现并发修改问题</span><br><span class="line">定时器有容错性,可以保存在检查点</span><br></pre></td></tr></table></figure>

<h5 id="为什么水位线看起来比时间戳延迟一些"><a href="#为什么水位线看起来比时间戳延迟一些" class="headerlink" title="为什么水位线看起来比时间戳延迟一些"></a>为什么水位线看起来比时间戳延迟一些</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">水位线看起来比时间戳延迟一些,是因为水位线也是一个时间戳,也是事件驱动的,当第一条数据携带时间戳1000到来时,此时如果打印查看水位线,发现还没到200毫秒触发水位线的时间点,所以还是最小值</span><br><span class="line">当过了200毫秒,此时水位线就变成999了,但是我们看不到这个数字</span><br><span class="line">只有当第二条数据携带时间戳11000到来时,此时打印水位线才能看到999</span><br><span class="line">但是此时还是没有触发第二次水位线的更改,因为水位线都是在数据到来以后根据数据里的时间戳进行触发更改的</span><br><span class="line">所以过了200毫秒,水位线更改为10999,但是我们还是看不到这个数字</span><br><span class="line">只有下一条到来时才能继续看到</span><br></pre></td></tr></table></figure>

<h2 id="合流操作"><a href="#合流操作" class="headerlink" title="合流操作"></a>合流操作</h2><h5 id="flink如何进行合流操作"><a href="#flink如何进行合流操作" class="headerlink" title="flink如何进行合流操作?"></a>flink如何进行合流操作?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">union,数据流的合并,要求数据类型相同,可以合并多条流</span><br><span class="line">	水位线:按照最小的为准</span><br><span class="line">	可以使用map.flatmap,filter,process等</span><br><span class="line">connect,数据流的连接,数据类型可以不同,返回的是ConnectedStreams,不能直接打印,一次只能连接一条流,对应的map,flatmap,process算子,使用的是CoMapFunction,CoFlatMapFunction,coProcessFunction等</span><br></pre></td></tr></table></figure>

<h5 id="flink如何实现分流操作"><a href="#flink如何实现分流操作" class="headerlink" title="flink如何实现分流操作?"></a>flink如何实现分流操作?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">process结合侧输出流</span><br><span class="line">实际应用中,例如ETL,只选择自己需要的数据去Kafka消费,其他数据通过侧输出流发送到Kafka其他标签</span><br><span class="line">或者在侧输出流提示告警信息</span><br><span class="line"></span><br><span class="line">使用侧输出流,是底层processFunction的处理方式</span><br><span class="line"></span><br><span class="line">使用ctx.output输出侧输出流数据</span><br><span class="line">使用out.collect输出主流数据</span><br><span class="line"></span><br><span class="line">定义侧输出流标签时,使用&#123;&#125;方便提取泛型</span><br><span class="line"></span><br><span class="line">使用getSideOutput来获取指定的侧输出流的数据</span><br></pre></td></tr></table></figure>

<h5 id="flink的窗口join有哪些"><a href="#flink的窗口join有哪些" class="headerlink" title="flink的窗口join有哪些"></a>flink的窗口join有哪些</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">可以理解为基于时间的合流connect操作</span><br><span class="line">实际上是更为顶层化的算子,不必从底层开始写</span><br><span class="line"></span><br><span class="line">分类:窗口联结window join,间隔连接Interval join,窗口组联结coGroup</span><br><span class="line"></span><br><span class="line">窗口联结使用方式:</span><br><span class="line">	两条DataStream流,调用join,然后使用where,where指定第一条流的key,equalTo,equalTo指定第二条流的key,进行连接,然后使用window开窗,使用apply设置窗口函数</span><br><span class="line">	就相当于select * from tb1 join tb2 on tb1.id=tb2.id;</span><br><span class="line">	apply窗口函数去实现JoinFunction类或者FlatJoinFunction类</span><br><span class="line">	Window和apply之间可以使用触发器,允许延迟等算子</span><br><span class="line">	每一对匹配的数据进入apply的实现类的join方法,有返回值,flat版本的实现类可以没有返回值,更灵活</span><br><span class="line">	做笛卡尔积</span><br><span class="line">	</span><br><span class="line">间隔连接使用方式:</span><br><span class="line">	间隔连接指的是针对数据开辟前后的一段时间间隔,看在这段时间内是否有另一条流的数据过来匹配上,</span><br><span class="line">	默认是闭区间,边界值能够匹配上</span><br><span class="line">	先keyby,然后intervalJoin,里面的流也要keyby,再between确定范围,在process处理,实现ProcessJoinFunction类</span><br><span class="line">	每检测到一组匹配的数据就调用一次processElement方法</span><br><span class="line">	keyby intervaljoin between process</span><br><span class="line">	仅仅支持事件时间,内连接</span><br><span class="line">	</span><br><span class="line">窗口组联结使用方式:</span><br><span class="line">	是更加通用的window join,window join底层使用的就是cogroup</span><br><span class="line">	可以实现外连接满外连接,更通用</span><br><span class="line">	使用方式把join换成coGroup即可</span><br><span class="line">	把窗口内数据放入集合中,一次性调用一次coGroup方法</span><br></pre></td></tr></table></figure>

<h2 id="状态编程"><a href="#状态编程" class="headerlink" title="状态编程"></a>状态编程</h2><h5 id="按键分区状态有哪些-有哪些方法"><a href="#按键分区状态有哪些-有哪些方法" class="headerlink" title="按键分区状态有哪些,有哪些方法"></a>按键分区状态有哪些,有哪些方法</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">必须基于keyed stream</span><br><span class="line"></span><br><span class="line">必须状态描述器 参数1状态数据的名称,参数2状态保存数据的类型</span><br><span class="line"></span><br><span class="line">值状态ValueState接口,value获取值,update更新值,还需要有状态描述器ValueStateDescriptor	</span><br><span class="line">	每一个key的状态从null开始不断进行更新</span><br><span class="line">	value和update方法</span><br><span class="line"></span><br><span class="line">列表状态ListState接口,add添加值,update更新,get获取迭代器,状态描述器ListStateDescriptor</span><br><span class="line">	每一个key的状态从空列表开始,列表会一直追加状态数据</span><br><span class="line">	add和get方法,update</span><br><span class="line"></span><br><span class="line">映射状态MapState接口,get获取key对应value,put更新,remove移除,等</span><br><span class="line">	get和put方法,remove,等等</span><br><span class="line">	key,value形式存储数据</span><br><span class="line"></span><br><span class="line">归约状态ReducingState,描述器是ReducingStateDescriptor,类似于值状态,但是需要针对所有值进行归约</span><br><span class="line">	add和get方法进行使用</span><br><span class="line">	输入和输出类型必须一致</span><br><span class="line"></span><br><span class="line">聚合状态AggregatingState,和归约状态区别在于,是更加一般化的聚合函数</span><br><span class="line">	add和get方法使用</span><br><span class="line">	输入和输出类型可以不一致,和中间值也可以不一致</span><br></pre></td></tr></table></figure>

<h5 id="本地变量和按键分区状态变量的区别"><a href="#本地变量和按键分区状态变量的区别" class="headerlink" title="本地变量和按键分区状态变量的区别?"></a>本地变量和按键分区状态变量的区别?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">本地变量无法根据key进行隔离操作</span><br><span class="line">状态变量则能够进行key进行隔离操作</span><br></pre></td></tr></table></figure>

<h5 id="ReducingState和AggregatingState的区别"><a href="#ReducingState和AggregatingState的区别" class="headerlink" title="ReducingState和AggregatingState的区别?"></a>ReducingState和AggregatingState的区别?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">AggregatingState是更加一般化通用化的聚合状态</span><br><span class="line">输入,中间值,输出,都可以不一致</span><br></pre></td></tr></table></figure>

<h5 id="状态的生存时间TTL"><a href="#状态的生存时间TTL" class="headerlink" title="状态的生存时间TTL"></a>状态的生存时间TTL</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">即TTL,状态可能会越来越多,那么就需要进行clear清理,或者设置TTL</span><br><span class="line">在open方法中,创建了状态以后,就可以设置TTL</span><br><span class="line">setUpdateType,代表什么时候更新状态的TTL,默认是创建状态和写操作时会更新TTL</span><br><span class="line">setStateVisibility,由于状态的清除并不是实时操作的,所以到了过期时间以后还是可能继续存在,那么此时能不能获取状态呢?,默认是从不返回过期值,也就是只要过期就认为被清除状态,不能获取</span><br><span class="line"></span><br><span class="line">只支持处理时间</span><br><span class="line">针对每一项状态进行设置,例如集合或者映射状态</span><br></pre></td></tr></table></figure>

<h5 id="算子状态有哪些"><a href="#算子状态有哪些" class="headerlink" title="算子状态有哪些"></a>算子状态有哪些</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">一个算子并行子任务上面定义的状态,和key无关</span><br><span class="line">一般用在source和sink上面,例如自定义Sink或者source时,为了保证故障恢复使用算子状态</span><br><span class="line">例如kafka消费,维护的偏移量,保证精确一次</span><br><span class="line"></span><br><span class="line">类型有哪些:</span><br><span class="line">列表状态ListState</span><br><span class="line">	不会按照key分开,所以一个并行子任务只有一个列表状态</span><br><span class="line">	并行度改变时,按照轮询方式,将所有子任务合并成一个大列表的状态,然后分发</span><br><span class="line">	算子状态没有键组的概念,所以一个并行子任务只有一个大的状态,所以没办法按照键组的形式进行重分配,所以算子状态中,没有值状态的概念,因为一旦只有一个值状态,那就没办法进行重新分配了,只有最基本的列表状态,才能在合并一起以后再进行重新分配</span><br><span class="line">	</span><br><span class="line">联合列表状态UnionListState</span><br><span class="line">	区别在于,并行度缩放时,分配策略不同</span><br><span class="line">	会把整个状态的大列表广播给下游所有子任务,子任务自行选择使用哪些丢弃哪些,资源和效率考虑不建议使用</span><br><span class="line">	</span><br><span class="line">广播状态BroadcastState</span><br><span class="line">	必须基于广播流使用</span><br><span class="line">	每一个并行子任务,都有一份相同的广播状态</span><br><span class="line"></span><br><span class="line">对于算子状态的使用,发生故障恢复时,比较麻烦,因为没有key,不知道去下游哪个分区,发生故障重启之后，我们不能保证某个数据跟之前一样，进入到同一个并行子任务、访问同一个状态,所以需要实现CheckpointedFunction更为底层的接口</span><br></pre></td></tr></table></figure>

<h5 id="状态后端的分类"><a href="#状态后端的分类" class="headerlink" title="状态后端的分类"></a>状态后端的分类</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">默认哈希表状态后端</span><br><span class="line"></span><br><span class="line">1哈希表状态后端</span><br><span class="line">状态存储在内存,把状态当做对象存储JVM的堆上,普通状态,窗口数据,触发器,都按照kv存储,底层是一个哈希表</span><br><span class="line">检查点的保存,一般放在分布式文件系统</span><br><span class="line">性能最佳,耗费内存</span><br><span class="line"></span><br><span class="line">2内嵌RocksDB状态后端</span><br><span class="line">kv类型数据库,持久化到该数据库,存储本地硬盘</span><br><span class="line">适用于状态很大,窗口周期长,键值对状态很大的场景</span><br><span class="line">支持增量检查点的状态后端</span><br><span class="line"></span><br><span class="line">最大的区别在于本地状态存放哪里,第一种存储内容,第二种存储数据库RocksDB</span><br></pre></td></tr></table></figure>

<h1 id="容错机制"><a href="#容错机制" class="headerlink" title="容错机制"></a>容错机制</h1><h5 id="检查点和保存点的区别"><a href="#检查点和保存点的区别" class="headerlink" title="检查点和保存点的区别?"></a>检查点和保存点的区别?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">保存点是手动触发的,建议使用uid方法为每一个算子指定id</span><br><span class="line">检查点是自动的</span><br></pre></td></tr></table></figure>

<h5 id="flink-kafka如何保证精确一次"><a href="#flink-kafka如何保证精确一次" class="headerlink" title="flink+kafka如何保证精确一次"></a>flink+kafka如何保证精确一次</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">flink端,开启检查点并设置精确一次</span><br><span class="line"></span><br><span class="line">flink kafka consumer端,没有特殊的设置</span><br><span class="line">	如果读一个设置了精确一次的主题,需要添加ConsumerConfig.ISOLATION_LEVEL_CONFIG为read_committed</span><br><span class="line"></span><br><span class="line">flink kafka producer端,</span><br><span class="line">	ProducerConfig.TRANSACTIONAL_ID_CONFIG事务id</span><br><span class="line">	FlinkKafkaProducer.Semantic.EXACTLY_ONCE精确一次</span><br><span class="line">	ProducerConfig.TRANSACTION_TIMEOUT_CONFIG小于15min</span><br></pre></td></tr></table></figure>

<h1 id="Table-API-SQL"><a href="#Table-API-SQL" class="headerlink" title="Table API &amp; SQL"></a>Table API &amp; SQL</h1><h5 id="表环境有什么用处"><a href="#表环境有什么用处" class="headerlink" title="表环境有什么用处"></a>表环境有什么用处</h5><p>表环境,指的是tableEnvironment</p>
<p>可以注册catalog和表,useCatalog,useDatabase,createTemporaryView  </p>
<p>可以执行sql查询语句,executeSql,sqlQuery</p>
<p>可以注册udf函数,createTemporaryFunction</p>
<p>可以将dataStream和表table之间进行转换,fromDataStream,toDataStream等</p>
<p>设置参数,例如ttl之类,getConfig</p>
<h5 id="创建表-视图的语法"><a href="#创建表-视图的语法" class="headerlink" title="创建表&#x2F;视图的语法"></a>创建表&#x2F;视图的语法</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">1 创建连接器表,即输入source和输出sink的表</span><br><span class="line">create table if not exists xxx.xxx.xxx (</span><br><span class="line">    --常规列</span><br><span class="line">    user string,</span><br><span class="line">    --元数据列</span><br><span class="line">    record_time timestamp_ltz(3) metadata from &#x27;timestamp&#x27;,--kafka标记的时间戳</span><br><span class="line">    --计算列</span><br><span class="line">    cost as f1 * f2, --f1,f2是两个常规列</span><br><span class="line">    --定义水位线</span><br><span class="line">    watermark for ts as ts - interval &#x27;xxx&#x27; second,</span><br><span class="line">    --主键</span><br><span class="line">    primary key(xxx) not enforce</span><br><span class="line">)</span><br><span class="line">with(</span><br><span class="line">	k=v,...</span><br><span class="line">) ;</span><br><span class="line"></span><br><span class="line">2 like创建表,快速复制表使用</span><br><span class="line">create table xxx (增加的列)</span><br><span class="line">with (增加的/覆盖的属性kv)</span><br><span class="line">like xxx;</span><br><span class="line"></span><br><span class="line">3 ctas创建表,一般用来创建中间过程当中的临时视图view</span><br><span class="line">create view xxx</span><br><span class="line">with (xxx)//一般没加这个</span><br><span class="line">as select xxx from xxx;</span><br></pre></td></tr></table></figure>

<h5 id="表和流的转换"><a href="#表和流的转换" class="headerlink" title="表和流的转换"></a>表和流的转换</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">流转换成表,两种方式,追加查询和更新更新查询,更新查询当中的编码有insert,update_before,update_after,delete,要结合row类型和rowKind使用</span><br><span class="line">使用fromDataStream,转换成Table表,再使用createTemporaryView</span><br><span class="line">推荐直接使用:或者直接使用createTemporaryView,第一个依然是注册的表名，而第二个可以直接就是DataStream。之后仍旧可以传入多个参数，用来指定表中的字段</span><br><span class="line">使用$需要导包,$(&quot;id&quot;)这样使用,可以使用as</span><br><span class="line">fromChangelogStream将一个更新日志流转换成表,要求流中的数据类型只能是 Row,而且每一个数据都需要指定当前行的更新类型（RowKind）详见代码</span><br><span class="line"></span><br><span class="line">表转换成流,仅追加流,撤回流,更新插入流,区别在于更新插入流需要有一个主键key,根据key判断是插入还是更新</span><br><span class="line">toDataStream 仅插入流</span><br><span class="line">toChangelogStream 有更新操作的表(更新日志流),仅插入流</span><br><span class="line">在api代码中使用,是没有办法调用相关方法转换成更新插入流的,这种流一般是提供给外部系统使用的,例如Kafka就能够实现更新插入流</span><br></pre></td></tr></table></figure>

<h5 id="Row类型如何使用"><a href="#Row类型如何使用" class="headerlink" title="Row类型如何使用"></a>Row类型如何使用</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Row.of定义Row</span><br><span class="line">RowKind定义编码类型</span><br><span class="line"></span><br><span class="line">可以在创建dataStream时使用,例如下面</span><br><span class="line">Row.ofKind(RowKind.INSERT, &quot;Alice&quot;, 12)</span><br></pre></td></tr></table></figure>

<h5 id="事件时间和处理时间如何定义"><a href="#事件时间和处理时间如何定义" class="headerlink" title="事件时间和处理时间如何定义"></a>事件时间和处理时间如何定义</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">事件时间,定义表时指定</span><br><span class="line">ts BIGINT,</span><br><span class="line">ts_ltz AS TO_TIMESTAMP_LTZ(ts, 3),</span><br><span class="line">WATERMARK FOR ts_ltz AS time_ltz - INTERVAL &#x27;5&#x27; SECOND</span><br><span class="line"></span><br><span class="line">处理时间,定义表时指定</span><br><span class="line">ts AS PROCTIME()</span><br></pre></td></tr></table></figure>

<h5 id="窗口如何定义"><a href="#窗口如何定义" class="headerlink" title="窗口如何定义"></a>窗口如何定义</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">  id,</span><br><span class="line">  window_start, --窗口开始时间</span><br><span class="line">  window_end,</span><br><span class="line">  sum(vc) as s</span><br><span class="line">from table(</span><br><span class="line">  cumulate (table ws, descriptor(et), interval &#x27;5&#x27; minute, interval &#x27;1&#x27; hour)累计</span><br><span class="line">  TUMBLE(table ws,descriptor(et),interval &#x27;1&#x27; hour)滚动</span><br><span class="line">  hop(table ws,descriptor(et),interval &#x27;5&#x27; minutes,interval &#x27;1&#x27; hour)滑动</span><br><span class="line">)</span><br><span class="line">group by id,</span><br><span class="line">window_start,window_end --可选</span><br></pre></td></tr></table></figure>

<h5 id="连接器种类有哪些"><a href="#连接器种类有哪些" class="headerlink" title="连接器种类有哪些"></a>连接器种类有哪些</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">输出</span><br><span class="line">&#x27;connector&#x27;=&#x27;print&#x27;</span><br><span class="line"></span><br><span class="line">快速生成数据集</span><br><span class="line">&#x27;connector&#x27;=&#x27;datagen&#x27;,</span><br><span class="line">&#x27;rows-per-second&#x27;=&#x27;1&#x27;, --每秒条数</span><br><span class="line">&#x27;fields.ts.kind&#x27;=&#x27;sequence&#x27;, --序列类型</span><br><span class="line">&#x27;fields.ts.start&#x27;=&#x27;1&#x27;,</span><br><span class="line">&#x27;fields.ts.end&#x27;=&#x27;1000000&#x27;,</span><br><span class="line">&#x27;fields.vc.kind&#x27;=&#x27;random&#x27;, --随机数类型</span><br><span class="line">&#x27;fields.vc.min&#x27;=&#x27;1&#x27;,</span><br><span class="line">&#x27;fields.vc.max&#x27;=&#x27;100&#x27;</span><br><span class="line">&#x27;fields.dim.length&#x27;=&#x27;1&#x27; --字段值长度</span><br><span class="line"></span><br><span class="line">连接jdbc</span><br><span class="line">&#x27;connector&#x27;=&#x27;jdbc&#x27;,</span><br><span class="line">&#x27;url&#x27;=&#x27;jdbc:mysql://hadoop101:3306/customerdb&#x27;,</span><br><span class="line">&#x27;table-name&#x27;=&#x27;customers&#x27;</span><br><span class="line"></span><br><span class="line">连接kafka</span><br><span class="line">  &#x27;connector&#x27; = &#x27;kafka&#x27;,</span><br><span class="line">  &#x27;topic&#x27; = &#x27;user_behavior&#x27;,</span><br><span class="line">  &#x27;properties.bootstrap.servers&#x27; = &#x27;localhost:9092&#x27;,</span><br><span class="line">  &#x27;properties.group.id&#x27; = &#x27;testGroup&#x27;,</span><br><span class="line">  &#x27;scan.startup.mode&#x27; = &#x27;earliest-offset&#x27;,</span><br><span class="line">  &#x27;format&#x27; = &#x27;csv&#x27;</span><br><span class="line">  </span><br><span class="line">连接upsert kafka</span><br><span class="line">创建表时必须指定主键,并且不支持offset,因为都是得从头读取的</span><br><span class="line">主键的选择就可以选择分组字段,支持联合主键</span><br><span class="line">  &#x27;connector&#x27; = &#x27;upsert-kafka&#x27;,</span><br><span class="line">  &#x27;properties.bootstrap.servers&#x27; = &#x27;hadoop102:9092&#x27;,</span><br><span class="line">  &#x27;topic&#x27; = &#x27;ws2&#x27;,</span><br><span class="line">  &#x27;key.format&#x27; = &#x27;json&#x27;,</span><br><span class="line">  &#x27;value.format&#x27; = &#x27;json&#x27;</span><br><span class="line"></span><br><span class="line">文件系统</span><br><span class="line">  &#x27;connector&#x27; = &#x27;filesystem&#x27;,</span><br><span class="line">  &#x27;path&#x27; = &#x27;hdfs://hadoop102:8020/data/t3&#x27;,</span><br><span class="line">  &#x27;format&#x27; = &#x27;csv&#x27;</span><br><span class="line"></span><br><span class="line">jdbc</span><br><span class="line">建表时指定主键则按照upsert操作,没有指定主键则按照insert操作</span><br><span class="line">&#x27;connector&#x27;=&#x27;jdbc&#x27;,</span><br><span class="line">    &#x27;url&#x27; = &#x27;jdbc:mysql://hadoop102:3306/test?useUnicode=true&amp;characterEncoding=UTF-8&#x27;,</span><br><span class="line">    &#x27;username&#x27; = &#x27;root&#x27;,</span><br><span class="line">    &#x27;password&#x27; = &#x27;000000&#x27;,</span><br><span class="line">    &#x27;connection.max-retry-timeout&#x27; = &#x27;60s&#x27;,</span><br><span class="line">    &#x27;table-name&#x27; = &#x27;ws2&#x27;,</span><br><span class="line">    &#x27;sink.buffer-flush.max-rows&#x27; = &#x27;500&#x27;,</span><br><span class="line">    &#x27;sink.buffer-flush.interval&#x27; = &#x27;5s&#x27;,</span><br><span class="line">    &#x27;sink.max-retries&#x27; = &#x27;3&#x27;,</span><br><span class="line">    &#x27;sink.parallelism&#x27; = &#x27;1&#x27;</span><br><span class="line"></span><br><span class="line">连接hbase做lookup join</span><br><span class="line">指定主键,指定rowkey,指定f1 row&lt;&gt;等</span><br><span class="line">&#x27;connector&#x27;=&#x27;hbase14&#x27;</span><br><span class="line">&#x27;lookup.cache-type&#x27;=&#x27;None&#x27;</span><br><span class="line">&#x27;lookup.parallelism&#x27;=&#x27;1&#x27;</span><br><span class="line">&#x27;lookup.cache.ttl&#x27;=&#x27;60000&#x27;</span><br><span class="line">&#x27;lookup.cache-max-rows&#x27;=&#x27;10000&#x27;</span><br><span class="line">&#x27;table-name&#x27;=&#x27;xx:xxx&#x27;</span><br><span class="line">&#x27;zookeeper.znode.parent&#x27;=&#x27;/xxx&#x27;</span><br><span class="line">&#x27;zookeeper.quorum&#x27;=&#x27;xxx:2181,...&#x27;</span><br></pre></td></tr></table></figure>

<h5 id="sql客户端如何使用"><a href="#sql客户端如何使用" class="headerlink" title="sql客户端如何使用?"></a>sql客户端如何使用?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">启动了flink集群后(yarn或者standalone模式)</span><br><span class="line">embedded可以省略</span><br><span class="line">使用bin/sql-client embedded -s yarn-session (基于yarn session模式下)</span><br></pre></td></tr></table></figure>

<h5 id="sql客户端的常用配置"><a href="#sql客户端的常用配置" class="headerlink" title="sql客户端的常用配置"></a>sql客户端的常用配置</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">结果的显式模式:changelog,tableau,table(默认)</span><br><span class="line">set sql-client.execution.result-mode=changelog;</span><br><span class="line"></span><br><span class="line">执行环境的设置: streaming(默认) batch</span><br><span class="line">set execution.runtime-mode=streaming;</span><br><span class="line"></span><br><span class="line">设置并行度</span><br><span class="line">set parallelism.default=1;</span><br><span class="line"></span><br><span class="line">设置状态的ttl时间,单位毫秒</span><br><span class="line">set table.exec.state.ttl=1000;</span><br></pre></td></tr></table></figure>

<h5 id="如何执行sql文件"><a href="#如何执行sql文件" class="headerlink" title="如何执行sql文件"></a>如何执行sql文件</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">使用bin/sql-client embedded -s yarn-session (基于yarn session模式下) -i sql文件路径</span><br></pre></td></tr></table></figure>

<h5 id="topn和去重的实现方式有啥区别"><a href="#topn和去重的实现方式有啥区别" class="headerlink" title="topn和去重的实现方式有啥区别"></a>topn和去重的实现方式有啥区别</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">6 支持固定格式实现topN</span><br><span class="line">topN对应的查询是更新查询,因为topN的数据一直在动态变化,所以涉及到先删除再插入</span><br><span class="line">SELECT </span><br><span class="line">	user, url, ts, row_num</span><br><span class="line">FROM (</span><br><span class="line">    SELECT *,</span><br><span class="line">        ROW_NUMBER() OVER (</span><br><span class="line">        PARTITION BY user</span><br><span class="line">        ORDER BY CHAR_LENGTH(url) desc 可以非时间字段,可以降序,固定写法不能改变</span><br><span class="line">        ) AS row_num</span><br><span class="line">	FROM EventTable)</span><br><span class="line">WHERE row_num &lt;= 2</span><br><span class="line">	</span><br><span class="line">7 支持duplication去重</span><br><span class="line">语法格式和topN一样,区别在于order by的字段是时间类型字段</span><br><span class="line">所以和topN的区分主要看order by的字段,是时间类型字段则代表去重,如果是非时间类型字段则代表topN</span><br><span class="line">建议使用asc去重,这样可以避免-U+U</span><br><span class="line">SELECT </span><br><span class="line">	user, url, ts, row_num</span><br><span class="line">FROM (</span><br><span class="line">    SELECT *,</span><br><span class="line">        ROW_NUMBER() OVER (</span><br><span class="line">        PARTITION BY user</span><br><span class="line">        ORDER BY ts asc 可以非时间字段,可以降序,固定写法不能改变</span><br><span class="line">        ) AS row_num</span><br><span class="line">	FROM EventTable)</span><br><span class="line">WHERE row_num = 1</span><br></pre></td></tr></table></figure>

<h5 id="lookup-join如何书写"><a href="#lookup-join如何书写" class="headerlink" title="lookup join如何书写"></a>lookup join如何书写</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">即维表关联</span><br><span class="line">10 支持lookup join,相当于流数据和外部系统(例如hbase,redis,mysql等)进行join</span><br><span class="line">示例:</span><br><span class="line">	select</span><br><span class="line">	*</span><br><span class="line">	from tb1</span><br><span class="line">	join tb2_dim for system_time as of tb1.proc_time as tb2</span><br><span class="line">	on tb1.id=tb2.id</span><br></pre></td></tr></table></figure>

<h5 id="如何自定义udtf函数"><a href="#如何自定义udtf函数" class="headerlink" title="如何自定义udtf函数"></a>如何自定义udtf函数</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">继承TableFunction,一般泛型设置Row类型,返回也是Row类型</span><br><span class="line">一般使用@FunctionHint注解指定类型信息</span><br><span class="line">在代码中createTemporaryFunction注册udtf函数</span><br><span class="line">或者add jar,然后create table function xxx with xxx</span><br><span class="line">然后结合侧视图使用</span><br><span class="line">from xxx left join lateral table(xx(xx)) as xx(xx,xx)</span><br></pre></td></tr></table></figure>

<h5 id="sql查询语法"><a href="#sql查询语法" class="headerlink" title="sql查询语法"></a>sql查询语法</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line">1 distinct语句</span><br><span class="line">状态实现,所以必须设置ttl,否则状态越来越大</span><br><span class="line"></span><br><span class="line">2 支持count(*)</span><br><span class="line">实现方式就是更新查询,即先删除再增加</span><br><span class="line"></span><br><span class="line">3 支持分组聚合查询,使用更新查询方式</span><br><span class="line"></span><br><span class="line">4 支持grouping sets,rollup,cube</span><br><span class="line">group by grouping sets (</span><br><span class="line">	(xx,xx,xx),</span><br><span class="line">	(...)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">5 支持开窗函数</span><br><span class="line">select xxx() over(partition by xxx order by xxx range/rows xxx)</span><br><span class="line">注意order by的字段只能是时间戳,而且只能升序</span><br><span class="line">range范围表示按照时间区间 range between interval &#x27;1&#x27; second preceding and current row</span><br><span class="line">rows范围表示按照行数 rows between xx and xx</span><br><span class="line">示例</span><br><span class="line">select</span><br><span class="line">  id,</span><br><span class="line">  et,</span><br><span class="line">  vc,</span><br><span class="line">  count(vc) over(</span><br><span class="line">    partition by id </span><br><span class="line">    order by et </span><br><span class="line">    range between interval &#x27;10&#x27; second preceding and current row</span><br><span class="line">  ) as cnt</span><br><span class="line">from ws;</span><br><span class="line">或者写作:</span><br><span class="line">select</span><br><span class="line">  id,</span><br><span class="line">  et,</span><br><span class="line">  vc,</span><br><span class="line">  count(vc) over w as cnt</span><br><span class="line">from ws</span><br><span class="line">window w as (</span><br><span class="line">  partition by id </span><br><span class="line">  order by et </span><br><span class="line">  range between interval &#x27;10&#x27; second preceding and current row</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">6 支持固定格式实现topN</span><br><span class="line">topN对应的查询是更新查询,因为topN的数据一直在动态变化,所以涉及到先删除再插入</span><br><span class="line">select</span><br><span class="line">	*</span><br><span class="line">from (</span><br><span class="line">	select</span><br><span class="line">		*,</span><br><span class="line">		row_number() over(partition by xxx order by xxx ) as rownum</span><br><span class="line">	from xxx</span><br><span class="line">)</span><br><span class="line">where </span><br><span class="line">	rownum &lt;= N</span><br><span class="line">	</span><br><span class="line">7 支持duplication去重</span><br><span class="line">语法格式和topN一样,区别在于order by的字段是时间类型字段</span><br><span class="line">所以和topN的区分主要看order by的字段,是时间类型字段则代表去重,如果是非时间类型字段则代表topN</span><br><span class="line">建议使用asc去重,这样可以避免-U+U</span><br><span class="line"></span><br><span class="line">8 支持常规join</span><br><span class="line">针对inner join,查询方式为追加查询,动态表转换成流是仅追加流,因为只有匹配到的数据才会输出,并且不会有撤回的情况,编码是+[L,R]</span><br><span class="line">针对左连接/右连接,主表的一边到达以后,无论从表是否有匹配数据,都会直接先输出+[L,R]或者+[L,null],如果右边到达了能够匹配的数据,则先撤销-[L,null]再插入+[L,R]</span><br><span class="line">full join类似</span><br><span class="line">注意事项</span><br><span class="line">	可以是不等值join,但是不推荐使用,压力大</span><br><span class="line">	等值join数据进行hash shuffle,按照关联条件发送下游</span><br><span class="line">	状态会无限增大,因为所有数据都需要存储在状态中,所以必须设置合适的ttl</span><br><span class="line">	撤销编码使用-D,插入编码使用+I,这里没有使用-U和+U</span><br><span class="line"></span><br><span class="line">9 支持间隔连接join</span><br><span class="line">针对间隔连接,全部都是+I,这个和inner join编码一致,不存在删除撤回	</span><br><span class="line">示例:</span><br><span class="line">	select</span><br><span class="line">	*</span><br><span class="line">	from ws,ws1</span><br><span class="line">	where ws.id=ws1.id</span><br><span class="line">	and ws.et between ws1.et-interval &#x27;2&#x27; second and ws1.et+interval &#x27;2&#x27; second;</span><br><span class="line"></span><br><span class="line">10 支持lookup join,相当于流数据和外部系统(例如hbase,redis,mysql等)进行join</span><br><span class="line">示例:</span><br><span class="line">	select</span><br><span class="line">	*</span><br><span class="line">	from tb1</span><br><span class="line">	join tb2_dim for system_time as of tb1.proc_time as tb2</span><br><span class="line">	on tb1.id=tb2.id</span><br><span class="line">	</span><br><span class="line">11 支持自定义函数udf,udtf,udaf</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://nfsp412.github.io/asuka.github.io/2024/10/21/bigdata004/" data-id="cm3ae93so000b6curf57khtdl" data-title="Flink学习笔记" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/asuka.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag">大数据</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata003" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/asuka.github.io/2024/10/16/bigdata003/" class="article-date">
  <time class="dt-published" datetime="2024-10-16T01:30:13.000Z" itemprop="datePublished">2024-10-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/asuka.github.io/2024/10/16/bigdata003/">Spark学习笔记</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>本文是自己在学习Spark过程中整理的一些基础笔记,仅做记录</p>
<h3 id="Spark初体验"><a href="#Spark初体验" class="headerlink" title="Spark初体验"></a>Spark初体验</h3><h5 id="spark常用默认端口"><a href="#spark常用默认端口" class="headerlink" title="spark常用默认端口"></a>spark常用默认端口</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">4040：每一个spark-shell产生的端口</span><br><span class="line">8080：standalone模式下的集群监控</span><br><span class="line">18080：历史服务地址</span><br><span class="line">7077:连接master的内部通信端口</span><br></pre></td></tr></table></figure>

<h5 id="spark的不同命令行命令有哪些"><a href="#spark的不同命令行命令有哪些" class="headerlink" title="spark的不同命令行命令有哪些?"></a>spark的不同命令行命令有哪些?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spark-shell  交互式输入scala代码进行开发</span><br><span class="line">pyspark 使用python进行交互式开发</span><br><span class="line">spark-sql sql命令行</span><br><span class="line">beeline 类似于Hive的beeline客户端</span><br><span class="line">spark-submit 提交spark任务,例如jar包或者py文件等</span><br></pre></td></tr></table></figure>

<h5 id="spark提交任务速度如何优化"><a href="#spark提交任务速度如何优化" class="headerlink" title="spark提交任务速度如何优化?"></a>spark提交任务速度如何优化?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">上传jars到HDFS</span><br><span class="line">在spark-defaults.conf配置</span><br><span class="line">spark.yarn.jars=hdfs上传的jar包地址</span><br></pre></td></tr></table></figure>

<h5 id="常见进程有哪些"><a href="#常见进程有哪些" class="headerlink" title="常见进程有哪些"></a>常见进程有哪些</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">local模式 </span><br><span class="line">	SparkSubmit进程</span><br><span class="line">yarn模式-client模式  </span><br><span class="line">	SparkSubmit 1G1C </span><br><span class="line">	ExecutorLauncher(client模式) 2G1C  即am进程</span><br><span class="line">	YarnCoarseGrainedExecutorBackend 2G1C</span><br><span class="line">	共7G4C</span><br><span class="line">yarn模式-cluster模式</span><br><span class="line">	SparkSubmit 2G1C </span><br><span class="line">	ApplicationMaster(cluster模式) 2G1C  即am进程</span><br><span class="line">	YarnCoarseGrainedExecutorBackend 2G1C</span><br><span class="line">	共8G4C</span><br></pre></td></tr></table></figure>

<h5 id="spark比hive-hadoop-mr快的原因"><a href="#spark比hive-hadoop-mr快的原因" class="headerlink" title="spark比hive&#x2F;hadoop&#x2F;mr快的原因"></a>spark比hive&#x2F;hadoop&#x2F;mr快的原因</h5><ul>
<li>Spark 和 Hadoop 的根本差异是多个作业之间的数据通信问题 : Spark 多个作业之间数据通信是基于内存，而 Hadoop 是基于磁盘  </li>
<li>Spark 只有在 shuffle 的时候将数据写入磁盘，而 Hadoop 中多个 MR 作业之间的数据交互都要依赖于磁盘交互  </li>
<li>Spark Task 的启动时间快。 Spark 采用 fork 线程的方式，而 Hadoop 采用创建新的进程的方式</li>
</ul>
<h5 id="spark有几种运行模式"><a href="#spark有几种运行模式" class="headerlink" title="spark有几种运行模式"></a>spark有几种运行模式</h5><ul>
<li>本地模式,解压直接使用,4040端口查看页面</li>
<li>standalone模式,即资源调度是交给spark本身进行,需要配置master和worker,master的端口8080查看页面,不依赖hive和hadoop,该方式提交任务会产生一个sparksubmit进程和多个executor进程,,executor内存默认1024mb,核数默认和linux核数一致</li>
<li>yarn模式,有client和cluster两种,区别在于Driver运行的位置,是否运行在am中</li>
</ul>
<h5 id="spark提交任务有哪些参数"><a href="#spark提交任务有哪些参数" class="headerlink" title="spark提交任务有哪些参数"></a>spark提交任务有哪些参数</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">--class指定入口类</span><br><span class="line">--master运行模式,有本地模式local[2],有standalone模式是spark://ip:7077,有yarn模式是yarn</span><br><span class="line">--executor-memory每一个executor的内存,默认1GB</span><br><span class="line">--total-executor-cores总共的executor的核数</span><br><span class="line">--executor-cores每一个executor的核数</span><br><span class="line">上面两个核数来控制一共有几个executor执行,注意standalone模式下,没有什么container的概念了,所以linux的资源都能使用,不受yarn的container相关资源的限制</span><br><span class="line"></span><br><span class="line">如果是yarn模式不同,如下设置</span><br><span class="line">--deploy-mode 有cluster和client两种,cluster需要去集群log查看输出日志,client可以在控制台查看</span><br><span class="line">--num-executors 控制executor总数</span><br><span class="line">--executor-cores 控制单个executor核数</span><br></pre></td></tr></table></figure>

<h5 id="配置了hadoop历史服务还需要配置spark历史服务吗"><a href="#配置了hadoop历史服务还需要配置spark历史服务吗" class="headerlink" title="配置了hadoop历史服务还需要配置spark历史服务吗"></a>配置了hadoop历史服务还需要配置spark历史服务吗</h5><p>是两码事,hadoop历史服务没有收集spark任务,因此需要额外配置spark的历史服务</p>
<h5 id="spark有哪些组件"><a href="#spark有哪些组件" class="headerlink" title="spark有哪些组件"></a>spark有哪些组件</h5><ul>
<li><p>Driver客户端,程序解析为job作业,调度任务,跟踪executor执行情况,UI展示</p>
</li>
<li><p>executor是一个进程,运行task的,task之间是隔离的,rdd缓存在executor进程内,</p>
<p>如果是基于yarn模式下,Driver和rm之间通过appmaster进行联系</p>
<p>而executor运行在container中</p>
</li>
<li><p>task: 封装了计算逻辑和数据的任务</p>
</li>
</ul>
<h5 id="client和cluster模式的区别"><a href="#client和cluster模式的区别" class="headerlink" title="client和cluster模式的区别"></a>client和cluster模式的区别</h5><ul>
<li><p>Client 模式将用于监控和调度的 Driver 模块在客户端执行，而不是在 Yarn 中，所以一<br>般用于测试  ,Driver 在任务提交的本地机器上运行  ,Driver 启动后会和 ResourceManager 通讯申请启动 ApplicationMaster  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">INFO Client: Uploading resource file:/tmp/spark-ff43e88d-faca-49a2-ad94-f9d44063f6be/__spark_conf__236058212614201651.zip -&gt; hdfs://hadoop101:8020/user/root/.sparkStaging/application_1718470178998_0002/__spark_conf__.zip</span><br><span class="line"></span><br><span class="line">INFO Client: Submitting application application_1718470178998_0002 to ResourceManager</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>cluster模式,在 YARN Cluster 模式下，任务提交后会和 ResourceManager 通讯申请启动ApplicationMaster  ,随后 ResourceManager 分配 container，在合适的 NodeManager 上启动 ApplicationMaster，此时的 ApplicationMaster 就是 Driver  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">INFO Client: Uploading resource file:/opt/module/spark-3.3.2/examples/jars/spark-examples_2.13-3.3.2.jar -&gt; hdfs://hadoop101:8020/user/root/.sparkStaging/application_1718470178998_0005/spark-examples_2.13-3.3.2.jar</span><br><span class="line">INFO Client: Uploading resource file:/tmp/spark-f615f7f5-2565-4eb2-9706-dc3a640194f8/__spark_conf__8205004691630990196.zip -&gt; hdfs://hadoop101:8020/user/root/.sparkStaging/application_1718470178998_0005/__spark_conf__.zip</span><br><span class="line">这里多了一个jar包的上传</span><br></pre></td></tr></table></figure>
</li>
<li><p>源码中,如果是客户端Client模式,Driver执行在集群之外,此时集群NM中的AM进程名字叫做ExecutorLauncher而不是ApplicationMaster</p>
</li>
</ul>
<h5 id="spark有哪些数据结构"><a href="#spark有哪些数据结构" class="headerlink" title="spark有哪些数据结构"></a>spark有哪些数据结构</h5><ul>
<li><p>RDD : 弹性分布式数据集</p>
</li>
<li><p>累加器: 分布式共享只写变量, 将Driver端变量分布式共享到Executor端,完成各自的计算后,再返回Driver端,最后在Driver端进行merge</p>
<p>使用自带累加器,或者自定义累加器,将元素添加到累加器,调用value方法获取累加后的结果</p>
</li>
</ul>
<p>​	一般累加器放在行动算子中操作,避免少加或者多加</p>
<ul>
<li>广播变量: 分布式共享只读变量, 用来实现map join</li>
</ul>
<h5 id="rdd是什么"><a href="#rdd是什么" class="headerlink" title="rdd是什么"></a>rdd是什么</h5><p>RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是 Spark 中最基本的数据<br>处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行<br>计算的集合  </p>
<p>弹性指的是内存和磁盘自动切换,数据有容错性,计算出错可以重试,可以重新分片</p>
<p>分布式指的是消费的数据分布式存储,例如hdfs</p>
<p>数据集指的是rdd封装的计算逻辑,没有保存数据,有点类似于java的io流操作,一层一层包装</p>
<p>rdd是一个抽象类,所以有很多具体实现子类,例如maprdd等</p>
<p>不可变指的是每一个rdd封装的逻辑不可变,想要改变就需要再外面再封装一层rdd</p>
<p>可分区可并行计算</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1、分区列表 a list of partitions：实现并行计算</span><br><span class="line">	getPartitions</span><br><span class="line">2、每个分区都有一个计算函数 a function for computing each split 每个分区计算逻辑是一样的</span><br><span class="line">	compute(Partition, taskContext)</span><br><span class="line">3、rdd之间存在依赖关系 a list of dependencies on other rdd</span><br><span class="line">	getDependencies</span><br><span class="line">4、分区器，有默认分区器，可以自定义分区器，可以给kv类型的rdd自定义分区器</span><br><span class="line">	partitioner</span><br><span class="line">5、首选位置 将task任务尽量发送到同数据节点的服务器上 移动存储不如移动计算</span><br><span class="line">	getPreferredLocations</span><br></pre></td></tr></table></figure>

<h3 id="SparkCore"><a href="#SparkCore" class="headerlink" title="SparkCore"></a>SparkCore</h3><h5 id="makerdd和parallelize的区别"><a href="#makerdd和parallelize的区别" class="headerlink" title="makerdd和parallelize的区别"></a>makerdd和parallelize的区别</h5><p>makeRDD 方法其实就是 parallelize 方法  </p>
<h5 id="textFile和wholeTextFile的区别"><a href="#textFile和wholeTextFile的区别" class="headerlink" title="textFile和wholeTextFile的区别"></a>textFile和wholeTextFile的区别</h5><p>前者按照行读取,后者按照文件读取,读取结果是元组,第一个元素是文件路径</p>
<h5 id="分区切片规则是什么"><a href="#分区切片规则是什么" class="headerlink" title="分区切片规则是什么"></a>分区切片规则是什么</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">内存加载数据时，指定分区规则</span><br><span class="line">生效顺序 算子numSlices &gt; 环境spark.default.parallelism &gt; totalCores</span><br><span class="line">scheduler.conf.getInt(&quot;spark.default.parallelism&quot;, totalCores)</span><br><span class="line">1、创建rdd时，传入第二个参数，代表分区数量numSlices</span><br><span class="line">2、不传的话，存在默认值</span><br><span class="line">	默认值计算方法：对于本地模式而言：spark.default.parallelism,即conf中set的这个参数</span><br><span class="line">3、取不到就取totalCores，totalCores代表当前运行环境最大CPU核数，6核心12线程则为12</span><br><span class="line">	totalCores如果是本地模式就是对应的local[n]里面的n,如果是*才是最大核数</span><br><span class="line"></span><br><span class="line">数据进入分区规则:</span><br><span class="line">1、positions方法：传入数组数据的长度length(内存数据不包含回车换行)，和分区数量numSlices</span><br><span class="line">3、0 until numSlices遍历,每个元素是i</span><br><span class="line">2、计算开始start和结束end，组成元组</span><br><span class="line">	计算方法start = (i * length) / numSlices 取整</span><br><span class="line">	计算方法end = ((i + 1) * length) / numSlices 取整</span><br><span class="line">	i的范围[0, numSlices-1]闭区间</span><br><span class="line">3、计算每个分区（文件）的数据范围</span><br><span class="line">	计算方法[start, end)包左不包右</span><br><span class="line"></span><br><span class="line">文件加载数据时，指定分区规则,注意是最小分区,但是实际上是多少还需要继续看分区规则,而且Spark读取文件使用的是hadoop读取文件那一套</span><br><span class="line">1、创建rdd时，传入第二个参数，代表最小分区数量，minPartitions</span><br><span class="line">2、不传则存在默认值</span><br><span class="line">	该参数默认值是：min(spark.default.parallelism, 2)</span><br><span class="line">	spark.default.parallelism该参数默认值和内存读取时一致,先conf中set查看,再默认totalCores</span><br><span class="line">那么关注hadoop文件切片规则,来判断具体分区数量</span><br><span class="line">1、所有的文件统计总的字节数totalSize，需要包含回车换行等特殊字符</span><br><span class="line">2、求出 goalSize = totalSize / (numSlices == 0 ? 1 : numSlices)，即为totalSize除以分区数量，整除；这个参数表示每一个分区存放的字节数</span><br><span class="line">3,块大小同hadoop 128MB</span><br><span class="line">4,看文件总大小 / 填写的分区数  和块大小比较  谁小拿谁进行切分</span><br><span class="line">5、切分时候考虑1.1倍数</span><br><span class="line"></span><br><span class="line">数据进入分区规则:还是按照hadoop的数据进入规则,尽量考虑业务数据完整性,不按照字节计算</span><br><span class="line">	原则1：数据按照行为单位读取</span><br><span class="line">	原则2：数据读取时，以偏移量为单位，偏移量从0开始，</span><br><span class="line">	原则3：每个分区数据的范围，是按照偏移量进行计算的，例如第一个分区的数据，应该是[每一行的起始偏移量, 每个分区字节数]，闭区间</span><br><span class="line">	原则4：一行一行读取，所以即使读取了第二行的一个字符，该行也应该全部读取到</span><br><span class="line">	原则5：数据不会被重复读取</span><br><span class="line">1. 就按照分区时计算出来的每个分区放多少数据来进入的,即上面的totalSize,goalSize,分区数量</span><br><span class="line">2. 按照偏移量计算出每个分区的范围,并且是闭区间</span><br><span class="line">3. 考虑原则1和原则4和原则5读取</span><br></pre></td></tr></table></figure>

<h5 id="分区和并行度概念"><a href="#分区和并行度概念" class="headerlink" title="分区和并行度概念"></a>分区和并行度概念</h5><p>并行度可能只有一个executor,但是分区可能是多个,那么就不是并行而是并发执行了</p>
<h5 id="常用算子"><a href="#常用算子" class="headerlink" title="常用算子"></a>常用算子</h5><ul>
<li>转换算子</li>
</ul>
<p>单值类型的,比如map,mapPartition,flatmap,filter,groupby,sortby,distinct,coalesce,repartition等等</p>
<p>双值类型的,intersection交集,union并集,subtract差集</p>
<p>kv类型的,比如partitionBy,groupbykey,reducebykey,sortbykey,join等等</p>
<p>输入算子,例如集合读取parallelize,可以指定分区(切片)的数量, 文件读取textFile,wholeTextFile(以文件为单位读取),sequenceFile,objectFile等</p>
<ul>
<li>行动算子</li>
</ul>
<p>例如reduce,collect,count,take,takeordered,top,aggregate,fold,reduce,foreach</p>
<p>例如输出的saveAsTextFile,saveasobjectfile,saveassequencefile等,</p>
<h5 id="区分几个算子"><a href="#区分几个算子" class="headerlink" title="区分几个算子"></a>区分几个算子</h5><p>sortBy和sortByKey</p>
<p>groupBy和groupByKey</p>
<p>reduceByKey,aggregateByKey,foldByKey,combineByKey</p>
<p>reduce,aggregate,fold</p>
<h5 id="map和mapPartitions等等的区别"><a href="#map和mapPartitions等等的区别" class="headerlink" title="map和mapPartitions等等的区别?"></a>map和mapPartitions等等的区别?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">map类似于串行操作,性能较低</span><br><span class="line">而mappartitions类似于批处理,一次性处理一个分区数据,性能较好,但是内存占用多</span><br><span class="line"></span><br><span class="line">map不会更改元素条数,</span><br><span class="line">但是后者可以更改,输入迭代器输出迭代器</span><br><span class="line"></span><br><span class="line">mappartitionswithindex,额外能获取分区编号信息</span><br><span class="line"></span><br><span class="line">mapvalues操作kv类型,一般在groupby后面使用</span><br></pre></td></tr></table></figure>

<h5 id="sample算子的算法"><a href="#sample算子的算法" class="headerlink" title="sample算子的算法"></a>sample算子的算法</h5><p>数据倾斜的时候,使用该算子去进行抽样,针对key进行抽样来判断是否是倾斜key</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line"> <span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span></span><br><span class="line">),<span class="number">1</span>)</span><br><span class="line"><span class="comment">// 抽取数据不放回（伯努利算法）</span></span><br><span class="line"><span class="comment">// 伯努利算法：又叫 0、1 分布。例如扔硬币，要么正面，要么反面。</span></span><br><span class="line"><span class="comment">// 具体实现：根据种子和随机算法算出一个数和第二个参数设置几率比较，小于第二个参数要，大于不</span></span><br><span class="line">要</span><br><span class="line"><span class="comment">// 第一个参数：抽取的数据是否放回，false：不放回</span></span><br><span class="line"><span class="comment">// 第二个参数：抽取的几率，范围在[0,1]之间,0：全不取；1：全取；</span></span><br><span class="line"><span class="comment">// 第三个参数：随机数种子</span></span><br><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.sample(<span class="literal">false</span>, <span class="number">0.5</span>)</span><br><span class="line"><span class="comment">// 抽取数据放回（泊松算法）</span></span><br><span class="line"><span class="comment">// 第一个参数：抽取的数据是否放回，true：放回；false：不放回</span></span><br><span class="line"><span class="comment">// 第二个参数：重复数据的几率，范围大于等于 0.表示每一个元素被期望抽取到的次数</span></span><br><span class="line"><span class="comment">// 第三个参数：随机数种子</span></span><br><span class="line"><span class="keyword">val</span> dataRDD2 = dataRDD.sample(<span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">行动算子有一个takeSample,也是随机取值</span><br><span class="line"><span class="comment">// 第一个参数：抽取的数据是否放回，true：放回；false：不放回</span></span><br><span class="line"><span class="comment">// 第二个参数：指定要取值的元素的个数</span></span><br><span class="line"><span class="comment">// 第三个参数：随机数种子</span></span><br></pre></td></tr></table></figure>

<h5 id="distinct算子实现原理"><a href="#distinct算子实现原理" class="headerlink" title="distinct算子实现原理"></a>distinct算子实现原理</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">map(x =&gt; (x, <span class="literal">null</span>)).reduceByKey((x, _) =&gt; x, numPartitions).map(_._1)</span><br></pre></td></tr></table></figure>

<h5 id="coalesce和repartition的区别"><a href="#coalesce和repartition的区别" class="headerlink" title="coalesce和repartition的区别"></a>coalesce和repartition的区别</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">coalesce 可以增加或者减少分区数量，默认是不经过shuffle</span><br><span class="line">repartition 可以增加或者减少分区数量，一定经过 shuffle，底层使用 coalesce 并且shuffle 参数为 true</span><br><span class="line">所以对于减少分区，应该使用 coalesce 并且不设置第二个参数， 对于增加分区，可以直接使用 repartition </span><br></pre></td></tr></table></figure>

<h5 id="groupBy和groupByKey的区别"><a href="#groupBy和groupByKey的区别" class="headerlink" title="groupBy和groupByKey的区别?"></a>groupBy和groupByKey的区别?</h5><p>groupby是单值类型算子,按照指定条件进行分组(例如一个判断条件),返回的数据是元组,第一个元素是分区编号,第二个元素是分区内数据的迭代器集合,底层调用的groupbykey,存在shuffle</p>
<p>key是确定的，但是value是不同的，groupByKey会把value单独拿出来组合成为一个集合，而groupBy会把kv键值对组合成为集合</p>
<p>对于groupBy不确定分组的key是什么，而对于groupByKey，确定是使用key进行分组的</p>
<h5 id="sortBy和sortByKey-区别"><a href="#sortBy和sortByKey-区别" class="headerlink" title="sortBy和sortByKey 区别"></a>sortBy和sortByKey 区别</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sortBy算子，底层调用sortByKey算子，会创建一个RangePartitioner分区器，目的是为了将数据分散到不同分区，而再底层调用了sample抽样算子和collect行动算子，所以会创建一个ActiveJob任务，目的是为了尽可能抽样确定分区边界来让数据均匀分散到不同分区</span><br><span class="line">sortByKey的key必须实现Ordered特质而且还需要实现序列化接口</span><br></pre></td></tr></table></figure>

<h5 id="交集并集差集算子的区别"><a href="#交集并集差集算子的区别" class="headerlink" title="交集并集差集算子的区别"></a>交集并集差集算子的区别</h5><p>交集intersection数据类型不一致报错</p>
<p>并集union数据类型不一致不报错</p>
<p>差集subtract数据类型不一致不报错</p>
<h5 id="zip算子的特点"><a href="#zip算子的特点" class="headerlink" title="zip算子的特点"></a>zip算子的特点</h5><p>按照位置组合成键值对</p>
<p>数据类型可以不一致,但是分区数量一致,而且元素个数一致</p>
<h5 id="partitionBy和coalesce重分区有啥区别"><a href="#partitionBy和coalesce重分区有啥区别" class="headerlink" title="partitionBy和coalesce重分区有啥区别"></a>partitionBy和coalesce重分区有啥区别</h5><p>都能够进行分区的重分区操作</p>
<p>但是partitionBy是按照key进行重分区的,默认可以使用hash重分区,也能自定义分区器,操作的kv类型rdd</p>
<p>重复调用使用相同分区器,而且分区数量相同,不会产生新的rdd,即不会重复调用的,源码可以看出来</p>
<p>coalesce操作单值类型或者kv类型都可以</p>
<h5 id="有哪些分区器"><a href="#有哪些分区器" class="headerlink" title="有哪些分区器"></a>有哪些分区器</h5><p>相关rdd算子是partitionBy算子</p>
<p>hash分区器</p>
<p>range分区器,用在排序相关算子中</p>
<p>自定义分区器,</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">HashPartitioner 是默认分区器，例如reduceByKey等算子都存在一个默认分区器，默认的分区规则是：哈希取模规则</span><br><span class="line">RangePartitioner</span><br><span class="line">自定义分区器 继承 Partitoner 抽象类，重写抽象方法，同时在 reduceByKey 等算子中，作为第一个参数传递。如果重复调用还想保证只 shuffle 一次，那么还需要重写 equals 方法</span><br><span class="line"></span><br><span class="line">如果使用了两次相关的算子，并且传入的分区器类型和分区数量都一样，则不会做任何操作，即不会产生新的rdd</span><br></pre></td></tr></table></figure>

<p>自定义分区器代码演示</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomerPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">        key <span class="keyword">match</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;nba&quot;</span> =&gt; <span class="number">0</span></span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;cba&quot;</span> =&gt; <span class="number">1</span></span><br><span class="line">            <span class="keyword">case</span> _ =&gt; <span class="number">2</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="分组和分区的联系"><a href="#分组和分区的联系" class="headerlink" title="分组和分区的联系?"></a>分组和分区的联系?</h5><p>分组和分区没有必然的联系</p>
<p>groupBy算子，分区数量没变化，但是实际上经过了shuffle，极限情况下数据会分到同一个分区(就是同一组数据分到一个分区了)</p>
<p>但是不代表一个分区只能处理一个组的数据</p>
<h5 id="groupByKey-和-reduceByKey-的区别"><a href="#groupByKey-和-reduceByKey-的区别" class="headerlink" title="groupByKey 和 reduceByKey 的区别"></a>groupByKey 和 reduceByKey 的区别</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">两个都经过shuffle,但是reduceByKey 会进行预先聚合，这样可以让 shuffle write 的数据条数变少，性能好,而group不进行预先聚合,因为只是单纯分组,没有聚合计算的逻辑</span><br><span class="line">若是为了分组聚合,在不影响业务逻辑的前提下尽量多使用 reduceByKey</span><br><span class="line">reduce是分组聚合,而group只是分组</span><br></pre></td></tr></table></figure>

<h5 id="4个ByKey"><a href="#4个ByKey" class="headerlink" title="4个ByKey"></a>4个ByKey</h5><p>reduceByKey：没有初始值，分区内，分区间规则一致</p>
<p>aggregateByKey：有初始值，分区内，分区间规则可以不同,函数的柯里化</p>
<p>foldByKey：有初始值，分区内，分区间规则一致</p>
<p>combineByKey：可以将相同key的第一个值转换格式，分区内，分区间规则可以不同</p>
<p>底层都是combineByKeyWithClassTag：</p>
<p>​	createCombiner：相同的key的第一条数据的处理函数</p>
<p>​	mergeValue：分区内数据的处理函数</p>
<p>​	mergeCombiner：分区间的数据的处理函数</p>
<h5 id="cogroup和leftOuterJoin的区别"><a href="#cogroup和leftOuterJoin的区别" class="headerlink" title="cogroup和leftOuterJoin的区别"></a>cogroup和leftOuterJoin的区别</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">都需要操作kv类型rdd数据</span><br><span class="line"></span><br><span class="line">leftouterjoin类似于sql的左连接,会把value值存入元组,没关联上的值设置为None</span><br><span class="line">cogroup类似于sql的全外连接,会把两个表的value存入二元组,每个value都是一个迭代器,根据源码, cogroup有可能存在shuffle</span><br><span class="line"></span><br><span class="line">left只能是两个rdd关联</span><br><span class="line">而cogroup一个rdd最多可以连接三个rdd</span><br></pre></td></tr></table></figure>

<h5 id="top和takeOrdered的区别"><a href="#top和takeOrdered的区别" class="headerlink" title="top和takeOrdered的区别"></a>top和takeOrdered的区别</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">top降序</span><br><span class="line">takeOrdered升序</span><br></pre></td></tr></table></figure>

<h5 id="countByKey和countByValue的区别"><a href="#countByKey和countByValue的区别" class="headerlink" title="countByKey和countByValue的区别"></a>countByKey和countByValue的区别</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">countByKey统计每一个key的数量,返回map,操作kv类型rdd数据</span><br><span class="line">countByValue统计每一个value值的数量,返回map,一般操作单值类型的rdd数据,如果操作kv类型,那么是按照一个完整的kv键值对去判断重复出现的次数的,而不是单纯按照value进行判断的</span><br></pre></td></tr></table></figure>

<h5 id="何区分转换算子和行动算子"><a href="#何区分转换算子和行动算子" class="headerlink" title="何区分转换算子和行动算子"></a>何区分转换算子和行动算子</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">按照是否产生Job是不准确的，因为对于sortByKey这样的转换算子，即使不调用行动算子，在底层也会产生Job</span><br><span class="line">应该根据算子的返回类型是否是 RDD 来区分是否是转换算子</span><br></pre></td></tr></table></figure>

<h5 id="依赖和血缘"><a href="#依赖和血缘" class="headerlink" title="依赖和血缘"></a>依赖和血缘</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">依赖和血缘是RDD的其中一个特性</span><br><span class="line">依赖指的是某个RDD依赖到的上游的RDD,即两个rdd之间的关系</span><br><span class="line">而血缘指的是某个RDD的完整的依赖链条,注意这里的完整链条指的是什么,具体看代码演示</span><br><span class="line">使用.toDebugString可以查看某个RDD的血缘关系</span><br><span class="line">使用.rdd.dependencies 可以查看某个RDD 的依赖列表</span><br><span class="line">依赖分为宽依赖和窄依赖，窄依赖即 OneToOneDependency类，宽依赖即 ShuffleDependency类</span><br></pre></td></tr></table></figure>

<h5 id="宽窄依赖"><a href="#宽窄依赖" class="headerlink" title="宽窄依赖"></a>宽窄依赖</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">区分宽窄依赖的标志是看父RDD的一个分区的数据被下游几个RDD使用，如果对应多个下游RDD，那么就是宽依赖，如果对应下游一个RDD，那么就是窄依赖,所以一对多就是宽依赖,一对一或者多对一就是窄依赖,这就是为什么缩小分区可以不经过shuffle</span><br><span class="line">宽依赖的底层一定使用了shuffle</span><br></pre></td></tr></table></figure>

<h5 id="哪些算子会产生shufflerdd"><a href="#哪些算子会产生shufflerdd" class="headerlink" title="哪些算子会产生shufflerdd"></a>哪些算子会产生shufflerdd</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">ShuffledRDD类继承RDD,实现了compute方法,会在shuffle read端读取数据,在下面的函数中创建了该类的对象</span><br><span class="line"></span><br><span class="line">PairRDDFunctions类</span><br><span class="line">combineByKeyWithClassTag方法</span><br><span class="line">partitionBy方法</span><br><span class="line"></span><br><span class="line">OrderedRDDFunctions类</span><br><span class="line">sortByKey方法 </span><br><span class="line">repartitionAndSortWithinPartitions方法 </span><br><span class="line"></span><br><span class="line">RDD类</span><br><span class="line">repartition方法</span><br></pre></td></tr></table></figure>

<h5 id="spark的shuffle-shuffle是什么"><a href="#spark的shuffle-shuffle是什么" class="headerlink" title="spark的shuffle&#x2F;shuffle是什么"></a>spark的shuffle&#x2F;shuffle是什么</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">从groupBy引入</span><br><span class="line">默认情况下数据处理后分区不改变,但是Spark要求同一个组内的数据必须在同一个分区,所以groupBy会把数据打乱分区重新组合,这个操作就叫做shuffle</span><br><span class="line">(注意所有分区进入同一个分区这个不叫做shuffle,因为分区就没有被打乱)</span><br><span class="line"></span><br><span class="line">像groupBy这样的算子,具有改变分区数量的能力(因为我们分组聚合的时候,分组的个数可能变少,此时如果分区数量还保持不变可能出现分区无数据导致资源浪费)</span><br><span class="line"></span><br><span class="line">为了保证DAG,Spark要求所有数据必须分组完成后才能继续执行后续的操作,但是由于RDD不保存数据,所以数据必须落盘</span><br><span class="line"></span><br><span class="line">shuffle会把完整计算流程一分为二,一部分shuffle write一部分shuffle read,而且写磁盘操作完成后才能读取磁盘</span><br></pre></td></tr></table></figure>

<h5 id="cache，persist，checkpoint的区别"><a href="#cache，persist，checkpoint的区别" class="headerlink" title="cache，persist，checkpoint的区别"></a>cache，persist，checkpoint的区别</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1 cache底层是persist设置缓存级别为MEMORY_ONLY来实现</span><br><span class="line">2 persist有多种缓存级别。</span><br><span class="line">cache和persist,会在血缘关系中增加依赖关系</span><br><span class="line">对于一些经过 shuffle 的算子，例如 reduceByKey，会自动进行缓存，但是仍然推荐使用缓存算子和检查点算子的形式进行持久化</span><br><span class="line">触发了行动算子执行时才会真正的进行缓存，而并不是按照代码顺序执行</span><br><span class="line">明确缓存使用结束后应该调用.unpersist 来释放缓存</span><br><span class="line">缓存文件在作业执行完成后会被删除,</span><br><span class="line">3 checkpoint，指的是将rdd中间结果写入磁盘,会切断（改变）血缘关系。需要使用 SparkContext 来设置 CheckpointDir 目录，由于会多执行一次完整的流程，所以建议在检查点之前设置缓存cache或persist,这样就可以从cache/persist开始继续执行而不需要从头执行,doCheckpoint方法,会发现会触发job执行,而先执行cache,就可以直接从内存读取,不需要从头读取了,</span><br><span class="line">持久化落盘,需要设置存储路径,例如hdfs</span><br><span class="line"></span><br><span class="line">cache和persist会在血缘关系中添加新的依赖关系,出现问题的时候可以通过血缘关系查询数据源头</span><br><span class="line">而checkpoint会切断之前的血缘关系,重新建立新的血缘关系,其实就相当于数据源发生改变了</span><br></pre></td></tr></table></figure>

<h5 id="广播变量怎么使用"><a href="#广播变量怎么使用" class="headerlink" title="广播变量怎么使用"></a>广播变量怎么使用</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sc.broadcast()定义广播变量</span><br><span class="line">broadcast_var.value调用广播变量值</span><br></pre></td></tr></table></figure>

<h5 id="累加器怎么使用"><a href="#累加器怎么使用" class="headerlink" title="累加器怎么使用"></a>累加器怎么使用</h5><p>累加器有系统自带的三种累加器,也可以自定义累加器</p>
<p>系统自带的累加器,add方法添加元素,value方法获取元素</p>
<p>自定义累加器需要实现AccumulatorV2抽象类并实现相关抽象方法</p>
<h3 id="SparkSql"><a href="#SparkSql" class="headerlink" title="SparkSql"></a>SparkSql</h3><h5 id="sparksql-hive-hive-on-spark-spark-on-hive的区别"><a href="#sparksql-hive-hive-on-spark-spark-on-hive的区别" class="headerlink" title="sparksql,hive,hive on spark,spark on hive的区别"></a>sparksql,hive,hive on spark,spark on hive的区别</h5><ul>
<li>spark sql和hive on spark</li>
</ul>
<p>SparkSQL前身是shark,和hive类似,也是希望将sql转换成底层的代码,rdd,去执行,hive是为了转换成mr,所以也有metastore,解析器,翻译器,优化器,执行器等等,相当于是代替hive的mr引擎,</p>
<p>问题是Hive发展缓慢,导致spark受制于Hive发展速度,所以产生了Hive-On-spark和spark SQL两个分类</p>
<p>Hive-On-spark指的是:Hive的一个发展计划,将spark作为Hive底层执行的引擎之一,所以Hive不止一个引擎,有mr,spark,tez等,使用的是hive的解析器优化器metastore那一套,</p>
<p>sparkSQL是spark生态的成员,不受限于Hive发展,而是兼容Hive,所以sparkSQL是为了简化RDD开发,提供两个编程模型抽象,一个是DataFrame,一个是DataSet</p>
<p>DataFrame执行效率高于RDD,因为执行计划进行优化了</p>
<p>DataSet是DataFrame的扩展,对于DataFrame,查询结果要想获取指定字段,只能通过索引获取,不方便,而DataSet可以通过字段名获取,很方便,这是其中一个优点</p>
<p>spark sql: 使用的上下文环境对象是SparkSession,实际上就是老版本的SQLContext和HiveContext的组合,底层计算实际上还是使用的SparkContext</p>
<p>hive on spark: 使用hive context</p>
<p>都是解析,优化,执行功能</p>
<ul>
<li>spark on hive</li>
</ul>
<p>指的是在spark sql这层api,或者说具体的dataset&#x2F;dataframe 编程模型写代码的时候,操作已经现有的hive当中的表,进行读取写入等操作</p>
<h5 id="RDD-DataFrame和DataSet三者区别"><a href="#RDD-DataFrame和DataSet三者区别" class="headerlink" title="RDD,DataFrame和DataSet三者区别"></a>RDD,DataFrame和DataSet三者区别</h5><p>rdd只关心数据,不关心数据含义,而DataFrame还关心数据的结构信息,类似于二维表结构,带有schema元数据信息,关心字段名称,字段类型</p>
<p>根据catalyst优化器进行了优化,所以比自己编写rdd 一般情况下效率高</p>
<p>而DataSet是扩展,可以通过字段名称而不是索引去操作数据,对结果的处理更方便</p>
<p>dataFrame实际上就是dataset,类型是row类型</p>
<p>这三者都可以相互之间转换</p>
<h5 id="如何创建使用DataFrame-DataSet"><a href="#如何创建使用DataFrame-DataSet" class="headerlink" title="如何创建使用DataFrame&#x2F;DataSet"></a>如何创建使用DataFrame&#x2F;DataSet</h5><p>从已经存在的rdd创建转换</p>
<p>从hive表查询返回</p>
<p>直接读取数据源返回,支持多种丰富的数据源格式</p>
<p>还有样例类或者普通序列方式可以创建,很少使用,Seq</p>
<h5 id="SparkSql的默认数据类型"><a href="#SparkSql的默认数据类型" class="headerlink" title="SparkSql的默认数据类型"></a>SparkSql的默认数据类型</h5><p>内存读取数值,默认int,</p>
<p>数据源文件读取数值,默认bigint,可以和long转换</p>
<h5 id="DSL编码风格是什么"><a href="#DSL编码风格是什么" class="headerlink" title="DSL编码风格是什么"></a>DSL编码风格是什么</h5><p>DataFrame提供DSL去管理结构化表数据,不必创建临时视图</p>
<p>可以在scala,Java,r,python中使用</p>
<p>这样就不需要先创建表再去使用SQL查询,可以直接使用df调用查询算子,例如select</p>
<p>但是需要注意的是,如果参与运算,必须将字段添加$符号,例如$”age”,注意双引号位置;或者使用单引号表示字段而不是字符串,例如’age注意只有一个单引号</p>
<p>注意需要导包import spark.implicits._ 注意spark指的是SparkSession对象名称,而不是spark包名</p>
<p>DSL中还能使用sql不能使用的强类型UDAF等函数</p>
<h5 id="rdd-df-ds三者转换"><a href="#rdd-df-ds三者转换" class="headerlink" title="rdd,df,ds三者转换"></a>rdd,df,ds三者转换</h5><p>rdd转换成df,一般结合样例类进行转换</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//RDD转换成DF,注意先导包</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> sc = spark.sparkContext</span><br><span class="line"><span class="keyword">val</span> rdd = sc.textFile(<span class="string">&quot;input/word.txt&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> df = rdd.toDF()</span><br><span class="line">df.show()</span><br><span class="line">df.printSchema()<span class="comment">//默认是value名称string类型</span></span><br></pre></td></tr></table></figure>

<p>df转换成rdd</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//DF转换成RDD</span></span><br><span class="line"><span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;input/user.json&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> rdd = df.rdd</span><br><span class="line">rdd.collect().foreach(println)<span class="comment">//Row类型</span></span><br></pre></td></tr></table></figure>

<p>rdd转换成ds</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//RDD转换成DS,注意先导包,一般结合样例类使用</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> sc = spark.sparkContext</span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;zhangsan&quot;</span>, <span class="number">30</span>), (<span class="string">&quot;lisi&quot;</span>, <span class="number">40</span>)))</span><br><span class="line"><span class="keyword">val</span> ds1 = rdd1.map(x =&gt; <span class="type">User1</span>(x._1, x._2)).toDS()</span><br><span class="line">ds1.show()</span><br><span class="line">ds1.printSchema()</span><br></pre></td></tr></table></figure>

<p>ds转换成rdd</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//DS转换成RDD</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> sc = spark.sparkContext</span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;zhangsan&quot;</span>, <span class="number">30</span>), (<span class="string">&quot;lisi&quot;</span>, <span class="number">40</span>)))</span><br><span class="line"><span class="keyword">val</span> ds1 = rdd1.map(x =&gt; <span class="type">User2</span>(x._1, x._2)).toDS()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd = ds1.rdd</span><br><span class="line">rdd.collect().foreach(println)</span><br></pre></td></tr></table></figure>

<p>df转换成ds</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//如果是通过rdd转换df再转换ds,那么toDF(&quot;username&quot;,&quot;age&quot;)必须指定属性名称而且和样例类的属性保持一致</span></span><br><span class="line"><span class="comment">//而且还需要注意数据类型是否一致的问题</span></span><br><span class="line"><span class="comment">//DF转换成DS</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> sc = spark.sparkContext</span><br><span class="line"><span class="keyword">val</span> df = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;zhangsan&quot;</span>, <span class="number">30</span>), (<span class="string">&quot;lisi&quot;</span>, <span class="number">49</span>))).toDF(<span class="string">&quot;username&quot;</span>, <span class="string">&quot;age&quot;</span>)</span><br><span class="line">df.show()</span><br><span class="line">df.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> ds = df.as[<span class="type">User4</span>]</span><br><span class="line">ds.show()</span><br><span class="line">ds.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="comment">//直接转换</span></span><br><span class="line"><span class="keyword">val</span> df1 = spark.read.json(<span class="string">&quot;input/user.json&quot;</span>)</span><br><span class="line">df1.show()</span><br><span class="line">df1.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> ds1 = df1.as[<span class="type">User4</span>]</span><br><span class="line">ds1.show()</span><br><span class="line">ds1.printSchema()</span><br></pre></td></tr></table></figure>

<p>ds转换成df</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//DS转换成DF</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> sc = spark.sparkContext</span><br><span class="line"><span class="keyword">val</span> df = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;zhangsan&quot;</span>, <span class="number">30</span>), (<span class="string">&quot;lisi&quot;</span>, <span class="number">49</span>))).toDF(<span class="string">&quot;username&quot;</span>, <span class="string">&quot;age&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> ds = df.as[<span class="type">User5</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> newDF = ds.toDF()</span><br><span class="line">newDF.show()</span><br><span class="line">newDF.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="comment">//直接转换</span></span><br><span class="line"><span class="keyword">val</span> df1 = spark.read.json(<span class="string">&quot;input/user.json&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> ds1 = df1.as[<span class="type">User5</span>]</span><br><span class="line"><span class="keyword">val</span> newDF1 = ds1.toDF()</span><br><span class="line">newDF1.show()</span><br><span class="line">newDF1.printSchema()</span><br></pre></td></tr></table></figure>

<h5 id="view生效范围"><a href="#view生效范围" class="headerlink" title="view生效范围"></a>view生效范围</h5><p>只在当前session生效</p>
<p>可以建设全局生效的view,但是访问时需要添加global_temp.view访问</p>
<p>使用spark.newSession可以创建新的session验证效果</p>
<h5 id="spark-sql的默认文件格式是什么"><a href="#spark-sql的默认文件格式是什么" class="headerlink" title="spark sql的默认文件格式是什么"></a>spark sql的默认文件格式是什么</h5><p>spark默认保存和读取的文件格式是parquet列式存储</p>
<h5 id="spark-sql的默认加载和保存方式是什么"><a href="#spark-sql的默认加载和保存方式是什么" class="headerlink" title="spark sql的默认加载和保存方式是什么"></a>spark sql的默认加载和保存方式是什么</h5><p>使用spark.read.load是通用的读取方式,但是一定得是parquet格式才可以</p>
<p>安装目录有样例文件</p>
<p>使用spark.write.save是通用的保存方式,也同样是默认parquet格式</p>
<p>有csv,json,jdbc,parquet,orc,text,avro等等多种格式</p>
<p>甚至可以在sql里面直接查询文件</p>
<p>可以设置压缩格式直接读取压缩文件</p>
<h5 id="sparksql存储文件savemode加锁了吗-有哪些savemode"><a href="#sparksql存储文件savemode加锁了吗-有哪些savemode" class="headerlink" title="sparksql存储文件savemode加锁了吗,有哪些savemode"></a>sparksql存储文件savemode加锁了吗,有哪些savemode</h5><p>error报错,append追加,overwrite覆盖,ignore忽略</p>
<p>没有加锁,不是原子操作,没有事务性保证</p>
<h5 id="sparksql如何连接Hive"><a href="#sparksql如何连接Hive" class="headerlink" title="sparksql如何连接Hive"></a>sparksql如何连接Hive</h5><p>spark内置了Hive,也可以连接外部Hive</p>
<p>内置Hive会产生metastore_db的目录,只要操作像show tables这样的sql语句就会创建,创建表还会产生spark-warehouse目录,还有一个derby.log文件证明元数据内置derby数据库中,内置Hive用来练习使用</p>
<p>外置Hive的使用:</p>
<p>hive-site.xml的拷贝,以及数据库连接jar包,还有一些相关配置,还有core-site.xml和hdfs-site.xml等拷贝</p>
<h5 id="thriftserver是什么"><a href="#thriftserver是什么" class="headerlink" title="thriftserver是什么"></a>thriftserver是什么</h5><p>Spark Thrift Server 是 Spark 社区基于 HiveServer2 实现的一个 Thrift 服务。旨在无缝兼容HiveServer2。因为 Spark Thrift Server 的接口和协议都和 HiveServer2 完全一致，因此我们部署好 Spark Thrift Server 后，可以直接使用 hive 的 beeline 访问 Spark Thrift Server 执行相关语句。 Spark Thrift Server 的目的也只是取代 HiveServer2，因此它依旧可以和 Hive Metastore进行交互，获取到 hive 的元数据  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/opt/module/spark-yarn/sbin/start-thriftserver.sh --hiveconf hive.server2.thrift.port=10001 --hiveconf hive.server2.thrift.bind.host=hadoop102 --conf spark.sql.warehouse.dir=hdfs://hadoop102:8020/user/hive/warehouse --master yarn</span><br></pre></td></tr></table></figure>

<h3 id="优化部分"><a href="#优化部分" class="headerlink" title="优化部分"></a>优化部分</h3><h5 id="工作中的一点执行时间参考"><a href="#工作中的一点执行时间参考" class="headerlink" title="工作中的一点执行时间参考"></a>工作中的一点执行时间参考</h5><p>文件大小2.3GB,日增,参数设置–master yarn –deploy-mode client –driver-memory 1g –num-executors 3 –executor-cores 2 –executor-memory 4g,三个executor,每一个核数2核4gb,最大并发度2*3&#x3D;6,文件切分按照128,一共切分了19个task,执行时间4min,数据条数2.2千万,建设分桶表和不建设的时间基本一致</p>
<h5 id="sparksql转换rdd的流程"><a href="#sparksql转换rdd的流程" class="headerlink" title="sparksql转换rdd的流程"></a>sparksql转换rdd的流程</h5><p>parser进行语法校验,通过后生成逻辑计划,然后通过catalog存储库来校验表名,列名,类型等信息,通过后使用catalyst优化器进行优化,生成优化后的逻辑执行计划,然后转换成物理执行计划,然后基于cbo选择代价较小的物理计划,最终生成rdd代码层面的job任务开始执行</p>
<h5 id="spark的执行计划"><a href="#spark的执行计划" class="headerlink" title="spark的执行计划"></a>spark的执行计划</h5><p>默认只展示物理执行计划,使用explain算子查询,或者sql查询</p>
<p>一般就查询extended物理计划和逻辑计划</p>
<p>从下往上查看计划,可以发现,会提前预聚合,并且尽量走广播join,列裁剪和谓词下推也会尽量执行</p>
<p>也可以在web界面查看执行计划,但是注意idea本地没有历史服务的话,可以通过死循环查看</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Parsed Logical Plan,即未决断的逻辑计划</span><br><span class="line">Analyzed Logical Plan,即决断的逻辑执行计划</span><br><span class="line">	会展示列名,列裁剪</span><br><span class="line">	会展示列的序号</span><br><span class="line">	会进行类型转换</span><br><span class="line">	列的信息和表格式</span><br><span class="line">Optimized Logical Plan,即优化逻辑执行计划</span><br><span class="line">	例如将关联条件过滤空,谓词下推</span><br><span class="line">Physical Plan,即物理执行计划</span><br><span class="line">	前面的小括号数字代表执行的先后顺序</span><br><span class="line">	缩进量相同代表同时执行</span><br><span class="line">	Project可以理解为列裁剪</span><br><span class="line">	Exchange表示shuffle过程</span><br><span class="line">	Aggregate聚合会存在预聚合</span><br><span class="line">HashAggregate 运算符表示数据聚合，一般 HashAggregate 是成对出现，第一个</span><br><span class="line">HashAggregate 是将执行节点本地的数据进行局部聚合，另一个 HashAggregate 是</span><br><span class="line">将各个分区的数据进一步进行聚合计算</span><br><span class="line">Exchange 运算符其实就是 shuffle，表示需要在集群上移动数据。很多时候</span><br><span class="line">HashAggregate 会以 Exchange 分隔开来</span><br><span class="line">Project 运算符是 SQL 中的投影操作，就是选择列</span><br><span class="line">BroadcastHashJoin 运算符表示通过基于广播方式进行 HashJoin</span><br><span class="line">LocalTableScan 运算符就是全表扫描本地的表</span><br></pre></td></tr></table></figure>

<h5 id="参数调优思路"><a href="#参数调优思路" class="headerlink" title="参数调优思路"></a>参数调优思路</h5><p>参考集群可用资源,在spark-submit提交任务时指定参数</p>
<p>executor个数越多,单个cpu核数越多,能够提高task最大并发,并发&#x3D;executor个数*每一个cpu核数</p>
<p>executor内存越大,能cache数据量越大,减少io,shuffle缓存数据量越大,减少io,避免频繁gc</p>
<p>executor-cores,每个executor的最大核数,一般3-6,这里设置4</p>
<p>并行度设置为并发度也就是cores数量的2-3倍,这里的cores&#x3D;num-executors*executor-cores</p>
<p>spark.default.parallelism,指的是rdd并行度</p>
<p>spark.sql.shuffle.partitions,默认200,sparksql并行度</p>
<p>根据经验,Driver调整较少</p>
<p>结果还要注意yarn配置的容器内存,默认1-8GB,一般设置1-4gb,看数据量大小,具体执行分析</p>
<h5 id="内存模型和估算"><a href="#内存模型和估算" class="headerlink" title="内存模型和估算"></a>内存模型和估算</h5><p>1 Execution内存:shuffle等0.3</p>
<ul>
<li>估算,execution&#x3D;每个executor核数*(数据集大小&#x2F;并行度)</li>
<li>计算单个并行度有多大数据量,假设默认200并行度,数据集100GB,每个并行度task的数据量是512MB</li>
<li>一个executor假设4core,那么需要同时跑4个Task,所以内存需要2GB至少</li>
</ul>
<p>2 Storage内存:缓存数据等0.3,一个executor内部,存储的广播变量,数据缓存cache</p>
<ul>
<li>估算,storage&#x3D;广播变量+cache&#x2F;executor数量,因为cache是分散到所有executor存储的</li>
</ul>
<p>3 other其他内存:0.4代码内存,内部元数据等</p>
<ul>
<li>估算,other&#x3D;自定义的数据结构内存*每个executor的核数</li>
<li>一般不太大不需要改</li>
</ul>
<p>4 预留300MB</p>
<p>比例一般不需要调整,因为可以动态调整</p>
<p>spark.memory.fraction调整比例</p>
<p>spark.memory.storageFraction调整比例</p>
<h5 id="cache缓存估算"><a href="#cache缓存估算" class="headerlink" title="cache缓存估算"></a>cache缓存估算</h5><p>2.3GB文件加载到表,并cache缓存</p>
<p>提交参数是,Driver1GB,executors&#x3D;3,executor-cores&#x3D;2,executor-memory&#x3D;6GB,使用client模式</p>
<p>可以查看估计需要7-8GB左右内存去缓存</p>
<p>如果使用kryo序列化,估计需要1GB左右</p>
<p>对于dataset进行缓存cache调用,实际上不是java序列化也不是kryo序列化,默认MEMORY_AND_DISK  ,大概600mb</p>
<p>所以尽量使用dataset的api进行编程,效率更好</p>
<h5 id="并行度和并发度的区别"><a href="#并行度和并发度的区别" class="headerlink" title="并行度和并发度的区别"></a>并行度和并发度的区别</h5><p>spark.default.parallelism,指的是rdd并行度</p>
<p>spark.sql.shuffle.partitions,默认200,sparksql并行度</p>
<p>并行度指的是总共的Task的数量,</p>
<p>并发度:指的是同时可以执行的Task的数量,这个取决于executor的cores数量,因为一个Task使用一个core,所以并发度就是executor的cores数量,executor-num*executor-core</p>
<p>并行度设置较低,所以Task数量较低,所以数据在Task数据量比较大,CPU容易线程挂起,即单个Task需要的内存过大,导致只有一部分Task能并行运行,剩余的核数全部挂起</p>
<p>并行度设置较高,数据过于分散,一个Task处理很少的数据量,调度开销反而更多</p>
<p>原则:并行度设置为并发度也就是cores数量的2-3倍,这里的cores&#x3D;num-executors*executor-cores</p>
<h5 id="RBO"><a href="#RBO" class="headerlink" title="RBO"></a>RBO</h5><p>在spark sql的逻辑执行计划的优化环节,就是基于rbo进行的优化</p>
<p>即sql层面优化,逻辑计划进行优化,基于RBO优化,有80多种优化规则,例如谓词下推,列裁剪,常量替换等</p>
<p>spark使用catalyst优化器,有81种优化规则,分为27组,有些规则会分配到多个组,所以不考虑重复性,总体有129个优化规则</p>
<p>谓词下推:指的是将过滤条件的谓词尽可能提前执行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">select *</span><br><span class="line">from A join B on A.id=B.id</span><br><span class="line">and A.id&lt;10;   指的是关联过程中的关联条件,会谓词下推优化</span><br><span class="line"></span><br><span class="line">select *</span><br><span class="line">from A join B on A.id=B.id</span><br><span class="line">where A.id&lt;10;  指的是关联以后的临时表where,会谓词下推优化</span><br><span class="line"></span><br><span class="line">即对于内连接,不管这个条件是on还是where,左表限定范围了,那么右表自然也会限定范围,所以都会提前谓词下推过滤,注意都是id</span><br><span class="line"></span><br><span class="line">select *</span><br><span class="line">from A left join B on A.id=B.id</span><br><span class="line">and A.id&lt;10;   可以发现,谓词下推把右表进行了范围过滤,因为左表需要保留所有数据,</span><br><span class="line"></span><br><span class="line">select *</span><br><span class="line">from A left join B on A.id=B.id</span><br><span class="line">where A.id&lt;10;   可以发现,相当于内连接</span><br><span class="line"></span><br><span class="line">对于外连接,条件写在on中,无论这个条件是左表的还是右表的条件,首先都会进行谓词下推,然后都是对右表的条件进行过滤,从结果来看,都是保留全部左表的数据,即所谓正确结果</span><br><span class="line">如果条件写在where中就要关注本身语义了,相当于内连接了,此时两张表都会进行事先的谓词下推过滤,从结果来看不是所谓保留全部左表数据的所谓的正确结果</span><br><span class="line"></span><br><span class="line">select *</span><br><span class="line">from A left join B on A.id=B.id</span><br><span class="line">and B.id&lt;10;   左连接,但是加入右表的范围过滤,先进行右表的谓词下推过滤,因为左表需要保留所有数据,</span><br><span class="line"></span><br><span class="line">select *</span><br><span class="line">from A left join B on A.id=B.id</span><br><span class="line">where B.id&lt;10;   左连接,但是where写了右表的过滤条件,相当于内连接,发现谓词下推两张表都先进行过滤,然后内连接</span><br></pre></td></tr></table></figure>

<h5 id="CBO"><a href="#CBO" class="headerlink" title="CBO"></a>CBO</h5><p>首先analyze表信息和关联相关字段信息,然后开启cbo两个参数,这样有机会可以触发广播join</p>
<p>即代价选择较小的优化手段,先计算所有物理计划的代价,然后选择代价最小的,最终生成物理执行计划</p>
<p>考虑的是执行节点数据本身的代价,以及操作算子的代价,例如join算子的优化</p>
<p>主要可以用来优化join顺序</p>
<p>默认关闭,参数是spark.sql.cbo.enabled</p>
<p>要先进行statistics收集,即先统计表和列的信息,统计行数和字节数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ANALYZE TABLE 表名 COMPUTE STATISTICS,收集行数,表大小(字节数)</span><br><span class="line"></span><br><span class="line">可以从hive元数据库查看:TBLS表,查询TBL_ID,再查看TABLE_PARAMS,根据TBL_ID查看</span><br></pre></td></tr></table></figure>

<p>再进行列信息统计</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ANALYZE TABLE 表名 COMPUTE STATISTICS FOR COLUMNS 列 1,列 2,列 3</span><br></pre></td></tr></table></figure>

<p>或者直接使用</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">desc formatted 表名,在statistics查看</span><br><span class="line">desc formatted 表名 列名  注意一次只能查看一列</span><br></pre></td></tr></table></figure>

<p>重要参数:</p>
<p>spark.sql.cbo.enabled默认false</p>
<p>spark.sql.cbo.joinReorder.enabled默认false,指的是使用CBO来自动调整连续内连接的表关联的顺序,</p>
<p>spark.sql.cbo.joinReorder.dp.threshold默认12,代表CBO自动调整内连接表的关联顺序的表的个数,超过该阈值就不会进行自动调整</p>
<p>使用方式</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SparkConf</span><br><span class="line">...</span><br><span class="line">.set(&quot;spark.sql.cbo.enabled&quot;, &quot;true&quot;)</span><br><span class="line">.set(&quot;spark.sql.cbo.joinReorder.enabled&quot;, &quot;true&quot;)</span><br></pre></td></tr></table></figure>

<p>使用演示</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">	A.cid,</span><br><span class="line">	sum(B.col)</span><br><span class="line">from A,B</span><br><span class="line">where A.id=B.id</span><br><span class="line">and B.col=&#x27;xxx&#x27;</span><br><span class="line">group by A.cid</span><br><span class="line"></span><br><span class="line">未开启CBO的join优化:不会走广播join</span><br><span class="line">开启CBO,会提前过滤,然后满足广播join条件</span><br><span class="line"></span><br><span class="line">需要先进行表分析analyze</span><br><span class="line"></span><br><span class="line">不开启CBO,判断是否广播join,是根据原始表大小判断,不会进行过滤等操作,但是实际上sql中存在提前过滤,开启以后,进行提前过滤,就满足广播join了</span><br></pre></td></tr></table></figure>

<h5 id="广播join"><a href="#广播join" class="headerlink" title="广播join"></a>广播join</h5><p>广播join类比于hive的mapjoin;如果一张表小于条件要求,会先将小表聚合到Driver端,然后广播到各个大表分区中,本地join避免shuffle</p>
<p>spark默认是sortmergejoin</p>
<p>参数:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">spark.sql.autoBroadcastJoinThreshold参数,默认10MB</span><br><span class="line">默认广播join是开启的,设置-1代表禁用广播join,禁用广播就会走sortmergejoin,是要经过shuffle过程,而且要排序,性能很低的</span><br><span class="line"></span><br><span class="line">强行广播,即使用Hint方式,sc指的是小表,可以写别名</span><br><span class="line">select /*+ BROADCASTJOIN(sc) */  或者</span><br><span class="line">select /*+ BROADCAST(sc) */  或者</span><br><span class="line">select /*+ MAPJOIN(sc) */  </span><br><span class="line"></span><br><span class="line">使用dataset api的话</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">                .setAppName(&quot;word count&quot;)</span><br><span class="line">                .setMaster(&quot;local[*]&quot;);</span><br><span class="line">SparkSession spark = InitUtil.initSparkSession(conf);</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; sql1 = spark.sql(&quot;select * from sparktuning.sale_course&quot;);</span><br><span class="line">Dataset&lt;Row&gt; sql2 = spark.sql(&quot;select * from sparktuning.course_shopping_cart&quot;);</span><br><span class="line">org.apache.spark.sql.functions.broadcast(sql1)</span><br><span class="line">        .join(sql2, &quot;courseid&quot;)</span><br><span class="line">        .select(&quot;courseid&quot;)</span><br><span class="line">        .explain();</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="SMB-join"><a href="#SMB-join" class="headerlink" title="SMB join"></a>SMB join</h5><p>概述: 适用于两张大表进行join,指的是sort merge bucket join,所以需要进行分桶</p>
<p>原理是: 首先进行排序,然后根据key进行合并,将相同key的数据放置在同一个桶中,相同key的数据在同一个桶然后进行join,就变成小表之间的join了</p>
<p>要求是: 两张表能分桶,且分桶个数相同,且join的关联列必须是分桶列,也必须是排序列</p>
<p>数据写入分桶表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark</span><br><span class="line">...</span><br><span class="line">.bucketBy(numBuckets, &quot;colName&quot;)</span><br><span class="line">.sortBy(&quot;colName&quot;)</span><br><span class="line">...</span><br><span class="line">.saveAsTable(&quot;databaseName.tableName&quot;)</span><br></pre></td></tr></table></figure>

<p>执行效果差别:通过页面查看,使用和不使用分桶表,都是体现为SortMergeJoin,但是Sort环节的时间发现,分桶join时间缩短</p>
<p>然后观察文件可以发现,hdfs生成文件中,普通的生成文件数量是等同于并行度,即spark.sql.shuffle.partitions数量,分桶的生成文件的数量是等同于桶数量,变相减少了文件数量</p>
<h5 id="数据倾斜"><a href="#数据倾斜" class="headerlink" title="数据倾斜"></a>数据倾斜</h5><p>原因:一般是经过shuffle的时候产生的,涉及到数据的重新分区时,某个key是大key,就产生数据倾斜,可能导致OOM</p>
<p>如何定位大key:首先已经发生了数据倾斜时,此时就进行数据采样,来定位大key</p>
<p>web界面,有一条特别长,其他都短的;或者查看shuffle read的数据量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Dataset&lt;Row&gt; sql1 = spark.sql(&quot;select courseid from sparktuning.course_shopping_cart;&quot;);</span><br><span class="line">        System.out.println(sql1.select(&quot;courseid&quot;)</span><br><span class="line">                .sample(false, 0.1)</span><br><span class="line">                .toJavaRDD()</span><br><span class="line">                .mapToPair(x -&gt; new Tuple2&lt;&gt;(x, 1))</span><br><span class="line">                .reduceByKey((x1, x2) -&gt; (x1 + x2))</span><br><span class="line">                .mapToPair(x -&gt; new Tuple2&lt;&gt;(x._2, x._1))</span><br><span class="line">                .sortByKey(false)</span><br><span class="line">                .take(10));</span><br><span class="line">                </span><br><span class="line">[(500138,[101]), (500022,[103]), (258,[6482]), (254,[2373]), (253,[2770]), (249,[5912]), (246,[2743]), (245,[7628]), (245,[3878]), (245,[3986])]</span><br><span class="line">发现101和103是大key</span><br></pre></td></tr></table></figure>

<ul>
<li>单表数据倾斜优化</li>
</ul>
<p>这种情况不是join产生,而是分组聚合产生,例如dataset的groupby,或者rdd的groupbykey算子等,这种情况实际上不太需要关注,因为本来就会进行预聚合操作,但是有一种场景,倾斜的key大量分布在不同切片中,如果分片很少,那么就不会倾斜了,因为会预聚合,但是分片数量很多,预聚合的效果就不明显了,解决思路</p>
<p>二次聚合,先加盐后去盐:即先给key添加一个随机数前缀或者后缀,然后按照这个新key进行分组聚合,这样大key就被打散,然后分组聚合,然后将新key去盐,去除随机数前缀或者后缀,然后按照key进行分组聚合,这样大key就在不同分区进行分组聚合避免数据倾斜</p>
<ul>
<li><p>join数据倾斜优化</p>
<p>有以下几种解决方式</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">广播join,类似于mapjoin,为了解决大表join小表的问题</span><br><span class="line"></span><br><span class="line">拆分大key,打散大表,扩容小表;适用于join时产生的数据倾斜</span><br><span class="line">逻辑是:</span><br><span class="line">	先进行抽样,找到大key</span><br><span class="line">	假设A表存在数据倾斜,B表不存在数据倾斜</span><br><span class="line">	将A表拆分为skew表和common表,skew表是倾斜key表,common表是不倾斜key表</span><br><span class="line">	将skew表的key全部加上随机数前缀(创建新的一列存储随机数前缀列):封装一个JavaBean,记得实现getset方法,将skew表返回携带随机数前缀的新表</span><br><span class="line">	然后将B表数据量扩大N倍:封装JavaBean,记得实现getset方法,扩容可以使用flatMap实现</span><br><span class="line">	N的取值可以使用分区数量</span><br><span class="line">	然后关联</span><br><span class="line">	from skew join new</span><br><span class="line">	union all</span><br><span class="line">	from common join B</span><br><span class="line">	代价是shuffle增加,适用于顽固倾斜问题</span><br><span class="line">具体代码见IDEA,参考hive优化的场景,是类似的</span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="map端优化"><a href="#map端优化" class="headerlink" title="map端优化"></a>map端优化</h5><p>实际上不需要我们操作什么,因为本身就会执行</p>
<p>那如果使用rdd算子,那么建议使用reducebykey等预聚合算子</p>
<h5 id="小文件优化"><a href="#小文件优化" class="headerlink" title="小文件优化"></a>小文件优化</h5><p>输入小文件优化</p>
<p>对于hive的mr程序,有一个CombineTextInputFormat用来读取小文件使用的</p>
<p>对于spark,需要控制N个文件总大小+N个文件开销&lt;&#x3D;spark.sql.files.maxPartitionBytes参数</p>
<p>文件开销参数的设置?接近小文件大小</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">set(&quot;spark.files.openCostInBytes&quot;, &quot;7194304&quot;)</span><br><span class="line">set(&quot;spark.sql.files.maxPartitionBytes&quot;, &quot;128MB&quot;)</span><br></pre></td></tr></table></figure>

<p>输出小文件优化</p>
<ul>
<li><p>join后的结果插入新表</p>
<p>join后的结果默认生成文件数量就是200,可以调整并行度,或者缩小分区例如coalesce算子,repartition算子</p>
</li>
<li><p>动态分区插入数据</p>
<p>如果没有shuffle,假设对于数据源分成3个Task处理,动态分区有4个,并且极端情况每个分区都在同一个Task出现了,此时产生的小文件数量&#x3D;Task数量*表分区数量&#x3D;12;对于有shuffle情况,有可能是200乘以4&#x3D;800</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">样例sql</span><br><span class="line">insert overwrite table A partition(aa)</span><br><span class="line">select</span><br><span class="line">*</span><br><span class="line">from B</span><br><span class="line">where aa!=大key</span><br><span class="line">distribute by aa;//将小key的数据按照分区字段aa进行分区,相同分区的数据会进入同一个分区中,这样每个分区下就只有一个小文件了</span><br><span class="line"></span><br><span class="line">insert overwrite table A partition(aa)</span><br><span class="line">select</span><br><span class="line">*</span><br><span class="line">from B</span><br><span class="line">where aa=大key</span><br><span class="line">distribute by cast(rand()*5 as int);//将大key分成随机的5个分区,然后写入分区表,这样该分区下就只有五个文件产生,避免产生太多的小文件</span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="Shuffle过程"><a href="#Shuffle过程" class="headerlink" title="Shuffle过程"></a>Shuffle过程</h5><p>1, map端的shuffle叫做shuffle write,reduce端的shuffle叫做shuffle read</p>
<p>有一个缓冲区,大小默认5MB,超过阈值就会尝试2*当前使用的内存,去申请新的内存</p>
<p>​	如果当前使用6MB,默认5MB,则去申请够12MB,即再申请7MB</p>
<p>如果申请不到内存,就会触发溢出写操作,这个参数我们不能设置</p>
<p>2, 溢出写使用的输出流缓冲区的大小默认32KB,可以适当提高来提升效率</p>
<p>3, shuffle涉及到文件序列化,默认每一批次1万条数据量去读写,这个参数我们无法指定</p>
<p>所以我们能调整的参数只有输出流缓冲区大小,kb</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">.set(&quot;spark.shuffle.file.buffer&quot;, &quot;32&quot;)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h5 id="reduce端优化"><a href="#reduce端优化" class="headerlink" title="reduce端优化"></a>reduce端优化</h5><p>reduce个数设置,reduce拉取缓冲区大小设置,减少join后并行度</p>
<ul>
<li><p>合理设置reduce数量</p>
<p>主要是关于并行度,并发度参数的设置,并发度设置&#x3D;executor-cores*num-executor</p>
<p>executor-cores一般设置3-6,设置4</p>
<p>并行度一般是2-3倍的并发度</p>
</li>
<li><p>增大reduce缓冲区,减少拉取次数,讲</p>
<p>reduce将相同分区数据拉到一起的时候,存在一个缓冲区的概念,默认48MB,建议96MB</p>
</li>
<li><p>调节reduce端拉取数据重试次数,不讲</p>
<p>默认3次重试,一般调整到6次</p>
<p>实践发现,针对超级大数据量(数十亿到上百亿)的Shuffle过程,调节该参数可以大幅度提高稳定性</p>
</li>
<li><p>调节reduce端拉取数据的等待间隔,不讲</p>
<p>拉取重试过程中的等待间隔,默认5秒,建议调整60秒</p>
</li>
</ul>
<p>上面三个参数如下配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;spark.reducer.maxSizeInFlight&quot;, &quot;96m&quot;</span><br><span class="line">&quot;spark.shuffle.io.maxRetries&quot;, &quot;6&quot;</span><br><span class="line">&quot;spark.shuffle.io.retryWait&quot;, &quot;60s&quot;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>合理的利用bypass</p>
<p>当shuffleManager是sortshuffleManager的时候,如果shuffle read task的数量小于阈值默认200,并且不需要进行map端合并,则不会进行排序操作,就可以使用bypassMergesortshufflewriter写数据,最后将每个task产生的临时磁盘文件合并成为一个文件,创建单独索引文件</p>
<p>什么时候没有预聚合?预聚合指的是聚合之前先聚合,所以没有聚合自然就没有预聚合,简单的join操作就没有聚合</p>
<p>执行计划看不出来bypass,看执行结果发现不走bypass出现了任务失败情况</p>
<p>不需要设置,因为默认看能不能触发</p>
</li>
</ul>
<h5 id="其他优化"><a href="#其他优化" class="headerlink" title="其他优化"></a>其他优化</h5><ul>
<li><p>调节数据本地化等待时间</p>
<p>一般追求NODE_LOCAL节点本地化即可,如果不是可以吧这个等待时间调大一些</p>
<p>参数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spark.locality.wait  默认3秒,建议6秒,10秒</span><br><span class="line">spark.locality.wait.process  默认3秒,建议60秒</span><br><span class="line">spark.locality.wait.node  默认3秒,建议30秒</span><br><span class="line">spark.locality.wait.rack  默认3秒,建议20秒</span><br><span class="line">注意参数6s,带s</span><br></pre></td></tr></table></figure>
</li>
<li><p>堆外内存参数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.memory.enable.offheap.enable,设置true</span><br></pre></td></tr></table></figure>
</li>
<li><p>调节连接等待时长</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--conf spark.core.connection.ack.wait.timeout=300s</span><br><span class="line">需要在spark-submit设置</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="源码部分"><a href="#源码部分" class="headerlink" title="源码部分"></a>源码部分</h3><h5 id="环境准备阶段源码学习"><a href="#环境准备阶段源码学习" class="headerlink" title="环境准备阶段源码学习"></a>环境准备阶段源码学习</h5><p>以spark on yarn为例说明</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">spark-submit提交任务到RM: 启动一个SparkSubmit的进程,走main方法; 所谓提交指的是封装一些指令提交给rm</span><br><span class="line">类:  org.apache.spark.deploy.SparkSubmit</span><br><span class="line">方法: main                                                                                  	1：submit = new SparkSubmit()</span><br><span class="line">  2：submit.doSubmit(args)</span><br><span class="line">  	...</span><br><span class="line">  	1：解析命令行参数: appArgs = parseArguments(args)</span><br><span class="line">    	创建对象: new SparkSubmitArguments(args)</span><br><span class="line">        1：类的加载过程中，调用了parse方法解析命令行参数，使用正则表达式的方式进行解析,是Java的方法: parse(args.asJava),将--class这样的命令行参数赋值给变量,然后去匹配,具体细节看SparkSubmitArguments的handle方法</span><br><span class="line">        2：类的加载过程中，给action赋默认值SUBMIT, action.getOrElse(SUBMIT),在loadEnvironmentArguments方法中</span><br><span class="line">    2：action方法进行匹配,SUBMIT为提交，触发submit方法: appArgs.action匹配submit(appArgs, uninitLog)方法</span><br><span class="line">	判断是什么集群类型standalone或者其他,然后执行doRunMain方法</span><br><span class="line">    然后判断是否有代理用户,然后执行runMain方法</span><br><span class="line">        1：准备提交环境，获取childMainClass：prepareSubmitEnvironment(args)</span><br><span class="line">            判断是什么集群类型,为childMainClass字符串赋值org.apache.spark.deploy.yarn.YarnClusterApplication,需要添加spark-yarn_2.12依赖查看源码</span><br><span class="line">        2：mainClass = Util.classForName(childMainClass)：通过类名反射获取类的信息</span><br><span class="line">        3：两种情况,创造实例对象,java或者scala版本：SparkApplication app = new xxx</span><br><span class="line">        4：实例对象启动：app.start; 实际上走到了具体实现类YarnClusterApplication类的start方法</span><br><span class="line"></span><br><span class="line">类: org.apache.spark.deploy.yarn.YarnClusterApplication</span><br><span class="line">方法: start方法</span><br><span class="line">    1：实例化过程中：new Client()</span><br><span class="line">    	1：实例化过程中,实例化一个非常重要的对象yarnClient：yarnClient = YarnClient.createYarnClient(),还会创建一个rmClient对象</span><br><span class="line">    2：new Client调用run方法</span><br><span class="line">       获取一个appId,同时调用提交应用程序的方法: appId = submitApplication方法</span><br><span class="line">        	1：connect方法,再加上yarnClient对象调用init、start，此时建立和服务器RM连接</span><br><span class="line">            2：yarnClient对象调用createApplication方法，告诉rm创建一个应用newApp</span><br><span class="line">            3：获取响应的同时,获取app id: newAppResponse = newApp.getNewApplicationResponse</span><br><span class="line">            4：创建容器环境containerContext = createContainerLauncherContext(newAppResponse)</span><br><span class="line">                方法1：包含了一些配置参数</span><br><span class="line">                方法2：包含amClass：集群环境下是org.apache.spark.deploy.yarn.ApplicationMaster；非集群环境下是org.apache.spark.deploy.yarn.ExecutorLauncher</span><br><span class="line">            5：使用app和容器环境，创建提交环境appContext = createApplicationSubmissionContext(newApp,containerContext),appContext包含了一些配置参数</span><br><span class="line">            6：yarnClient等同于建立RM连接，submitApplication等同于提交任务；yarnClient.submitApplication(appContext),</span><br><span class="line">            </span><br></pre></td></tr></table></figure>

<h5 id="启动AM阶段源码"><a href="#启动AM阶段源码" class="headerlink" title="启动AM阶段源码"></a>启动AM阶段源码</h5><p>以spark on yarn为例说明,cluster模式</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">进程方式启动AM: rm让某一个nm以进程的方式启动am</span><br><span class="line">类：org.apache.spark.deploy.yarn.ApplicationMaster</span><br><span class="line">方法：main</span><br><span class="line">	1：解析命令行参数,封装: amArgs = new ApplicationMasterArguments(args)</span><br><span class="line">    	parseArgs解析参数</span><br><span class="line">    2：创建sparkConf和yarnConf，</span><br><span class="line">    3：然后作为参数传入构造器，创建master = new ApplicationMaster(amArgs,sparkConf,yarnConf)</span><br><span class="line">        实例化过程中，client = new YarnRMClient，即AM连接RM的客户端</span><br><span class="line">    4：master.run方法</span><br><span class="line">    	1：集群模式下isClusterMode是true，runDriver方法，会启动一个应用程序，然后一直阻塞等待一个上下文环境对象，所以startUserApplication方法一定要传出一个上下文环境对象,不然下面的ThreadUtils.awaitResult方法会一直阻塞</span><br><span class="line">        	1：调用方法过程中， startUserApplication方法，启动用户应用程序</span><br><span class="line">                1：类加载器，加载--class对应的参数,即用户自定义的主程序类</span><br><span class="line">                2：创建线程启动Driver，包含SparkContext环境对象,AM根据参数启动Driver线程，并初始化SparkContext环境对象，此时开始执行自己的代码的main	</span><br><span class="line">        2：如果sc环境对象不为空，则rpcEnv = sc.env.rpcEnv；rpcEnv指的是通信环境</span><br><span class="line">        3：registerAM方法，即注册AM，保持和Yarn的RM的连接，申请资源,使用YarnRMClient客户端</span><br><span class="line">        4：createAllocator方法,同样使用YarnRMClient客户端</span><br><span class="line">            1：通过allocator = client.createAllocator方法,使用YarnRMClient客户端</span><br><span class="line">            2：allocator.allocateResource方法得到可以分配的资源，即返回资源可用列表               </span><br><span class="line">				1；获取allocatedContainers可用容器</span><br><span class="line">                2：处理可用容器handleAllocatedContainers(allocatedContainers.asScala),做了分类的处理,涉及到机架感知,位置选择等内容, 然后runAllocatedContainers运行已经分配好的容器: 当资源不够的时候,launcherPool线程池，使用线程池启动Executor(YarnCoarseGrainedExecutorBackend类)：实例化对象ExecutorRunnable(该对象里面有一个NMClient,意味着要和NM进行交互关联)，添加相关的构造器参数，并调用run方法启动,run方法明细如下</span><br><span class="line">                	run方法1：实例化对象ExecutorRunnable过程中，产生nmClient对象,用来和某个NM进行关联</span><br><span class="line">                    run方法2：Executor对象的run方法中，nmClient调用init、start方法</span><br><span class="line">                    run方法3：Executor对象的run方法中，还调用startContainer方法启动容器: 向指定NM启动容器，还是进程方式启动：nmClient.startContainer(容器,环境信息)，prepareCommand方法会准备好启动指令; 环境信息中包含命令参数；org.apache.spark.executor.YarnCoarseGrainedExecutorBackend类</span><br><span class="line">        5: resumeDriver方法,等环境都准备好,会让线程继续执行,即自己的开发的程序继续执行  ,查看YarnClusterScheduler类  </span><br></pre></td></tr></table></figure>

<h5 id="启动Executor阶段源码"><a href="#启动Executor阶段源码" class="headerlink" title="启动Executor阶段源码"></a>启动Executor阶段源码</h5><p>以spark on yarn为例说明</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">类: org.apache.spark.executor.YarnCoarseGrainedExecutorBackend</span><br><span class="line">方法: main</span><br><span class="line">	1：run</span><br><span class="line">    	方法1：driver = fetcher.方法，即通过fetcher找到driver建立联系</span><br><span class="line">        方法2：env = SparkEnv.createExecutorEnv通过该方法创建环境</span><br><span class="line">        方法3：env.rpcEnv.setupEndpoint即创建Executor终端，这里是把YarnCoarseGrainedExecutorBackend创建为通信终端，所以我们可以理解为YarnCoarseGrainedExecutorBackend进程就是Executor，但是实际上是用来通信的而不是用来真正计算的; 通信环境的生命周期，constructor、onstart、receive、onstop; setupEndpoint方法具体实现类是NettyRpcEnv</span><br><span class="line">;同时CoarseGrainedExecutorBackend类的onStart方法能够收到消息; 该方法中得到driver,并ask发送消息,消息就是注册Executor;同时Driver线程的SparkContext进行接收注册信息; 使用ask方法发消息,参数是RegisterExecutor注册Executor消息; SparkContext类的SchedulerBackend抽象类的实现类CoarseGrainedSchedulerBackend，包含接收回复等方法</span><br><span class="line">		1: receiveAndReply方法,接收到注册Executor的消息,返回true,自己给自己发消息的</span><br><span class="line">		2: receive方法，自己给自己发消息注册成功了，此时executor = new Executor实例化Executor对象,这个Executor才是真正用来计算的Executor,是一个计算对象</span><br><span class="line">		</span><br></pre></td></tr></table></figure>

<h5 id="整体如下"><a href="#整体如下" class="headerlink" title="整体如下"></a>整体如下</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Yarn,cluster版本：</span><br><span class="line">	1) 执行脚本提交任务，实际是启动一个 SparkSubmit 的 JVM 进程；</span><br><span class="line">    2) SparkSubmit 类中的 main 方法反射调用 YarnClusterApplication 的 main 方法；</span><br><span class="line">    3) YarnClusterApplication 创建 Yarn 客户端，然后向 Yarn 服务器发送执行指令：bin/java </span><br><span class="line">    ApplicationMaster；</span><br><span class="line">    4) Yarn 框架收到指令后会在指定的 NM 中启动 ApplicationMaster；</span><br><span class="line">    5) ApplicationMaster 启动 Driver 线程，执行用户的作业；</span><br><span class="line">    6) AM 向 RM 注册，申请资源；</span><br><span class="line">    7) 获取资源后 AM 向 NM 发送指令：bin/java YarnCoarseGrainedExecutorBackend；</span><br><span class="line">    8) CoarseGrainedExecutorBackend 进程会接收消息，跟 Driver 通信，注册已经启动的</span><br><span class="line">    Executor；然后启动计算对象 Executor 等待接收任务</span><br><span class="line">    9) Driver 线程继续执行完成作业的调度和任务的执行。交叉执行,即Driver线程执行时,注册资源是阻塞的,当执行main方法创建SparkContext以后,注册资源继续执行,完成资源注册初始化后,main方法继续执行,执行到行动算子等</span><br><span class="line">    10) Driver 分配任务并监控任务的执行。</span><br></pre></td></tr></table></figure>

<h5 id="作业-阶段-任务等的关系"><a href="#作业-阶段-任务等的关系" class="headerlink" title="作业,阶段,任务等的关系"></a>作业,阶段,任务等的关系</h5><p>Application对应一个初始化的SparkContext</p>
<p>Job对应一个行动算子</p>
<p>Stage对应宽依赖数量+1</p>
<p>Task对应单个Stage最后RDD的分区个数</p>
<p>由上到下是一对多关系</p>
<h5 id="作业-阶段-任务的源码"><a href="#作业-阶段-任务的源码" class="headerlink" title="作业,阶段,任务的源码"></a>作业,阶段,任务的源码</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">行动算子当中会使用SparkContext对象来调用runJob方法运行任务,经过层层调用后,最终会在DAGScheduler对象处调用runJob方法,最终发送了一个任务提交的消息并由handleJobSubmitted方法进行接收处理,该方法中做了几件事情</span><br><span class="line">	第一件事情,调用createResultStage方法,会计算出一个Job有几个Stage,创建ResultStage对象,该阶段只有一个,调用getOrCreateParentStages方法来根据ShuffleDependency的数量来创建ShuffleMapStage阶段对象的数量,所以说Stage的数量=ShuffleDependency宽依赖的数量+1</span><br><span class="line">	这里需要注意的是如果调用两次相同计算规则的 shuffle 算子，则第二次的算子不会产生 shuffle 过程，自然不会创建新的阶段</span><br><span class="line">	</span><br><span class="line">	第二件事,创建ActiveJob对象,即对应一个Job作业</span><br><span class="line">	第三件事,调用submitStage方法,并传入ResultStage对象,该方法中会判断是否还有上一级阶段,并使用递归调用的方式,一直找到没有上一级阶段的那个阶段,从这里开始向后按照顺序执行,一直执行到ResultStage阶段为止</span><br><span class="line">	这里需要注意的是前一个阶段不执行完成就不会执行后续阶段</span><br><span class="line">	</span><br><span class="line">在第三件事中,具体调用到了submitMissingTasks方法,该方法中做了几件事</span><br><span class="line">	第一件事,任务切分,和任务Task相关,会根据不同阶段类型来创建不同任务类型,例如ShuffleMapStage创建的任务类型是ShuffleMapTask,而ResultStage创建的是ResultTask,至于创建多少个Task对象,根据不同阶段里面的findMissingPartitions方法进行判断,任务数量=每个阶段最后一个RDD的分区数量</span><br><span class="line">	第二件事,任务调度: taskScheduler.submitTasks(new TaskSet()),提交任务到TaskSchedulerImpl中</span><br><span class="line">		1.会将任务Task包装成为TaskSet任务集</span><br><span class="line">		2.TaskSet包装成为TaskSetManager</span><br><span class="line">		3.将TaskSetManager添加到调度器中任务池TaskPool中,调度器有FIFO模式,Fair模式,默认配置来自于配置文件,默认使用FIFO调度模式; rootPool对象调用addSchedulable(传入manager参数)</span><br><span class="line">		4.自己给自己发消息,从池子中取出任务,然后makeOffers方法,任务不为空则启动任务launcherTasks启动任务</span><br><span class="line">		5.取出任务的过程调用resourceOffers,从调度器中取出任务,涉及到FIFO和Fair算法</span><br><span class="line">		6.判断本地化级别,然后决定发给哪里Executor去执行:计算和数据存储级别</span><br><span class="line">			进程本地化:数据和计算在同一进程;最高级别</span><br><span class="line">			节点本地化:数据和计算在同一节点机器</span><br><span class="line">			机架本地化:数据和计算在同一机架</span><br><span class="line">			任意;最低级别</span><br><span class="line">		7.launcherTasks启动任务</span><br><span class="line">		8.找到executorEndpoint终端,发消息send给某个Executor执行,消息是: LauncherTask,当然要把任务序列化传输过去</span><br><span class="line">	第三件事,任务执行:发消息以后Executor端进行接收消息执行,这个就在Executor端执行了</span><br><span class="line">		1.coarseGrainedExecutorBackend类,接收消息</span><br><span class="line">		2.判断是否任务为空,接收任务,反序列化</span><br><span class="line">		3.executor.launcherTask启动任务,将任务数据参数传入</span><br><span class="line">		4.层级关系是:Executor -&gt; ThreadPool -&gt; Thread -&gt; Task,实例化TaskRunner对象,然后线程池threadPool开始执行这个对象任务,每个Task用一个线程执行; tr = new TaskRunner(); threadPool.execute(tr)</span><br><span class="line">		5.每一个具体的任务,都需要重写抽象方法runTask</span><br></pre></td></tr></table></figure>

<h5 id="Netty通信框架"><a href="#Netty通信框架" class="headerlink" title="Netty通信框架"></a>Netty通信框架</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">并不会直接使用Socket进行通信</span><br><span class="line"></span><br><span class="line">spark3版本使用的内部通信框架是Netty框架,各个组件之间的通信依靠EndPoint,一个EndPoint有一个InBox和多个OutBox,具体多少个OutBox取决于你当前的EndPoint要和多少其他的EndPoint进行通信,接收的消息存入的是InBox,而发送出去的消息存入的是OutBox并写入其他EndPoint的InBox,在spark中有两种EndPoint,一个是DriverEndPoint,一个是CoarseGrainedExecutorBackend</span><br><span class="line"></span><br><span class="line">支持AIO异步非阻塞式通信</span><br><span class="line"></span><br><span class="line">Driver通信环境:</span><br><span class="line">SparkContext类，创建Env环境，创建的是NettyRpcEnv环境对象</span><br><span class="line">	NettyRpcEnv类</span><br><span class="line">		服务器TransportServer类,使用Epoll方式模拟AIO</span><br><span class="line">		通信终端RpcEndpoint,receive*,用来收数据</span><br><span class="line">			inbox收件箱,用来存储接收到的数据</span><br><span class="line">		通信终端引用RpcEndpointRef,ask*,用来发数据</span><br><span class="line">			outbox发件箱,根据地址存在多个发件箱,这个地址就是ip地址</span><br><span class="line">		客户端TransportClient,和服务器TransportServer类建立的连接</span><br><span class="line"></span><br><span class="line">Executor通信环境:</span><br><span class="line">CoarseGrainedExecutorBackend类</span><br><span class="line">	NettyRpcEnv类</span><br><span class="line">		服务器TransportServer类,使用Epoll方式模拟AIO</span><br><span class="line">		通信终端RpcEndpoint,receive*,用来收数据</span><br><span class="line">			inbox收件箱,用来存储接收到的数据</span><br><span class="line">		通信终端引用RpcEndpointRef,ask*,用来发数据</span><br><span class="line">			outbox发件箱,根据地址存在多个发件箱,这个地址就是ip地址</span><br><span class="line">		客户端TransportClient,和服务器TransportServer类建立的连接</span><br><span class="line"></span><br><span class="line">Driver发信息就代表Executor收信息,这样的对应关系</span><br></pre></td></tr></table></figure>

<h5 id="不同IO"><a href="#不同IO" class="headerlink" title="不同IO"></a>不同IO</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">java.io, 即BIO, 传统IO, 效率低</span><br><span class="line">java.nio, 即NIO, </span><br><span class="line">java AIO, Windows支持, 异步IO, Linux中使用Epoll来模拟实现AIO, 例如Netty框架</span><br></pre></td></tr></table></figure>

<h5 id="shuffle是什么"><a href="#shuffle是什么" class="headerlink" title="shuffle是什么"></a>shuffle是什么</h5><p>spark的shuffle指的是打乱数据重新组合分区的过程,分为shuffle write和shuffle read两个过程</p>
<p>判断是否经过shuffle,看上游一个分区是否对应下游多个分区</p>
<p>需要注意的是,shuffle前后的RDD的分区数量,可能改变,也可能不变,所以说分区数量不变不代表没有经过shuffle</p>
<p>注意需要等待所有分区的数据都进行了shuffle以后才能继续向下执行后续的RDD,所以数据在内存中无法持续等待,所以需要落盘</p>
<p>磁盘的交互就产生了性能的下降,所以可以通过减少落盘数据量而提升性能,例如预聚合操作(类似于MR的combine归约),另一方面,溢出写磁盘的文件越少性能越高</p>
<h5 id="shuffle写磁盘读磁盘过程"><a href="#shuffle写磁盘读磁盘过程" class="headerlink" title="shuffle写磁盘读磁盘过程"></a>shuffle写磁盘读磁盘过程</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">shuffle写磁盘读磁盘过程</span><br><span class="line">写磁盘:即ShuffleMapStage对应的ShuffleMapTask, 重写runTask方法</span><br><span class="line">    dep.shuffleWriterProcessor.write方法, 指的是Shuffle写磁盘处理器</span><br><span class="line">        1.manager = SparkEnv.get.ShuffleManager:是一个抽象接口,是Shuffle管理器,具体实现是SortShuffleManager一种(早期有两种)</span><br><span class="line">        2.writer = manager.getWriter方法中会匹配三种类型的Writer写磁盘方式,见表格; </span><br><span class="line">        3.writer.write(): 以SortShuffleWriter为例,里面有getWriter和write的具体实现方法,见表格</span><br><span class="line">            1.sorter排序器,需要先进行数据排序,要判断是否支持预聚合,使用的排序方式也不同</span><br><span class="line">            2.sorter.insertAll插入数据然后排序</span><br><span class="line">                1.根据预聚合有不同处理方式:changeValue或者insert方法预聚合或者不预聚合</span><br><span class="line">                  相同key把Value相加update,就是预聚合,即changeValue方法</span><br><span class="line">                2.buffer缓冲区的数据被不断插入数据就会触发溢出写到磁盘临时文件中:maybeSpillCollection方法</span><br><span class="line">                  溢出写的内存大小阈值默认5MB并且数据量%32==0,会尝试申请新的内存,否则会溢出写</span><br><span class="line">                  另外一种情况,强制溢出写数据,阈值是超过int最大值</span><br><span class="line">                  内存溢出写磁盘temp临时文件的buffer缓冲区大小默认32KB</span><br><span class="line">                  临时文件temp不一定存在,因为可能数据量少,没有触发溢写</span><br><span class="line">                  溢写过程的排序先按照分区再按照分区内key排序</span><br><span class="line">            3.sorter.writePartitionedMapOutput,区分是否有溢出写</span><br><span class="line">                没有溢出写,直接操作内存数据</span><br><span class="line">                有溢出写,使用Writer写出到磁盘,也就是合并temp临时文件</span><br><span class="line">                合并使用归并排序mergeSort,先按照分区再按照分区内key排序</span><br><span class="line">            4.commitAllPartitions,具体实现不同,只剩下索引文件和数据文件</span><br><span class="line">            </span><br><span class="line">读磁盘:即ResultStage对应的ResultTask, 真正是在ShuffleRDD中的compute方法中的getReader的read读取,在SortShuffleManager实现类中, 使用BlockStoreShuffleReader类进行读取,调用read方法,读取磁盘文件的内存缓冲区48MB	</span><br></pre></td></tr></table></figure>

<h5 id="shuffle演变的过程"><a href="#shuffle演变的过程" class="headerlink" title="shuffle演变的过程"></a>shuffle演变的过程</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">shuffle演变的过程</span><br><span class="line">    方式1: 上游一个Task生成三份文件,下游三个Task分别去读取对应的文件的数据,但是问题在于一个计算core可能包含多个Task,并且还会有多个core,这样会产生大量的小文件,导致性能急速下降</span><br><span class="line">    方式2: 一个core就生成三份文件,多个Task的数据都写入这三份文件中,然后下游三个Task去对应读取,性能高于方式1;但是下游Task的数量很大,而且core的数量也可能很大,还是会产生大量的小文件</span><br><span class="line">    方式3: 只生成一个文件,但是不同的Task的数据要有对应的索引文件,这样下游的三个Task去读取的时候,就按照索引进行读取(即读取具体的哪一段数据),一个core中的多个Task也写入这一个文件; 这种方式就必须排序,不然没有索引的产生</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="spark的shuffle分类"><a href="#spark的shuffle分类" class="headerlink" title="spark的shuffle分类"></a>spark的shuffle分类</h5><p>spark的shuffle分类,或者说ShuffleWriter的实现类,有三种,SortShuffleWriter,BypassMergeSortShuffleWriter,UnsafeShuffleWriter三种,如下	</p>
<table>
<thead>
<tr>
<th>处理器</th>
<th>写磁盘的对象</th>
<th>使用该对象的条件</th>
</tr>
</thead>
<tbody><tr>
<td>SerializedShuffleHandle</td>
<td>UnsafeShuffleWriter</td>
<td>1.序列化规则中必须支持重定位操作(Java序列化不支持,Kryo序列化支持)<br />2.不能使用预聚合功能<br />3.下游分区数量即Task数量&lt;&#x3D;16777216,这个条件一般都能满足</td>
</tr>
<tr>
<td>BypassMergeSortShuffleHandle</td>
<td>BypassMergeSortShuffleWriter</td>
<td>1.不能使用预聚合功能<br />2.下游分区数量即Task数量&lt;&#x3D;200<br />(分区数量一般该参数配置400到500)</td>
</tr>
<tr>
<td>BaseShuffleHandle</td>
<td>SortShuffleWriter</td>
<td>排序Shuffle写磁盘的方式,不满足上面两种就使用这种</td>
</tr>
</tbody></table>
<h5 id="spark的内存模型是什么样的"><a href="#spark的内存模型是什么样的" class="headerlink" title="spark的内存模型是什么样的"></a>spark的内存模型是什么样的</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">spark的内存模型,或者说Executor的内存模型,分为堆内内存和堆外内存,堆内内存受到jvm的管理,而堆外内存是直接向操作系统申请和释放</span><br><span class="line">堆内内存,由spark-submit参数executor-memory指定,多个task任务共享这个内存</span><br><span class="line">堆外内存默认不开启,可以设置开启并且设置大小,堆外内存没有other部分</span><br><span class="line">堆内和堆外都可以再细分包含storage存储内存和execute执行内存,堆内还包含other其他内存</span><br><span class="line">具体划分的比例如下(堆内为例)</span><br><span class="line">	storage存储内存(指的是Executor内存)</span><br><span class="line">	占比: 30%</span><br><span class="line">		缓存数据cache,persist,广播变量</span><br><span class="line">	execute执行内存</span><br><span class="line">	占比: 30%</span><br><span class="line">		Shuffle过程中的操作占用的内存</span><br><span class="line">	other其他内存</span><br><span class="line">	占比; 40%</span><br><span class="line">		系统占用的内存</span><br><span class="line">		RDD元数据信息占用内存</span><br><span class="line">	预留内存: 300MB, 不包含上面三部分内存</span><br><span class="line">	</span><br><span class="line">存储内存和执行内存可以相互占用,</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="https://nfsp412.github.io/asuka.github.io/2024/10/16/bigdata003/" data-id="cm3ae93sl00056cur5jch44pl" data-title="Spark学习笔记" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/asuka.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag">大数据</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata002" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/asuka.github.io/2024/10/15/bigdata002/" class="article-date">
  <time class="dt-published" datetime="2024-10-15T10:20:03.000Z" itemprop="datePublished">2024-10-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/asuka.github.io/2024/10/15/bigdata002/">Hive学习笔记</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>本文是自己在学习Hive过程中整理的一些基础笔记,仅做记录</p>
<h5 id="count的区别"><a href="#count的区别" class="headerlink" title="count的区别"></a>count的区别</h5><p>在hive中</p>
<p>count(字段)，字段可能为null，不会统计null值</p>
<p>count(*)，可以统计到有null的值</p>
<p>count(1)，和count(*)，底层执行计划完全一致，效果一致</p>
<h5 id="能否解压缩后直接使用hive"><a href="#能否解压缩后直接使用hive" class="headerlink" title="能否解压缩后直接使用hive"></a>能否解压缩后直接使用hive</h5><p>可以，因为默认可以使用derby数据库存储元数据，只需要保证hadoop集群启动即可</p>
<h5 id="不启动hiveserver2服务，能否使用hive"><a href="#不启动hiveserver2服务，能否使用hive" class="headerlink" title="不启动hiveserver2服务，能否使用hive"></a>不启动hiveserver2服务，能否使用hive</h5><p>可以，使用命令行客户端使用hive，命令行客户端的使用命令就是hive</p>
<h5 id="不启动metastore服务，能否使用hive"><a href="#不启动metastore服务，能否使用hive" class="headerlink" title="不启动metastore服务，能否使用hive"></a>不启动metastore服务，能否使用hive</h5><p>可以，因为metastore默认有嵌入式，会嵌入到命令行客户端，或者hiveserver2中</p>
<h5 id="metastore默认连接哪里"><a href="#metastore默认连接哪里" class="headerlink" title="metastore默认连接哪里"></a>metastore默认连接哪里</h5><p>只要在hive-site.xml文件中配置了metastore服务的地址，就会去连接该服务，而不再使用内置嵌入式的metastore</p>
<h5 id="有几种建表语法"><a href="#有几种建表语法" class="headerlink" title="有几种建表语法"></a>有几种建表语法</h5><ol>
<li><p>常规建表；这种较为常用</p>
</li>
<li><p>create table 表名 as select；该方式只能建设内部表，如果不想要数据，可以使用where 1&#x3D;2</p>
<p>该方式仍然可以指定注释，分隔符，文件存储格式，location位置，tbl属性</p>
<p>这种较为常用，保留格式例如字段数据类型</p>
</li>
<li><p>create table 表名 like 已存在表；该方式可以创建外部表，不包含数据只有表结构，可以指定分隔符，文件存储格式，location位置，tbl属性，但是无法指定注释，保留格式例如字段数据类型</p>
</li>
</ol>
<h5 id="hive里面的Driver运行在哪里"><a href="#hive里面的Driver运行在哪里" class="headerlink" title="hive里面的Driver运行在哪里"></a>hive里面的Driver运行在哪里</h5><p>如果是cli客户端,运行在该客户端中</p>
<p>如果是jdbc客户端,运行在hiveserver2</p>
<h5 id="hive和hdfs文件的关系是"><a href="#hive和hdfs文件的关系是" class="headerlink" title="hive和hdfs文件的关系是"></a>hive和hdfs文件的关系是</h5><p>Hive中的表在hdfs中是目录</p>
<p>Hive中的数据在hdfs中是该目录下的文件</p>
<h5 id="hiveserver2访问hadoop集群的用户身份是谁"><a href="#hiveserver2访问hadoop集群的用户身份是谁" class="headerlink" title="hiveserver2访问hadoop集群的用户身份是谁"></a>hiveserver2访问hadoop集群的用户身份是谁</h5><p>由Hiveserver2的hive.server2.enable.doAs参数决定</p>
<p>若启用，则Hiveserver2会模拟成客户端的登录用户去访问Hadoop集群的数据,即登录beeline使用-n指定的用户,注意在hadoop的core-site.xml中配置用户组和用户两个配置才能生效</p>
<p>不启用，则Hivesever2会直接使用启动该服务的用户访问Hadoop集群数据,即启动该hiveserver2服务的linux用户</p>
<p>默认开启,推荐开启,实现用户隔离</p>
<h5 id="hive的参数如何设置-优先级是什么"><a href="#hive的参数如何设置-优先级是什么" class="headerlink" title="hive的参数如何设置,优先级是什么"></a>hive的参数如何设置,优先级是什么</h5><p>配置文件,有hive-default.xml和hive-site.xml,对本机所有hive进程都生效</p>
<p>命令行参数,执行hive或者beeline命令时,添加-hiveconf k&#x3D;v 针对本次hive生效</p>
<p>命令行内使用set设置,set k&#x3D;v,针对本次hive生效</p>
<p>配置文件 &lt; 命令行参数 &lt; set</p>
<h5 id="不启动metastore服务能使用beeline客户端吗"><a href="#不启动metastore服务能使用beeline客户端吗" class="headerlink" title="不启动metastore服务能使用beeline客户端吗"></a>不启动metastore服务能使用beeline客户端吗</h5><p>只要启动了hiveserver2服务就能使用beeline客户端,和metastore服务没关系</p>
<p>不启动hiveserver2是无法使用beeline的,但是hive是可以的(前提是hive-site.xml配置了metastore而且开启了metastore)</p>
<h5 id="内部表和外部表"><a href="#内部表和外部表" class="headerlink" title="内部表和外部表"></a>内部表和外部表</h5><p>默认创建的表都是的内部表，有时也被称为管理表。对于内部表，Hive会完全管理表的元数据和数据文件,删除表会删除元数据和数据文件</p>
<p>外部表通常可用于处理其他工具上传的数据文件，对于外部表，Hive只负责管理元数据，不负责管理HDFS中的数据文件,,删除表不会删除数据文件</p>
<h5 id="如何处理json文件的输入"><a href="#如何处理json文件的输入" class="headerlink" title="如何处理json文件的输入"></a>如何处理json文件的输入</h5><p>使用json的serde再结合复杂数据类型即可映射</p>
<p>例如json的数组可以用array映射,花括号但是kv类型一致可以用map.花括号但是每个kv类型不一定一致可以用struct</p>
<p>使用JsonSerDe序列化</p>
<h5 id="hive的表和库本质是什么"><a href="#hive的表和库本质是什么" class="headerlink" title="hive的表和库本质是什么"></a>hive的表和库本质是什么</h5><p>本质就是hdfs的目录</p>
<h5 id="drop和truncate的区别"><a href="#drop和truncate的区别" class="headerlink" title="drop和truncate的区别"></a>drop和truncate的区别</h5><p>drop删除表信息,所以元数据是一定删除的,但是数据文件是否删除要看内部表还是外部表</p>
<p>truncate清除数据,但是元数据即表结构没删除,而且只能操作内部表,显然的</p>
<h5 id="hive或者beeline命令行中能使用hadoop的shell命令吗"><a href="#hive或者beeline命令行中能使用hadoop的shell命令吗" class="headerlink" title="hive或者beeline命令行中能使用hadoop的shell命令吗"></a>hive或者beeline命令行中能使用hadoop的shell命令吗</h5><p>可以,dfs -put xxx这样使用</p>
<h5 id="复杂数据类型有哪些"><a href="#复杂数据类型有哪些" class="headerlink" title="复杂数据类型有哪些"></a>复杂数据类型有哪些</h5><table>
<thead>
<tr>
<th>类型</th>
<th>说明</th>
<th>定义</th>
<th>取值</th>
</tr>
</thead>
<tbody><tr>
<td>array</td>
<td>数组是一组相同类型的值的集合</td>
<td>array<code>&lt;</code>string<code>&gt;</code></td>
<td>arr[0]</td>
</tr>
<tr>
<td>map</td>
<td>map是一组相同类型的键-值对集合</td>
<td>map&lt;string, int&gt;</td>
<td>map[‘key’]</td>
</tr>
<tr>
<td>struct</td>
<td>结构体由多个属性组成，每个属性都有自己的属性名和数据类型</td>
<td>struct&lt;id:int,  name:string&gt;</td>
<td>struct.id</td>
</tr>
</tbody></table>
<h5 id="分区表和分桶表的区别"><a href="#分区表和分桶表的区别" class="headerlink" title="分区表和分桶表的区别"></a>分区表和分桶表的区别</h5><p><strong>分区表</strong>:就是把一张大表的数据按照业务需要分散的存储到多个目录，每个目录就称为该表的一个分区。在查询时通过where子句中的表达式选择查询所需要的分区，这样的查询效率会提高很多,分区分的是文件目录</p>
<p><strong>分桶表</strong>:分桶分的是数据文件,建表关键字clustered by(字段) sorted by(字段)into n buckets,算法仍然是分桶字段的hash值取余桶的数量,可以在建表时额外使用stored by(字段)来指定分桶后的内部排序字段</p>
<h5 id="hive存储格式和压缩格式有哪些"><a href="#hive存储格式和压缩格式有哪些" class="headerlink" title="hive存储格式和压缩格式有哪些"></a>hive存储格式和压缩格式有哪些</h5><p>hive的压缩本质上指的就是hadoop的压缩,hadoop的压缩可以在三个位置设置压缩,输入文件压缩,map阶段输出压缩,reduce阶段输出压缩</p>
<p><strong>输入阶段</strong>: 如果是textfile文本类型,那么有一些压缩文件可以直接读取,无需指定参数或者解压缩</p>
<p>如果是列存储格式,需要在建表语句时指定</p>
<p><strong>计算过程中</strong>: map阶段输出,或者是两个mr任务之间的临时数据阶段,也可以设置压缩,需要开启相关参数</p>
<p>单个mr之间的压缩参数</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 开启MapReduce中间数据压缩功能</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.map.output.compress<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 设置MapReduce中间数据数据的压缩方式（以下示例为snappy）</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.map.output.compress.codec<span class="operator">=</span>org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure>

<p>不同mr之间的压缩参数</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 是否对两个MR之间的临时数据进行压缩</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.compress.intermediate<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 压缩格式（以下示例为snappy）</span></span><br><span class="line"><span class="keyword">set</span> hive.intermediate.compression.codec<span class="operator">=</span> org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure>

<p><strong>输出阶段</strong>: 输出文件或者写入其他表的阶段,也可以设置压缩,需要开启相关参数</p>
<p>导出为文件(insert overwrite local directory语法)的压缩参数,写入表的压缩参数在建表的时候设置即可</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL语句的最终输出结果是否压缩</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.compress.output<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 输出结果的压缩格式（以下示例为snappy）</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.output.fileoutputformat.compress.codec<span class="operator">=</span>org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure>

<h5 id="行存储和列存储区别"><a href="#行存储和列存储区别" class="headerlink" title="行存储和列存储区别"></a>行存储和列存储区别</h5><p>列存储更适合列裁剪查询的场景,效率更高,提高读写数据和处理数据的性能</p>
<p>行存储有textfile,sequence file,列存储有orc,parquet等</p>
<h5 id="文件null值问题"><a href="#文件null值问题" class="headerlink" title="文件null值问题"></a>文件null值问题</h5><p>文件中的\N会转换成hive底层的null值,使用is null进行过滤判断</p>
<h5 id="强制类型转换失败会怎样"><a href="#强制类型转换失败会怎样" class="headerlink" title="强制类型转换失败会怎样"></a>强制类型转换失败会怎样</h5><p>转换失败的话是转换成null，而不是报错，例如string类型的abc转换成int，会变成null</p>
<h5 id="5个by指的是什么"><a href="#5个by指的是什么" class="headerlink" title="5个by指的是什么"></a>5个by指的是什么</h5><ol>
<li>group by按照key分组,常见于单表分组聚合统计,完整的mr任务</li>
<li>distribute by按照指定字段key的hash码值和reducetask个数哈希取模,一般结合sort by使用</li>
<li>order by全局排序,reducetask个数强制为1个,设置多个也不生效,本质是map端排序然后reduce端排序,完整mr任务,一般结合limit使用,因为有优化,假设limit100,那么每个maptask阶段只需要排序后选择100条发送到reducetask阶段即可</li>
<li>sort by指的是reducetask局部排序,可以单独使用,也可以结合distribute by使用,需要手动设置reducetask个数大于1个才有效果,单独使用的时候就是随机分区数据到reduce</li>
<li>当distribute by和sort by字段相同时，可以使用cluster by代替,但是排序只能是升序排序</li>
</ol>
<h5 id="order-by为啥和limit结合使用"><a href="#order-by为啥和limit结合使用" class="headerlink" title="order by为啥和limit结合使用"></a>order by为啥和limit结合使用</h5><p>因为有优化,假设limit100,那么每个maptask阶段只需要排序后选择100条发送到reducetask阶段即可</p>
<h5 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h5><p>UDTF表生成函数,一般结合侧视图使用,例如explode炸裂函数这种,将一行转换成多行</p>
<p>collect_list,根据某个key将多行转换成一行</p>
<p>数学相关如floor向下取整,ceil向上取整,</p>
<p>字符串相关如substring截取,split切割,正则相关函数,例如regexp_replace,可以实现身份证或者手机号的脱敏,coalesce空值过滤,concat字符串拼接,get_json_object解析json</p>
<p>时间日期相关如,unix_timestamp日期转换时间戳,from_unixtime时间戳转换日期,date_format日期解析成指定格式等</p>
<h5 id="一行转多行和多行转一行如何实现"><a href="#一行转多行和多行转一行如何实现" class="headerlink" title="一行转多行和多行转一行如何实现"></a>一行转多行和多行转一行如何实现</h5><p>例如UDTF表生成函数,一般结合侧视图使用,例如explode炸裂函数这种,将一行转换成多行</p>
<p>例如collect_list,根据某个key将多行转换成一行</p>
<h5 id="行转列和列转行如何实现"><a href="#行转列和列转行如何实现" class="headerlink" title="行转列和列转行如何实现"></a>行转列和列转行如何实现</h5><p>行转列指的是,原本是多行数据,比如张三,语数外对应的分数,一共三条数据,我想给他转换成,一条数据,语数外不是字段值了,而是字段名称,所以根据张三进行分组,使用三个max加上case when进行统计,</p>
<p>列转行指的是反过来,使用union all,即select查询张三,以及语文作为字段值,以及对应的分数作为第三列即可</p>
<h5 id="窗口函数有哪些"><a href="#窗口函数有哪些" class="headerlink" title="窗口函数有哪些"></a>窗口函数有哪些</h5><p>窗口函数分为前面的开窗函数部分和后面的窗口范围</p>
<p>开窗函数部分,有排序相关的row_number,可以去重,有dense_rank稠密排序,等,有sum这种聚合函数的,有lag,lead这种跨行取值的,有first_value这种取首值尾值的</p>
<p>后面的窗口范围,分为基于行的范围rows和基于值的范围range</p>
<p>关键字有partition by分区,order by指定排序字段,rows或者range,最后的between and窗口范围</p>
<h5 id="窗口函数的默认值问题"><a href="#窗口函数的默认值问题" class="headerlink" title="窗口函数的默认值问题"></a>窗口函数的默认值问题</h5><p>over窗口内的关键字都可以不写</p>
<p>不写partition,代表不分区</p>
<p>如果是只包含order by,那么代表基于值range,范围是首值到当前值,即累计效果</p>
<p>如果是不包含order by,而且也不包含rows或者range,那么代表rows基于行,全窗口范围,求的是汇总值</p>
<p>注意对于lag和lead,row_number等排序函数,都是不能基于值的范围,而且也不能基于行的范围,即不支持自定义窗口</p>
<h5 id="order-by和range-between-and的关系"><a href="#order-by和range-between-and的关系" class="headerlink" title="order by和range between and的关系"></a>order by和range between and的关系</h5><p>order by在range中,实际上指的不是排序,而是基于order by的字段去划分窗口,因为基于值,排序不排序是无意义的</p>
<p>并且range是基于值的,所以order by的字段,在窗口范围包含了num具体数字的时候,此时一定是能够加减值的数据类型</p>
<p>但是如果不包含num具体数字,那么就不一定非要是能够加减值的数据类型了</p>
<h5 id="有几种去重方式"><a href="#有几种去重方式" class="headerlink" title="有几种去重方式"></a>有几种去重方式</h5><p>distinct</p>
<p>group by</p>
<p>row_number</p>
<p>从执行计划来看,前面两个一样,实际上distinct就是翻译成group by的,从执行计划上面来看是一样的</p>
<p>row_number应该速度慢一些,因为在map阶段并没有进行map side预聚合的操作,所以写入reducetask的数据量肯定是比前面两个更大</p>
<h5 id="hive的二级分区"><a href="#hive的二级分区" class="headerlink" title="hive的二级分区"></a>hive的二级分区</h5><p>二级分区指的是例如按照天进行了分区,再按照小时进行分区的这种操作,也是为了过滤查询效率的增加</p>
<h5 id="hive如何和hdfs的分区目录同步一致性"><a href="#hive如何和hdfs的分区目录同步一致性" class="headerlink" title="hive如何和hdfs的分区目录同步一致性"></a>hive如何和hdfs的分区目录同步一致性</h5><p>如果手动增删hdfs分区目录,使用msck repair table dept_partition sync partitions;这段sql进行分区目录同步一致性</p>
<h5 id="hive的动态分区"><a href="#hive的动态分区" class="headerlink" title="hive的动态分区"></a>hive的动态分区</h5><p>写入数据到分区表时,分区字段的值可以手动指定,但是我们可以使用动态分区的方式</p>
<p>动态分区是指向分区表insert数据时，被写往的分区不由用户指定，而是由每行数据的最后一个字段的值来动态的决定。使用动态分区，可只用一个insert语句将数据写入多个分区</p>
<p>两个参数</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 开启动态分区</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition<span class="operator">=</span><span class="literal">true</span></span><br><span class="line"><span class="comment">-- 开启非严格模式</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition.mode<span class="operator">=</span>nonstrict</span><br></pre></td></tr></table></figure>

<h5 id="hive调优思路"><a href="#hive调优思路" class="headerlink" title="hive调优思路"></a>hive调优思路</h5><ul>
<li>实际上基于mr的引擎的话,指的是mr的调优和yarn的调优</li>
</ul>
<ol>
<li><p>yarn集群的调优主要是设置集群内存cpu资源以及单个container内存cpu资源</p>
</li>
<li><p>针对不同任务调整mr资源参数</p>
<p>在hivesql中,使用set进行设置即可,例如maptask和reducetask的cpu和内存调整</p>
</li>
</ol>
<ul>
<li>sql层面的调优,要先分析执行计划explain,然后再针对性进行sql优化</li>
</ul>
<ol>
<li>map side端预聚合优化,相关参数已经开启,但是需要注意是否能够触发,可以调大占比0.5,这样更容易触发map side,执行计划中map端包含了一个group by就证明走map side预聚合优化了</li>
<li>map join,适用于小表join大表,不经过shuffle过程,效率高,可以使用hint提示在sql中触发,也可以调整参数触发,</li>
<li>在多个小表大小已知的情况下,调整hive.auto.convert.join.noconditionaltask参数尽量保证大于所有小表总和,走最优map join,或者尽量保证大于不同common join任务对应的小表中最大的那个也行,会无法合并但是也能提升效率</li>
<li>在存在小表大小未知的情况下,只能走条件任务,此时调整hive.mapjoin.smalltable.filesize参数尽量保证大于所有小表中最大的那个,保证有map join计划生成,同时保留common join备选</li>
<li>尽量满足条件走bucket map join和sort merge bucket map join</li>
</ol>
<ul>
<li><p>解决数据倾斜</p>
</li>
<li><p>设置合理的并行度,即map端并行度和reduce端并行度,</p>
</li>
</ul>
<ol>
<li>map端默认输入使用的是CombineHiveInputFormat,会对小文件进行优化,多个小文件合并一个切片处理,如果map端做了一些etl操作,比如json的解析,正则匹配进行脱敏等,可以调整切片大小,默认是大约256mb进行的,这个参数和hadoop的mr的默认值是不一样的,这里注意,</li>
<li>针对reduce端的并行度,该端文件大小也是约256mb大小,mapreduce.job.reduces参数可以调整reduce个数</li>
</ol>
<h5 id="小文件的处理方式"><a href="#小文件的处理方式" class="headerlink" title="小文件的处理方式"></a>小文件的处理方式</h5><ul>
<li>从hadoop或者说hdfs存储层面,</li>
</ul>
<ol>
<li>使用hadoop archive -archiveName input.har -p  &#x2F;small   &#x2F;output_small,实际上是一个mr任务,进行合并小文件,NN认为是一个文件,实际上还是多个小文件</li>
</ol>
<ul>
<li>从mr程序执行的角度</li>
</ul>
<ol>
<li>使用CombineTextInputFormat代替TextInputFormat类,去读取文件,设置合理的切片值,默认是4MB</li>
<li>使用jvm重用,即开启uber模式参数</li>
</ol>
<ul>
<li>从hivesql角度,</li>
</ul>
<ol>
<li>map端默认是使用了CombineHiveInputFormat,即自动合并了小文件进行处理</li>
<li>reduce端可以自行根据数据量设置并行度,即mapreduce.job.reduce,默认-1自动估算的,mr整体数据量除以约256mb,再求ceil,然后和1009默认值取min即可,所以一般会等同于map阶段的并行度,这样输出的小文件就合并只有一个了</li>
</ol>
<h5 id="数据倾斜解决思路"><a href="#数据倾斜解决思路" class="headerlink" title="数据倾斜解决思路"></a>数据倾斜解决思路</h5><p>数据倾斜产生的本质就是某个reduce包含的数据远远大于其他reduce,也就是某个key的数据远远大于其他数据</p>
<ul>
<li>分组聚合的数据倾斜,是因为某个key的数据量太大导致下游reduce处理数据量过大产生</li>
</ul>
<ol>
<li><p>map side预聚合,如果单纯做一些分组聚合的统计,比如pvuv计算,那么实际上可以走map side预聚合,通过执行计划查看是否在map阶段产生了group by即可,有一个参数是聚合后条数占比聚合前条数,默认是0.5,还有一个是检测预聚合的抽样条数默认是10w条,可以调整这两个参数来让他走map side聚合,总之可以通过执行计划和日志进行分析</p>
</li>
<li><p>使用skew group by方式,原理就是启动两个mr任务,第一个按照随机数分区,这样reduce端是打散的,第二个mr再按照分组字段进行分组聚合</p>
<p>首先要开启skew参数,默认是没开启的,就是hive.groupby.skewindata</p>
</li>
</ol>
<ul>
<li>join产生的数据倾斜,默认是common join,所以如果关联字段的值分步不均匀,就可能在reduce端产生数据倾斜</li>
</ul>
<ol>
<li>走map join</li>
<li>使用skew join,类似于skew group by,为那个大key单独启动一个mr任务,其余的使用另一个mr任务走common join,并且单独的mr任务使用map join,实际上这种是为了解决大表join大表的数据倾斜的情况的,并且要求某一张表的倾斜key的数据量较小,方便走map join,判断的条件是某个key的行数大于10w行这个条件,大于就这样走skew join,这也是一个运行时的策略,只有在运行时才能判断</li>
<li>调整sql语句,针对倾斜大表进行倾斜key的随机数打散,打散的方式可以使用rand()*打散倍数,拼接key,针对另一张表进行关联字段扩容,扩容的方式是针对key拼接打散随机数值,然后union all,例如</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span> <span class="operator">*</span>,concat(province_id,<span class="string">&#x27;_&#x27;</span>,<span class="built_in">cast</span>(rand()<span class="operator">*</span><span class="number">6</span> <span class="keyword">as</span> <span class="type">int</span>)) pid</span><br><span class="line">    <span class="keyword">from</span> order_detail</span><br><span class="line">     )ta</span><br><span class="line"><span class="keyword">join</span> (</span><br><span class="line">    <span class="keyword">select</span> <span class="operator">*</span>,concat(id,<span class="string">&#x27;_&#x27;</span>,<span class="number">0</span>) pid</span><br><span class="line">    <span class="keyword">from</span> province_info</span><br><span class="line">    <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line">    <span class="keyword">select</span> <span class="operator">*</span>,concat(id,<span class="string">&#x27;_&#x27;</span>,<span class="number">1</span>) pid</span><br><span class="line">    <span class="keyword">from</span> province_info</span><br><span class="line">    <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line">    <span class="keyword">select</span> <span class="operator">*</span>,concat(id,<span class="string">&#x27;_&#x27;</span>,<span class="number">2</span>) pid</span><br><span class="line">    <span class="keyword">from</span> province_info</span><br><span class="line">    <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line">    <span class="keyword">select</span> <span class="operator">*</span>,concat(id,<span class="string">&#x27;_&#x27;</span>,<span class="number">3</span>) pid</span><br><span class="line">    <span class="keyword">from</span> province_info</span><br><span class="line">    <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line">    <span class="keyword">select</span> <span class="operator">*</span>,concat(id,<span class="string">&#x27;_&#x27;</span>,<span class="number">4</span>) pid</span><br><span class="line">    <span class="keyword">from</span> province_info</span><br><span class="line">    <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line">    <span class="keyword">select</span> <span class="operator">*</span>,concat(id,<span class="string">&#x27;_&#x27;</span>,<span class="number">5</span>) pid</span><br><span class="line">    <span class="keyword">from</span> province_info</span><br><span class="line">)tb</span><br><span class="line"><span class="keyword">on</span> ta.pid<span class="operator">=</span>tb.pid;</span><br></pre></td></tr></table></figure>

<h5 id="hive中有几种join"><a href="#hive中有几种join" class="headerlink" title="hive中有几种join"></a>hive中有几种join</h5><ol>
<li><p>最普通的common join,由一个完整mr构成,一个sql语句中的相邻的且关联字段相同的多个join操作可以合并为一个Common Join任务</p>
</li>
<li><p>map join,小表驱动大表,由两个map阶段构成,无需经过shuffle,性能好</p>
</li>
<li><p>Bucket Map Join,可用于大表join大表的场景,join的表均为分桶表，且关联字段为分桶字段，且其中一张表的分桶数量是另外一张表分桶数量的整数倍</p>
</li>
<li><p>Sort Merge Bucket Map Join ,可用于大表join大表的场景,join的表均为分桶表，且需保证分桶内的数据是有序的，且分桶字段、排序字段和关联字段为相同字段，且其中一张表的分桶数量是另外一张表分桶数量的整数倍</p>
<p>效率更高,因为无需对整个桶构建hash table,也不需要缓存整个桶,每个mapper只需要按照顺序逐个key的方式去读取数据即可</p>
</li>
</ol>
<h5 id="执行计划查看"><a href="#执行计划查看" class="headerlink" title="执行计划查看"></a>执行计划查看</h5><p>使用explain关键字</p>
<p>stage对应的是mr任务</p>
<p>map operator tree和reduce operator tree对应map阶段和reduce阶段</p>
<p>操作树由一系列的operator组成,例如table scan operator等等,一个operator就代表一个单一操作</p>
<h5 id="map-join源码了解"><a href="#map-join源码了解" class="headerlink" title="map join源码了解"></a>map join源码了解</h5><ol>
<li>先判断hive.auto.convert.join是否是true,默认true,寻找大表候选人,大表候选人如何寻找是按照sql语句寻找的,即left join,right join的主表就作为大表候选人,根据执行计划,inner join顺序无所谓,full join不能走map join</li>
<li>有候选人后,进一步判断hive.auto.convert.join.noconditionaltask,即走不走条件任务,默认true即不走条件任务,不进行common join的后备计划,否则走条件任务</li>
<li>进一步判断hive.auto.convert.join.noconditionaltask.size,大表候选人以外的表大小均已知,并且sum总和小于这个参数,生成最优map join计划,否则走条件任务</li>
<li>条件任务的判断,是去排除一定不能生效的map join计划,然后看还有没有能够使用的map join计划,有的话额外添加common join作为备选,然后运行时决定,判断map join不能生效的两个条件是,大表候选人的大小已知并且之外的表总和大于hive.mapjoin.smalltable.filesize,默认250000即约244kb</li>
<li>生成最优map join计划时,判断是否能够map join合并,指的是原本的多个common join生成的多个map join能否合并,看的就是hive.auto.convert.join.noconditionaltask.size的大小</li>
</ol>
<h5 id="hive源码了解"><a href="#hive源码了解" class="headerlink" title="hive源码了解"></a>hive源码了解</h5><p>解析器,编译器,优化器,执行器,本质上就是hadoop的一个客户端程序</p>
<p>流程是:</p>
<ol>
<li>Antlr框架解析hivesql形成抽象语法树,AST</li>
<li>抽象出基本查询块,并转换成操作树</li>
<li>使用逻辑优化器进行优化,</li>
<li>然后转换成物理执行计划,并进行优化,mr任务没有具体实现因为默认就是mr任务</li>
<li>最后形成mr任务执行</li>
</ol>
<p>具体代码如下</p>
<p>启动主类是CliDriver类,找到main方法,</p>
<ul>
<li>解析命令行参数,</li>
<li>定义输入输出流,</li>
<li>按照分号切分sql语句</li>
</ul>
<p>Driver类</p>
<ul>
<li>解析器将sql解析为AST; ParseDriver类的parse方法</li>
<li>编译器转换成基本查询块,SemanticAnalyzer类analyzeInternal方法<ul>
<li>转换成基本查询块,</li>
<li>转换成操作树</li>
<li>逻辑优化器逻辑优化,例如各种ParseContext的实现类的transform方法</li>
<li>生成task tree </li>
<li>物理优化器物理优化,例如 <a href="apache-hive-3.1.2-src%5Cql%5Csrc%5Cjava%5Corg%5Capache%5Chadoop%5Chive%5Cql%5Cparse%5Cspark%5CSparkCompiler.java">SparkCompiler.java</a> 类的optimizeOperatorPlan方法</li>
</ul>
</li>
<li>提交任务TaskRunner类,线程类,run方法,具体是ExecDriver 类的execute方法,最终调用到mr源码里面提交任务了<ul>
<li>mr临时目录</li>
<li>分区器</li>
<li>mapper和reducer</li>
<li>提交任务</li>
</ul>
</li>
</ul>
<h5 id="远程debug如何操作"><a href="#远程debug如何操作" class="headerlink" title="远程debug如何操作"></a>远程debug如何操作</h5><p>idea使用远程debug模式,连接ip和端口,端口默认8000</p>
<p>hive –debug启动</p>
<p>然后开启debug模式</p>
<p>在循环输入sql语句就可以开始debug了</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://nfsp412.github.io/asuka.github.io/2024/10/15/bigdata002/" data-id="cm3ae93sj00026curdr3v927s" data-title="Hive学习笔记" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/asuka.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag">大数据</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata001" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/asuka.github.io/2024/10/09/bigdata001/" class="article-date">
  <time class="dt-published" datetime="2024-10-09T14:30:48.000Z" itemprop="datePublished">2024-10-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/asuka.github.io/2024/10/09/bigdata001/">Hadoop学习笔记</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>本文是自己在学习Hadoop过程中整理的一些基础笔记,仅做记录</p>
<h5 id="有哪些命令"><a href="#有哪些命令" class="headerlink" title="有哪些命令?"></a>有哪些命令?</h5><p>命令行操作方式</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">上传</span></span><br><span class="line">hadoop fs put </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">下载</span></span><br><span class="line">hadoop fs get </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">列举明细</span></span><br><span class="line">hadoop fs ls</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看文件</span></span><br><span class="line">hadoop fs cat</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建目录</span></span><br><span class="line">hadoop fs mkdir</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">复制</span></span><br><span class="line">hadoop fs cp</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">移动</span> </span><br><span class="line">hadoop fs mv</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">删除</span></span><br><span class="line">hadoop fs rm -r </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">大小</span></span><br><span class="line">hadoop fs du</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">用来提交mr程序job</span></span><br><span class="line">hadoop jar jarpath /input /output</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看fsimage文件</span></span><br><span class="line">hdfs oiv filename</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看edits</span></span><br><span class="line">hdfs oev filename</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置副本数量</span></span><br><span class="line">hadoop fs -setrep 3 filename</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">列出所有任务</span></span><br><span class="line">yarn application -list</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">根据具体状态列出任务</span></span><br><span class="line">yarn application -list -appStates</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">杀死任务</span></span><br><span class="line">yarn application -kill appID</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查询任务的日志，没有页面的时候很常用</span></span><br><span class="line">yarn logs -applicationId appID</span><br><span class="line">yarn logs -applicationId appID | less</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">列出所有的NM节点</span></span><br><span class="line">yarn node -list -all</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查询队列情况</span></span><br><span class="line">yarn queue -status QueueName</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">新加入磁盘时生成均衡计划</span></span><br><span class="line">hdfs diskbalancer -plan hostname</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">新加入磁盘时执行均衡计划</span></span><br><span class="line">hdfs diskbalancer -execute hostname.plan.json</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">刷新NameNode</span></span><br><span class="line">hdfs dfsadmin -refreshNodes</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">新加入服务器节点时开启数据均衡</span></span><br><span class="line">start-balancer.sh -threshold 10</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">强制转换成standby节点</span></span><br><span class="line">hdfs haadmin -transitionToStandby NN --forcemanual </span><br></pre></td></tr></table></figure>

<p>API操作方式</p>
<p>添加依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>简单演示</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1 获取文件系统</span></span><br><span class="line"><span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 需要配置用户,否则无权限</span></span><br><span class="line"><span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://ip:port&quot;</span>), configuration, <span class="string">&quot;user&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3 创建目录</span></span><br><span class="line">fs.mkdirs(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/path&quot;</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4 关闭资源</span></span><br><span class="line">fs.close();</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/r3.4.0/hadoop-project-dist/hadoop-common/FileSystemShell.html">官网链接</a></p>
<h5 id="参数的优先级"><a href="#参数的优先级" class="headerlink" title="参数的优先级"></a>参数的优先级</h5><ol>
<li>客户端代码中设置的值（configuration.set(“key”,”value”)）</li>
<li>classpath下的用户自定义配置文件（即resources目录下配置文件）</li>
<li>服务器的自定义配置（xxx-site.xml）</li>
<li>服务器的默认配置（xxx-default.xml）</li>
</ol>
<h5 id="如何保证数据完整性"><a href="#如何保证数据完整性" class="headerlink" title="如何保证数据完整性"></a>如何保证数据完整性</h5><p>DN读取数据块时，会计算checksum校验和，然后和创建时的校验和对比，相同则认为是数据完整的</p>
<p>读写数据时也是按照512字节的chunk加上4字节的checksum校验和，凑够64kb以packet形式发送</p>
<p>校验算法：crc，md5，sha1等</p>
<h5 id="fsimage文件中为何没有记录数据块所对应datanode服务器信息"><a href="#fsimage文件中为何没有记录数据块所对应datanode服务器信息" class="headerlink" title="fsimage文件中为何没有记录数据块所对应datanode服务器信息"></a>fsimage文件中为何没有记录数据块所对应datanode服务器信息</h5><p>是因为在集群启动后，要求DataNode上报数据块信息，并间隔一段时间后再次上报,所以没有记录</p>
<h5 id="NameNode如何确定下次开机启动的时候合并哪些Edits？"><a href="#NameNode如何确定下次开机启动的时候合并哪些Edits？" class="headerlink" title="NameNode如何确定下次开机启动的时候合并哪些Edits？"></a>NameNode如何确定下次开机启动的时候合并哪些Edits？</h5><p>fsimage文件选择最新序号,再加上edits文件从该序号之后的文件,即为需要合并的文件</p>
<p>但是注意的是,namenode中还存在inprogress最新操作文件,这个是snn中没有的,丢失是可能发生的</p>
<p>生产常用HA高可用</p>
<h5 id="maptask并行度和什么有关系"><a href="#maptask并行度和什么有关系" class="headerlink" title="maptask并行度和什么有关系?"></a>maptask并行度和什么有关系?</h5><p>数据切片是MapReduce程序计算输入数据的单位，一个切片会对应启动一个MapTask</p>
<p>默认切片大小为block块大小,即默认128MB,,注意IDEA调试的时候默认是32MB块</p>
<p>是针对每一个文件进行切片计算</p>
<p>注意判断条件是128MB的1.1倍,大于就切片128MB,小于就不切了,一整个作为一片</p>
<h5 id="分区-分文件-数和reducetask个数的关系"><a href="#分区-分文件-数和reducetask个数的关系" class="headerlink" title="分区(分文件)数和reducetask个数的关系"></a>分区(分文件)数和reducetask个数的关系</h5><p>如果reducetask个数(默认1)大于分区数,那么多产生空part-r-000xx文件</p>
<p>如果大于1小于分区数,会抛出异常,因为有一些数据没地方去</p>
<p>如果为1,那么最终只有一个part-r-00000</p>
<p>分区编号从0开始,在自定义分区器时需要注意</p>
<h5 id="reduce方法调用次数"><a href="#reduce方法调用次数" class="headerlink" title="reduce方法调用次数?"></a>reduce方法调用次数?</h5><p>和分组key数量有关,注意如果是bean类型,且实现排序接口,那么会按照排序属性字段分组</p>
<p>注意区分reduce方法调用次数和reducetask个数的区别</p>
<h5 id="reducetask和maptask个数"><a href="#reducetask和maptask个数" class="headerlink" title="reducetask和maptask个数?"></a>reducetask和maptask个数?</h5><ol>
<li>reducetask个数是手动指定的,默认1</li>
<li>maptask个数看切片个数</li>
</ol>
<h5 id="如果数据不均衡（某个节点数据少，其他节点数据多），怎么处理？"><a href="#如果数据不均衡（某个节点数据少，其他节点数据多），怎么处理？" class="headerlink" title="如果数据不均衡（某个节点数据少，其他节点数据多），怎么处理？"></a>如果数据不均衡（某个节点数据少，其他节点数据多），怎么处理？</h5><p>节点间数据均衡,使用sbin&#x2F;start-balancer.sh -threshold 10命令,10代表差异不超过10%</p>
<p>停止命令sbin&#x2F;stop-balancer.sh</p>
<p>尽量不在nn节点进行</p>
<h5 id="小文件怎么处理"><a href="#小文件怎么处理" class="headerlink" title="小文件怎么处理?"></a>小文件怎么处理?</h5><ol>
<li>使用CombineTextInputFormat代替TextInputFormat类,去读取文件,设置合理的切片值,默认是4MB</li>
<li>使用hadoop archive -archiveName input.har -p  &#x2F;small   &#x2F;output_small,实际上是一个mr任务,进行合并小文件,NN认为是一个文件,实际上还是多个小文件</li>
<li>使用jvm重用,即开启uber模式参数</li>
</ol>
<h5 id="mr优化的参数有哪些"><a href="#mr优化的参数有哪些" class="headerlink" title="mr优化的参数有哪些?"></a>mr优化的参数有哪些?</h5><ol>
<li><p>解决数据倾斜,使用自定义的分区类</p>
</li>
<li><p>相关参数的设置,例如环形缓冲区增大,溢出阈值,merge合并次数增加,reduce拉取的并行度增加,maptask和reducetask的cpu和内存的适当增加</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">增加环形缓冲区大小从100mb到更多</span><br><span class="line">mapreduce.task.io.sort.mb = 200</span><br><span class="line"></span><br><span class="line">增大阈值从80到更多</span><br><span class="line">mapreduce.map.sort.spill.percent = 90</span><br><span class="line"></span><br><span class="line">增加Merge次数从10到更多</span><br><span class="line">mapreduce.task.io.sort.factor = 20</span><br><span class="line"></span><br><span class="line">内存,cpu</span><br><span class="line">mapreduce.map.memory.mb</span><br><span class="line">mapreduce.map.cpu.vcores</span><br><span class="line">mapreduce.reduce.memory.mb</span><br><span class="line">mapreduce.reduce.cpu.vcores</span><br><span class="line"></span><br><span class="line">异常重试默认4次到更多</span><br><span class="line">mapreduce.map.maxattempts = 4</span><br><span class="line"></span><br><span class="line">reduce主动拉取的并行度,默认5到更多</span><br><span class="line">mapreduce.reduce.shuffle.Parallelcopies = 10</span><br><span class="line"></span><br><span class="line">拉取后先内存后磁盘,buffer大小,默认占比0.7到更多</span><br><span class="line">mapreduce.reduce.shuffle.input.buffer.percent = 0.8</span><br><span class="line"></span><br><span class="line">buffer数据占比开始溢出默认0.66到更多</span><br><span class="line">mapreduce.reduce.shuffle.merge.percent = 0.75</span><br><span class="line"></span><br><span class="line">reduce重试次数默认4次到更多</span><br><span class="line">mapreduce.reduce.maxattempts = 4</span><br></pre></td></tr></table></figure>
</li>
<li><p>三个阶段都考虑使用压缩</p>
</li>
</ol>
<h5 id="mr程序的数据倾斜怎么处理"><a href="#mr程序的数据倾斜怎么处理" class="headerlink" title="mr程序的数据倾斜怎么处理?"></a>mr程序的数据倾斜怎么处理?</h5><p>详见hive数据倾斜处理方式</p>
<h5 id="namenode启动源码的了解"><a href="#namenode启动源码的了解" class="headerlink" title="namenode启动源码的了解"></a>namenode启动源码的了解</h5><p>NameNode类的main方法</p>
<ol>
<li>启动9870端口服务,使用HttpServer2类</li>
<li>加载fsimage和edits log文件</li>
<li>初始化nn的rpc服务端,使用NameNodeRpcServer类</li>
<li>nn启动时的资源检查,心跳超时判断,超时10min+3s时间,安全模式检查,阈值是0.999,即1000块中只能有一个块失败</li>
</ol>
<h5 id="datanode启动源码的了解"><a href="#datanode启动源码的了解" class="headerlink" title="datanode启动源码的了解"></a>datanode启动源码的了解</h5><p>DataNode类的main方法</p>
<ol>
<li>初始化DataXceiverServer,这个对象和hdfs的上传有关</li>
<li>初始化http服务,使用HttpServer2类</li>
<li>初始化dn的rpc服务器,即创建了一个rpc客户端</li>
<li>dn向nn注册,发送心跳</li>
</ol>
<p>	</p>
<h5 id="hdfs上传源码的了解"><a href="#hdfs上传源码的了解" class="headerlink" title="hdfs上传源码的了解"></a>hdfs上传源码的了解</h5><ol>
<li>创建过程源码,以create方法为例,实际上调用的是NameNodeRpcServer的create方法</li>
<li>该方法创建DFSOutputStream输出流对象,相关参数是block 128mb,packet 64kb,chunk 512bytes,checksum 4bytes</li>
<li>该对象调用start方法,实际上是DataStreamer线程的start方法</li>
<li>创建了dataQueue队列和ackQueue队列</li>
<li>写入过程源码,以write方法为例,实际上最终也是调用到了 DFSOutputStream输出流对象的writeChunk方法</li>
<li>先按照512+4bytes写入packet,等packet64kb满了以后,向dataQueue队列添加,唤醒DataStreamer线程</li>
<li>DataStreamer线程会调用setPipeline创建管道,机架感知,获取存储位置信息</li>
<li>创建Sender线程调用send方法使用socket方式发送packet,发送一个packet就从dataQueue队列移除,然后添加到ackQueue队列</li>
<li>同时启动了ResponseProcessor线程用来接收ack响应,存储成功后再从ackQueue队列移除该packet,不成功就把这个packet再次添加到dataQueue队列</li>
</ol>
<h5 id="yarn提交任务源码的了解"><a href="#yarn提交任务源码的了解" class="headerlink" title="yarn提交任务源码的了解"></a>yarn提交任务源码的了解</h5><ol>
<li>在自定义的mr程序中,Driver驱动类中,job执行waitForCompletion方法的过程为例</li>
<li>客户端向rm提交job,调用到submit方法,最终获取jobid,指定提交路径,即.staging路径,然后进入切片环节,即默认TextFileInputFormat类,然后生成job.split切片信息和job.xml配置信息还有jar包,准备工作做好进入真正提交任务流程</li>
<li>rm启动am过程,上面提交完任务后,会拼接一个shell命令,rm执行这个命令就启动了am,在yarn环境下,对应的是MRAppMaster对象</li>
<li>启动MRAppMaster类,执行main方法,会创建一个dispatcher调度器,然后把任务job放在yarn的queue队列中</li>
<li>YarnChild类是队列中任务执行的类,执行该类main方法,创建Task对象后执行run方法,该Task对象有MapTask和ReduceTask两个实现类</li>
<li>MapTask分为map阶段和sort阶段,最终调用到了我们自定义程序的Mapper的map方法</li>
<li>ReduceTask分为copy阶段,sort阶段,reduce阶段,同样是最终调用到了我们自定义程序的Reducer的reduce方法</li>
</ol>
<h5 id="Shuffle工作机制"><a href="#Shuffle工作机制" class="headerlink" title="Shuffle工作机制"></a>Shuffle工作机制</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1 map方法对于数据进行一行一行的处理，得到的结果是kv对，调用collect方法，放入环形缓冲区</span><br><span class="line">2 放入环形缓冲区之前会打上分区标记，调用getPartitioner方法</span><br><span class="line">3 到达一定比例后开始溢写到磁盘，在溢写过程中会进行快排，即sortAndSpill</span><br><span class="line">4 这个环节可以预先进行map端聚合，即归约环节，减少进入reduce阶段的数据量；也可以进行压缩</span><br><span class="line">5 多个溢写的文件会进行多轮递归的归并排序，每一轮合并10个文件。最终形成一个file.out数据文件和file.out.index索引文件，即mergeParts方法</span><br><span class="line">6 reduce阶段的reducetask主动去拉取自己分区的数据，即copy环节</span><br><span class="line">7 由于可能来自于多个maptask文件，所以会优先内存其次磁盘的进行归并排序，sort环节</span><br><span class="line">8 合并以后shuffle过程就结束了，会针对每一个key的所有的数据执行reduce方法</span><br></pre></td></tr></table></figure>

<h5 id="mr任务的排序"><a href="#mr任务的排序" class="headerlink" title="mr任务的排序"></a>mr任务的排序</h5><p>maptask和reducetask都会对key进行排序，是默认行为，所以key必须实现了排序</p>
<p>默认是字典序，快排</p>
<p>maptask两次排序；快排时先按照分区编号排序，再按照编号内部的key排序，</p>
<p>reducetask一次排序</p>
<h5 id="yarn调度策略对比"><a href="#yarn调度策略对比" class="headerlink" title="yarn调度策略对比"></a>yarn调度策略对比</h5><p>CDH默认公平调度器</p>
<p>Apache默认容量调度器</p>
<p>容量调度器和公平调度器的对比：</p>
<p>相同点：都支持多队列，每个队列内部使用FIFO；每个队列可以设置资源的下限和上限；某个队列资源有剩余，可以暂时共享给其他队列；支持多用户和多程序运行</p>
<p>不同点：对于队列资源的分配方式不同，容量调度器优先选择资源利用率低的队列，而公平调度器优先选择对资源的缺额比例大的队列；容量调度器队列内部的资源分配使用FIFO，DRF，而公平调度器额外还能使用FAIR</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://nfsp412.github.io/asuka.github.io/2024/10/09/bigdata001/" data-id="cm3ae93sg00016cur9m8349rp" data-title="Hadoop学习笔记" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/asuka.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag">大数据</a></li></ul>

    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/asuka.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag">大数据</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/asuka.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" style="font-size: 10px;">大数据</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/asuka.github.io/archives/2024/10/">十月 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/asuka.github.io/2024/10/24/bigdata005/">Kafka学习笔记</a>
          </li>
        
          <li>
            <a href="/asuka.github.io/2024/10/23/bigdata006/">HBase学习笔记</a>
          </li>
        
          <li>
            <a href="/asuka.github.io/2024/10/21/bigdata007/">Clickhouse学习笔记</a>
          </li>
        
          <li>
            <a href="/asuka.github.io/2024/10/21/bigdata004/">Flink学习笔记</a>
          </li>
        
          <li>
            <a href="/asuka.github.io/2024/10/16/bigdata003/">Spark学习笔记</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 asuka<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/asuka.github.io/" class="mobile-nav-link">Home</a>
  
    <a href="/asuka.github.io/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/asuka.github.io/js/jquery-3.6.4.min.js"></script>



  
<script src="/asuka.github.io/fancybox/jquery.fancybox.min.js"></script>




<script src="/asuka.github.io/js/script.js"></script>





  </div>
</body>
</html>