<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Spark学习笔记 | Asuka的个人笔记</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="本文是自己在学习Spark过程中整理的一些基础笔记,仅做记录 Spark初体验spark常用默认端口12344040：每一个spark-shell产生的端口8080：standalone模式下的集群监控18080：历史服务地址7077:连接master的内部通信端口  spark的不同命令行命令有哪些?12345spark-shell  交互式输入scala代码进行开发pyspark 使用pyth">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark学习笔记">
<meta property="og:url" content="https://nfsp412.github.io/asuka.github.io/2024/10/16/bigdata003/index.html">
<meta property="og:site_name" content="Asuka的个人笔记">
<meta property="og:description" content="本文是自己在学习Spark过程中整理的一些基础笔记,仅做记录 Spark初体验spark常用默认端口12344040：每一个spark-shell产生的端口8080：standalone模式下的集群监控18080：历史服务地址7077:连接master的内部通信端口  spark的不同命令行命令有哪些?12345spark-shell  交互式输入scala代码进行开发pyspark 使用pyth">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-10-16T01:30:13.000Z">
<meta property="article:modified_time" content="2024-11-09T16:21:32.591Z">
<meta property="article:author" content="asuka">
<meta property="article:tag" content="大数据">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/asuka.github.io/atom.xml" title="Asuka的个人笔记" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/asuka.github.io/favicon.png">
  
  
  
<link rel="stylesheet" href="/asuka.github.io/css/style.css">

  
    
<link rel="stylesheet" href="/asuka.github.io/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/asuka.github.io/" id="logo">Asuka的个人笔记</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/asuka.github.io/">Home</a>
        
          <a class="main-nav-link" href="/asuka.github.io/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/asuka.github.io/atom.xml" title="RSS 订阅"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="搜索"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://nfsp412.github.io/asuka.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-bigdata003" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/asuka.github.io/2024/10/16/bigdata003/" class="article-date">
  <time class="dt-published" datetime="2024-10-16T01:30:13.000Z" itemprop="datePublished">2024-10-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Spark学习笔记
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>本文是自己在学习Spark过程中整理的一些基础笔记,仅做记录</p>
<h3 id="Spark初体验"><a href="#Spark初体验" class="headerlink" title="Spark初体验"></a>Spark初体验</h3><h5 id="spark常用默认端口"><a href="#spark常用默认端口" class="headerlink" title="spark常用默认端口"></a>spark常用默认端口</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">4040：每一个spark-shell产生的端口</span><br><span class="line">8080：standalone模式下的集群监控</span><br><span class="line">18080：历史服务地址</span><br><span class="line">7077:连接master的内部通信端口</span><br></pre></td></tr></table></figure>

<h5 id="spark的不同命令行命令有哪些"><a href="#spark的不同命令行命令有哪些" class="headerlink" title="spark的不同命令行命令有哪些?"></a>spark的不同命令行命令有哪些?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spark-shell  交互式输入scala代码进行开发</span><br><span class="line">pyspark 使用python进行交互式开发</span><br><span class="line">spark-sql sql命令行</span><br><span class="line">beeline 类似于Hive的beeline客户端</span><br><span class="line">spark-submit 提交spark任务,例如jar包或者py文件等</span><br></pre></td></tr></table></figure>

<h5 id="spark提交任务速度如何优化"><a href="#spark提交任务速度如何优化" class="headerlink" title="spark提交任务速度如何优化?"></a>spark提交任务速度如何优化?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">上传jars到HDFS</span><br><span class="line">在spark-defaults.conf配置</span><br><span class="line">spark.yarn.jars=hdfs上传的jar包地址</span><br></pre></td></tr></table></figure>

<h5 id="常见进程有哪些"><a href="#常见进程有哪些" class="headerlink" title="常见进程有哪些"></a>常见进程有哪些</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">local模式 </span><br><span class="line">	SparkSubmit进程</span><br><span class="line">yarn模式-client模式  </span><br><span class="line">	SparkSubmit 1G1C </span><br><span class="line">	ExecutorLauncher(client模式) 2G1C  即am进程</span><br><span class="line">	YarnCoarseGrainedExecutorBackend 2G1C</span><br><span class="line">	共7G4C</span><br><span class="line">yarn模式-cluster模式</span><br><span class="line">	SparkSubmit 2G1C </span><br><span class="line">	ApplicationMaster(cluster模式) 2G1C  即am进程</span><br><span class="line">	YarnCoarseGrainedExecutorBackend 2G1C</span><br><span class="line">	共8G4C</span><br></pre></td></tr></table></figure>

<h5 id="spark比hive-hadoop-mr快的原因"><a href="#spark比hive-hadoop-mr快的原因" class="headerlink" title="spark比hive&#x2F;hadoop&#x2F;mr快的原因"></a>spark比hive&#x2F;hadoop&#x2F;mr快的原因</h5><ul>
<li>Spark 和 Hadoop 的根本差异是多个作业之间的数据通信问题 : Spark 多个作业之间数据通信是基于内存，而 Hadoop 是基于磁盘  </li>
<li>Spark 只有在 shuffle 的时候将数据写入磁盘，而 Hadoop 中多个 MR 作业之间的数据交互都要依赖于磁盘交互  </li>
<li>Spark Task 的启动时间快。 Spark 采用 fork 线程的方式，而 Hadoop 采用创建新的进程的方式</li>
</ul>
<h5 id="spark有几种运行模式"><a href="#spark有几种运行模式" class="headerlink" title="spark有几种运行模式"></a>spark有几种运行模式</h5><ul>
<li>本地模式,解压直接使用,4040端口查看页面</li>
<li>standalone模式,即资源调度是交给spark本身进行,需要配置master和worker,master的端口8080查看页面,不依赖hive和hadoop,该方式提交任务会产生一个sparksubmit进程和多个executor进程,,executor内存默认1024mb,核数默认和linux核数一致</li>
<li>yarn模式,有client和cluster两种,区别在于Driver运行的位置,是否运行在am中</li>
</ul>
<h5 id="spark提交任务有哪些参数"><a href="#spark提交任务有哪些参数" class="headerlink" title="spark提交任务有哪些参数"></a>spark提交任务有哪些参数</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">--class指定入口类</span><br><span class="line">--master运行模式,有本地模式local[2],有standalone模式是spark://ip:7077,有yarn模式是yarn</span><br><span class="line">--executor-memory每一个executor的内存,默认1GB</span><br><span class="line">--total-executor-cores总共的executor的核数</span><br><span class="line">--executor-cores每一个executor的核数</span><br><span class="line">上面两个核数来控制一共有几个executor执行,注意standalone模式下,没有什么container的概念了,所以linux的资源都能使用,不受yarn的container相关资源的限制</span><br><span class="line"></span><br><span class="line">如果是yarn模式不同,如下设置</span><br><span class="line">--deploy-mode 有cluster和client两种,cluster需要去集群log查看输出日志,client可以在控制台查看</span><br><span class="line">--num-executors 控制executor总数</span><br><span class="line">--executor-cores 控制单个executor核数</span><br></pre></td></tr></table></figure>

<h5 id="配置了hadoop历史服务还需要配置spark历史服务吗"><a href="#配置了hadoop历史服务还需要配置spark历史服务吗" class="headerlink" title="配置了hadoop历史服务还需要配置spark历史服务吗"></a>配置了hadoop历史服务还需要配置spark历史服务吗</h5><p>是两码事,hadoop历史服务没有收集spark任务,因此需要额外配置spark的历史服务</p>
<h5 id="spark有哪些组件"><a href="#spark有哪些组件" class="headerlink" title="spark有哪些组件"></a>spark有哪些组件</h5><ul>
<li><p>Driver客户端,程序解析为job作业,调度任务,跟踪executor执行情况,UI展示</p>
</li>
<li><p>executor是一个进程,运行task的,task之间是隔离的,rdd缓存在executor进程内,</p>
<p>如果是基于yarn模式下,Driver和rm之间通过appmaster进行联系</p>
<p>而executor运行在container中</p>
</li>
<li><p>task: 封装了计算逻辑和数据的任务</p>
</li>
</ul>
<h5 id="client和cluster模式的区别"><a href="#client和cluster模式的区别" class="headerlink" title="client和cluster模式的区别"></a>client和cluster模式的区别</h5><ul>
<li><p>Client 模式将用于监控和调度的 Driver 模块在客户端执行，而不是在 Yarn 中，所以一<br>般用于测试  ,Driver 在任务提交的本地机器上运行  ,Driver 启动后会和 ResourceManager 通讯申请启动 ApplicationMaster  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">INFO Client: Uploading resource file:/tmp/spark-ff43e88d-faca-49a2-ad94-f9d44063f6be/__spark_conf__236058212614201651.zip -&gt; hdfs://hadoop101:8020/user/root/.sparkStaging/application_1718470178998_0002/__spark_conf__.zip</span><br><span class="line"></span><br><span class="line">INFO Client: Submitting application application_1718470178998_0002 to ResourceManager</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>cluster模式,在 YARN Cluster 模式下，任务提交后会和 ResourceManager 通讯申请启动ApplicationMaster  ,随后 ResourceManager 分配 container，在合适的 NodeManager 上启动 ApplicationMaster，此时的 ApplicationMaster 就是 Driver  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">INFO Client: Uploading resource file:/opt/module/spark-3.3.2/examples/jars/spark-examples_2.13-3.3.2.jar -&gt; hdfs://hadoop101:8020/user/root/.sparkStaging/application_1718470178998_0005/spark-examples_2.13-3.3.2.jar</span><br><span class="line">INFO Client: Uploading resource file:/tmp/spark-f615f7f5-2565-4eb2-9706-dc3a640194f8/__spark_conf__8205004691630990196.zip -&gt; hdfs://hadoop101:8020/user/root/.sparkStaging/application_1718470178998_0005/__spark_conf__.zip</span><br><span class="line">这里多了一个jar包的上传</span><br></pre></td></tr></table></figure>
</li>
<li><p>源码中,如果是客户端Client模式,Driver执行在集群之外,此时集群NM中的AM进程名字叫做ExecutorLauncher而不是ApplicationMaster</p>
</li>
</ul>
<h5 id="spark有哪些数据结构"><a href="#spark有哪些数据结构" class="headerlink" title="spark有哪些数据结构"></a>spark有哪些数据结构</h5><ul>
<li><p>RDD : 弹性分布式数据集</p>
</li>
<li><p>累加器: 分布式共享只写变量, 将Driver端变量分布式共享到Executor端,完成各自的计算后,再返回Driver端,最后在Driver端进行merge</p>
<p>使用自带累加器,或者自定义累加器,将元素添加到累加器,调用value方法获取累加后的结果</p>
</li>
</ul>
<p>​	一般累加器放在行动算子中操作,避免少加或者多加</p>
<ul>
<li>广播变量: 分布式共享只读变量, 用来实现map join</li>
</ul>
<h5 id="rdd是什么"><a href="#rdd是什么" class="headerlink" title="rdd是什么"></a>rdd是什么</h5><p>RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是 Spark 中最基本的数据<br>处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行<br>计算的集合  </p>
<p>弹性指的是内存和磁盘自动切换,数据有容错性,计算出错可以重试,可以重新分片</p>
<p>分布式指的是消费的数据分布式存储,例如hdfs</p>
<p>数据集指的是rdd封装的计算逻辑,没有保存数据,有点类似于java的io流操作,一层一层包装</p>
<p>rdd是一个抽象类,所以有很多具体实现子类,例如maprdd等</p>
<p>不可变指的是每一个rdd封装的逻辑不可变,想要改变就需要再外面再封装一层rdd</p>
<p>可分区可并行计算</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1、分区列表 a list of partitions：实现并行计算</span><br><span class="line">	getPartitions</span><br><span class="line">2、每个分区都有一个计算函数 a function for computing each split 每个分区计算逻辑是一样的</span><br><span class="line">	compute(Partition, taskContext)</span><br><span class="line">3、rdd之间存在依赖关系 a list of dependencies on other rdd</span><br><span class="line">	getDependencies</span><br><span class="line">4、分区器，有默认分区器，可以自定义分区器，可以给kv类型的rdd自定义分区器</span><br><span class="line">	partitioner</span><br><span class="line">5、首选位置 将task任务尽量发送到同数据节点的服务器上 移动存储不如移动计算</span><br><span class="line">	getPreferredLocations</span><br></pre></td></tr></table></figure>

<h3 id="SparkCore"><a href="#SparkCore" class="headerlink" title="SparkCore"></a>SparkCore</h3><h5 id="makerdd和parallelize的区别"><a href="#makerdd和parallelize的区别" class="headerlink" title="makerdd和parallelize的区别"></a>makerdd和parallelize的区别</h5><p>makeRDD 方法其实就是 parallelize 方法  </p>
<h5 id="textFile和wholeTextFile的区别"><a href="#textFile和wholeTextFile的区别" class="headerlink" title="textFile和wholeTextFile的区别"></a>textFile和wholeTextFile的区别</h5><p>前者按照行读取,后者按照文件读取,读取结果是元组,第一个元素是文件路径</p>
<h5 id="分区切片规则是什么"><a href="#分区切片规则是什么" class="headerlink" title="分区切片规则是什么"></a>分区切片规则是什么</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">内存加载数据时，指定分区规则</span><br><span class="line">生效顺序 算子numSlices &gt; 环境spark.default.parallelism &gt; totalCores</span><br><span class="line">scheduler.conf.getInt(&quot;spark.default.parallelism&quot;, totalCores)</span><br><span class="line">1、创建rdd时，传入第二个参数，代表分区数量numSlices</span><br><span class="line">2、不传的话，存在默认值</span><br><span class="line">	默认值计算方法：对于本地模式而言：spark.default.parallelism,即conf中set的这个参数</span><br><span class="line">3、取不到就取totalCores，totalCores代表当前运行环境最大CPU核数，6核心12线程则为12</span><br><span class="line">	totalCores如果是本地模式就是对应的local[n]里面的n,如果是*才是最大核数</span><br><span class="line"></span><br><span class="line">数据进入分区规则:</span><br><span class="line">1、positions方法：传入数组数据的长度length(内存数据不包含回车换行)，和分区数量numSlices</span><br><span class="line">3、0 until numSlices遍历,每个元素是i</span><br><span class="line">2、计算开始start和结束end，组成元组</span><br><span class="line">	计算方法start = (i * length) / numSlices 取整</span><br><span class="line">	计算方法end = ((i + 1) * length) / numSlices 取整</span><br><span class="line">	i的范围[0, numSlices-1]闭区间</span><br><span class="line">3、计算每个分区（文件）的数据范围</span><br><span class="line">	计算方法[start, end)包左不包右</span><br><span class="line"></span><br><span class="line">文件加载数据时，指定分区规则,注意是最小分区,但是实际上是多少还需要继续看分区规则,而且Spark读取文件使用的是hadoop读取文件那一套</span><br><span class="line">1、创建rdd时，传入第二个参数，代表最小分区数量，minPartitions</span><br><span class="line">2、不传则存在默认值</span><br><span class="line">	该参数默认值是：min(spark.default.parallelism, 2)</span><br><span class="line">	spark.default.parallelism该参数默认值和内存读取时一致,先conf中set查看,再默认totalCores</span><br><span class="line">那么关注hadoop文件切片规则,来判断具体分区数量</span><br><span class="line">1、所有的文件统计总的字节数totalSize，需要包含回车换行等特殊字符</span><br><span class="line">2、求出 goalSize = totalSize / (numSlices == 0 ? 1 : numSlices)，即为totalSize除以分区数量，整除；这个参数表示每一个分区存放的字节数</span><br><span class="line">3,块大小同hadoop 128MB</span><br><span class="line">4,看文件总大小 / 填写的分区数  和块大小比较  谁小拿谁进行切分</span><br><span class="line">5、切分时候考虑1.1倍数</span><br><span class="line"></span><br><span class="line">数据进入分区规则:还是按照hadoop的数据进入规则,尽量考虑业务数据完整性,不按照字节计算</span><br><span class="line">	原则1：数据按照行为单位读取</span><br><span class="line">	原则2：数据读取时，以偏移量为单位，偏移量从0开始，</span><br><span class="line">	原则3：每个分区数据的范围，是按照偏移量进行计算的，例如第一个分区的数据，应该是[每一行的起始偏移量, 每个分区字节数]，闭区间</span><br><span class="line">	原则4：一行一行读取，所以即使读取了第二行的一个字符，该行也应该全部读取到</span><br><span class="line">	原则5：数据不会被重复读取</span><br><span class="line">1. 就按照分区时计算出来的每个分区放多少数据来进入的,即上面的totalSize,goalSize,分区数量</span><br><span class="line">2. 按照偏移量计算出每个分区的范围,并且是闭区间</span><br><span class="line">3. 考虑原则1和原则4和原则5读取</span><br></pre></td></tr></table></figure>

<h5 id="分区和并行度概念"><a href="#分区和并行度概念" class="headerlink" title="分区和并行度概念"></a>分区和并行度概念</h5><p>并行度可能只有一个executor,但是分区可能是多个,那么就不是并行而是并发执行了</p>
<h5 id="常用算子"><a href="#常用算子" class="headerlink" title="常用算子"></a>常用算子</h5><ul>
<li>转换算子</li>
</ul>
<p>单值类型的,比如map,mapPartition,flatmap,filter,groupby,sortby,distinct,coalesce,repartition等等</p>
<p>双值类型的,intersection交集,union并集,subtract差集</p>
<p>kv类型的,比如partitionBy,groupbykey,reducebykey,sortbykey,join等等</p>
<p>输入算子,例如集合读取parallelize,可以指定分区(切片)的数量, 文件读取textFile,wholeTextFile(以文件为单位读取),sequenceFile,objectFile等</p>
<ul>
<li>行动算子</li>
</ul>
<p>例如reduce,collect,count,take,takeordered,top,aggregate,fold,reduce,foreach</p>
<p>例如输出的saveAsTextFile,saveasobjectfile,saveassequencefile等,</p>
<h5 id="区分几个算子"><a href="#区分几个算子" class="headerlink" title="区分几个算子"></a>区分几个算子</h5><p>sortBy和sortByKey</p>
<p>groupBy和groupByKey</p>
<p>reduceByKey,aggregateByKey,foldByKey,combineByKey</p>
<p>reduce,aggregate,fold</p>
<h5 id="map和mapPartitions等等的区别"><a href="#map和mapPartitions等等的区别" class="headerlink" title="map和mapPartitions等等的区别?"></a>map和mapPartitions等等的区别?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">map类似于串行操作,性能较低</span><br><span class="line">而mappartitions类似于批处理,一次性处理一个分区数据,性能较好,但是内存占用多</span><br><span class="line"></span><br><span class="line">map不会更改元素条数,</span><br><span class="line">但是后者可以更改,输入迭代器输出迭代器</span><br><span class="line"></span><br><span class="line">mappartitionswithindex,额外能获取分区编号信息</span><br><span class="line"></span><br><span class="line">mapvalues操作kv类型,一般在groupby后面使用</span><br></pre></td></tr></table></figure>

<h5 id="sample算子的算法"><a href="#sample算子的算法" class="headerlink" title="sample算子的算法"></a>sample算子的算法</h5><p>数据倾斜的时候,使用该算子去进行抽样,针对key进行抽样来判断是否是倾斜key</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line"> <span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span></span><br><span class="line">),<span class="number">1</span>)</span><br><span class="line"><span class="comment">// 抽取数据不放回（伯努利算法）</span></span><br><span class="line"><span class="comment">// 伯努利算法：又叫 0、1 分布。例如扔硬币，要么正面，要么反面。</span></span><br><span class="line"><span class="comment">// 具体实现：根据种子和随机算法算出一个数和第二个参数设置几率比较，小于第二个参数要，大于不</span></span><br><span class="line">要</span><br><span class="line"><span class="comment">// 第一个参数：抽取的数据是否放回，false：不放回</span></span><br><span class="line"><span class="comment">// 第二个参数：抽取的几率，范围在[0,1]之间,0：全不取；1：全取；</span></span><br><span class="line"><span class="comment">// 第三个参数：随机数种子</span></span><br><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.sample(<span class="literal">false</span>, <span class="number">0.5</span>)</span><br><span class="line"><span class="comment">// 抽取数据放回（泊松算法）</span></span><br><span class="line"><span class="comment">// 第一个参数：抽取的数据是否放回，true：放回；false：不放回</span></span><br><span class="line"><span class="comment">// 第二个参数：重复数据的几率，范围大于等于 0.表示每一个元素被期望抽取到的次数</span></span><br><span class="line"><span class="comment">// 第三个参数：随机数种子</span></span><br><span class="line"><span class="keyword">val</span> dataRDD2 = dataRDD.sample(<span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">行动算子有一个takeSample,也是随机取值</span><br><span class="line"><span class="comment">// 第一个参数：抽取的数据是否放回，true：放回；false：不放回</span></span><br><span class="line"><span class="comment">// 第二个参数：指定要取值的元素的个数</span></span><br><span class="line"><span class="comment">// 第三个参数：随机数种子</span></span><br></pre></td></tr></table></figure>

<h5 id="distinct算子实现原理"><a href="#distinct算子实现原理" class="headerlink" title="distinct算子实现原理"></a>distinct算子实现原理</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">map(x =&gt; (x, <span class="literal">null</span>)).reduceByKey((x, _) =&gt; x, numPartitions).map(_._1)</span><br></pre></td></tr></table></figure>

<h5 id="coalesce和repartition的区别"><a href="#coalesce和repartition的区别" class="headerlink" title="coalesce和repartition的区别"></a>coalesce和repartition的区别</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">coalesce 可以增加或者减少分区数量，默认是不经过shuffle</span><br><span class="line">repartition 可以增加或者减少分区数量，一定经过 shuffle，底层使用 coalesce 并且shuffle 参数为 true</span><br><span class="line">所以对于减少分区，应该使用 coalesce 并且不设置第二个参数， 对于增加分区，可以直接使用 repartition </span><br></pre></td></tr></table></figure>

<h5 id="groupBy和groupByKey的区别"><a href="#groupBy和groupByKey的区别" class="headerlink" title="groupBy和groupByKey的区别?"></a>groupBy和groupByKey的区别?</h5><p>groupby是单值类型算子,按照指定条件进行分组(例如一个判断条件),返回的数据是元组,第一个元素是分区编号,第二个元素是分区内数据的迭代器集合,底层调用的groupbykey,存在shuffle</p>
<p>key是确定的，但是value是不同的，groupByKey会把value单独拿出来组合成为一个集合，而groupBy会把kv键值对组合成为集合</p>
<p>对于groupBy不确定分组的key是什么，而对于groupByKey，确定是使用key进行分组的</p>
<h5 id="sortBy和sortByKey-区别"><a href="#sortBy和sortByKey-区别" class="headerlink" title="sortBy和sortByKey 区别"></a>sortBy和sortByKey 区别</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sortBy算子，底层调用sortByKey算子，会创建一个RangePartitioner分区器，目的是为了将数据分散到不同分区，而再底层调用了sample抽样算子和collect行动算子，所以会创建一个ActiveJob任务，目的是为了尽可能抽样确定分区边界来让数据均匀分散到不同分区</span><br><span class="line">sortByKey的key必须实现Ordered特质而且还需要实现序列化接口</span><br></pre></td></tr></table></figure>

<h5 id="交集并集差集算子的区别"><a href="#交集并集差集算子的区别" class="headerlink" title="交集并集差集算子的区别"></a>交集并集差集算子的区别</h5><p>交集intersection数据类型不一致报错</p>
<p>并集union数据类型不一致不报错</p>
<p>差集subtract数据类型不一致不报错</p>
<h5 id="zip算子的特点"><a href="#zip算子的特点" class="headerlink" title="zip算子的特点"></a>zip算子的特点</h5><p>按照位置组合成键值对</p>
<p>数据类型可以不一致,但是分区数量一致,而且元素个数一致</p>
<h5 id="partitionBy和coalesce重分区有啥区别"><a href="#partitionBy和coalesce重分区有啥区别" class="headerlink" title="partitionBy和coalesce重分区有啥区别"></a>partitionBy和coalesce重分区有啥区别</h5><p>都能够进行分区的重分区操作</p>
<p>但是partitionBy是按照key进行重分区的,默认可以使用hash重分区,也能自定义分区器,操作的kv类型rdd</p>
<p>重复调用使用相同分区器,而且分区数量相同,不会产生新的rdd,即不会重复调用的,源码可以看出来</p>
<p>coalesce操作单值类型或者kv类型都可以</p>
<h5 id="有哪些分区器"><a href="#有哪些分区器" class="headerlink" title="有哪些分区器"></a>有哪些分区器</h5><p>相关rdd算子是partitionBy算子</p>
<p>hash分区器</p>
<p>range分区器,用在排序相关算子中</p>
<p>自定义分区器,</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">HashPartitioner 是默认分区器，例如reduceByKey等算子都存在一个默认分区器，默认的分区规则是：哈希取模规则</span><br><span class="line">RangePartitioner</span><br><span class="line">自定义分区器 继承 Partitoner 抽象类，重写抽象方法，同时在 reduceByKey 等算子中，作为第一个参数传递。如果重复调用还想保证只 shuffle 一次，那么还需要重写 equals 方法</span><br><span class="line"></span><br><span class="line">如果使用了两次相关的算子，并且传入的分区器类型和分区数量都一样，则不会做任何操作，即不会产生新的rdd</span><br></pre></td></tr></table></figure>

<p>自定义分区器代码演示</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomerPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">        key <span class="keyword">match</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;nba&quot;</span> =&gt; <span class="number">0</span></span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;cba&quot;</span> =&gt; <span class="number">1</span></span><br><span class="line">            <span class="keyword">case</span> _ =&gt; <span class="number">2</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="分组和分区的联系"><a href="#分组和分区的联系" class="headerlink" title="分组和分区的联系?"></a>分组和分区的联系?</h5><p>分组和分区没有必然的联系</p>
<p>groupBy算子，分区数量没变化，但是实际上经过了shuffle，极限情况下数据会分到同一个分区(就是同一组数据分到一个分区了)</p>
<p>但是不代表一个分区只能处理一个组的数据</p>
<h5 id="groupByKey-和-reduceByKey-的区别"><a href="#groupByKey-和-reduceByKey-的区别" class="headerlink" title="groupByKey 和 reduceByKey 的区别"></a>groupByKey 和 reduceByKey 的区别</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">两个都经过shuffle,但是reduceByKey 会进行预先聚合，这样可以让 shuffle write 的数据条数变少，性能好,而group不进行预先聚合,因为只是单纯分组,没有聚合计算的逻辑</span><br><span class="line">若是为了分组聚合,在不影响业务逻辑的前提下尽量多使用 reduceByKey</span><br><span class="line">reduce是分组聚合,而group只是分组</span><br></pre></td></tr></table></figure>

<h5 id="4个ByKey"><a href="#4个ByKey" class="headerlink" title="4个ByKey"></a>4个ByKey</h5><p>reduceByKey：没有初始值，分区内，分区间规则一致</p>
<p>aggregateByKey：有初始值，分区内，分区间规则可以不同,函数的柯里化</p>
<p>foldByKey：有初始值，分区内，分区间规则一致</p>
<p>combineByKey：可以将相同key的第一个值转换格式，分区内，分区间规则可以不同</p>
<p>底层都是combineByKeyWithClassTag：</p>
<p>​	createCombiner：相同的key的第一条数据的处理函数</p>
<p>​	mergeValue：分区内数据的处理函数</p>
<p>​	mergeCombiner：分区间的数据的处理函数</p>
<h5 id="cogroup和leftOuterJoin的区别"><a href="#cogroup和leftOuterJoin的区别" class="headerlink" title="cogroup和leftOuterJoin的区别"></a>cogroup和leftOuterJoin的区别</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">都需要操作kv类型rdd数据</span><br><span class="line"></span><br><span class="line">leftouterjoin类似于sql的左连接,会把value值存入元组,没关联上的值设置为None</span><br><span class="line">cogroup类似于sql的全外连接,会把两个表的value存入二元组,每个value都是一个迭代器,根据源码, cogroup有可能存在shuffle</span><br><span class="line"></span><br><span class="line">left只能是两个rdd关联</span><br><span class="line">而cogroup一个rdd最多可以连接三个rdd</span><br></pre></td></tr></table></figure>

<h5 id="top和takeOrdered的区别"><a href="#top和takeOrdered的区别" class="headerlink" title="top和takeOrdered的区别"></a>top和takeOrdered的区别</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">top降序</span><br><span class="line">takeOrdered升序</span><br></pre></td></tr></table></figure>

<h5 id="countByKey和countByValue的区别"><a href="#countByKey和countByValue的区别" class="headerlink" title="countByKey和countByValue的区别"></a>countByKey和countByValue的区别</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">countByKey统计每一个key的数量,返回map,操作kv类型rdd数据</span><br><span class="line">countByValue统计每一个value值的数量,返回map,一般操作单值类型的rdd数据,如果操作kv类型,那么是按照一个完整的kv键值对去判断重复出现的次数的,而不是单纯按照value进行判断的</span><br></pre></td></tr></table></figure>

<h5 id="何区分转换算子和行动算子"><a href="#何区分转换算子和行动算子" class="headerlink" title="何区分转换算子和行动算子"></a>何区分转换算子和行动算子</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">按照是否产生Job是不准确的，因为对于sortByKey这样的转换算子，即使不调用行动算子，在底层也会产生Job</span><br><span class="line">应该根据算子的返回类型是否是 RDD 来区分是否是转换算子</span><br></pre></td></tr></table></figure>

<h5 id="依赖和血缘"><a href="#依赖和血缘" class="headerlink" title="依赖和血缘"></a>依赖和血缘</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">依赖和血缘是RDD的其中一个特性</span><br><span class="line">依赖指的是某个RDD依赖到的上游的RDD,即两个rdd之间的关系</span><br><span class="line">而血缘指的是某个RDD的完整的依赖链条,注意这里的完整链条指的是什么,具体看代码演示</span><br><span class="line">使用.toDebugString可以查看某个RDD的血缘关系</span><br><span class="line">使用.rdd.dependencies 可以查看某个RDD 的依赖列表</span><br><span class="line">依赖分为宽依赖和窄依赖，窄依赖即 OneToOneDependency类，宽依赖即 ShuffleDependency类</span><br></pre></td></tr></table></figure>

<h5 id="宽窄依赖"><a href="#宽窄依赖" class="headerlink" title="宽窄依赖"></a>宽窄依赖</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">区分宽窄依赖的标志是看父RDD的一个分区的数据被下游几个RDD使用，如果对应多个下游RDD，那么就是宽依赖，如果对应下游一个RDD，那么就是窄依赖,所以一对多就是宽依赖,一对一或者多对一就是窄依赖,这就是为什么缩小分区可以不经过shuffle</span><br><span class="line">宽依赖的底层一定使用了shuffle</span><br></pre></td></tr></table></figure>

<h5 id="哪些算子会产生shufflerdd"><a href="#哪些算子会产生shufflerdd" class="headerlink" title="哪些算子会产生shufflerdd"></a>哪些算子会产生shufflerdd</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">ShuffledRDD类继承RDD,实现了compute方法,会在shuffle read端读取数据,在下面的函数中创建了该类的对象</span><br><span class="line"></span><br><span class="line">PairRDDFunctions类</span><br><span class="line">combineByKeyWithClassTag方法</span><br><span class="line">partitionBy方法</span><br><span class="line"></span><br><span class="line">OrderedRDDFunctions类</span><br><span class="line">sortByKey方法 </span><br><span class="line">repartitionAndSortWithinPartitions方法 </span><br><span class="line"></span><br><span class="line">RDD类</span><br><span class="line">repartition方法</span><br></pre></td></tr></table></figure>

<h5 id="spark的shuffle-shuffle是什么"><a href="#spark的shuffle-shuffle是什么" class="headerlink" title="spark的shuffle&#x2F;shuffle是什么"></a>spark的shuffle&#x2F;shuffle是什么</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">从groupBy引入</span><br><span class="line">默认情况下数据处理后分区不改变,但是Spark要求同一个组内的数据必须在同一个分区,所以groupBy会把数据打乱分区重新组合,这个操作就叫做shuffle</span><br><span class="line">(注意所有分区进入同一个分区这个不叫做shuffle,因为分区就没有被打乱)</span><br><span class="line"></span><br><span class="line">像groupBy这样的算子,具有改变分区数量的能力(因为我们分组聚合的时候,分组的个数可能变少,此时如果分区数量还保持不变可能出现分区无数据导致资源浪费)</span><br><span class="line"></span><br><span class="line">为了保证DAG,Spark要求所有数据必须分组完成后才能继续执行后续的操作,但是由于RDD不保存数据,所以数据必须落盘</span><br><span class="line"></span><br><span class="line">shuffle会把完整计算流程一分为二,一部分shuffle write一部分shuffle read,而且写磁盘操作完成后才能读取磁盘</span><br></pre></td></tr></table></figure>

<h5 id="cache，persist，checkpoint的区别"><a href="#cache，persist，checkpoint的区别" class="headerlink" title="cache，persist，checkpoint的区别"></a>cache，persist，checkpoint的区别</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1 cache底层是persist设置缓存级别为MEMORY_ONLY来实现</span><br><span class="line">2 persist有多种缓存级别。</span><br><span class="line">cache和persist,会在血缘关系中增加依赖关系</span><br><span class="line">对于一些经过 shuffle 的算子，例如 reduceByKey，会自动进行缓存，但是仍然推荐使用缓存算子和检查点算子的形式进行持久化</span><br><span class="line">触发了行动算子执行时才会真正的进行缓存，而并不是按照代码顺序执行</span><br><span class="line">明确缓存使用结束后应该调用.unpersist 来释放缓存</span><br><span class="line">缓存文件在作业执行完成后会被删除,</span><br><span class="line">3 checkpoint，指的是将rdd中间结果写入磁盘,会切断（改变）血缘关系。需要使用 SparkContext 来设置 CheckpointDir 目录，由于会多执行一次完整的流程，所以建议在检查点之前设置缓存cache或persist,这样就可以从cache/persist开始继续执行而不需要从头执行,doCheckpoint方法,会发现会触发job执行,而先执行cache,就可以直接从内存读取,不需要从头读取了,</span><br><span class="line">持久化落盘,需要设置存储路径,例如hdfs</span><br><span class="line"></span><br><span class="line">cache和persist会在血缘关系中添加新的依赖关系,出现问题的时候可以通过血缘关系查询数据源头</span><br><span class="line">而checkpoint会切断之前的血缘关系,重新建立新的血缘关系,其实就相当于数据源发生改变了</span><br></pre></td></tr></table></figure>

<h5 id="广播变量怎么使用"><a href="#广播变量怎么使用" class="headerlink" title="广播变量怎么使用"></a>广播变量怎么使用</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sc.broadcast()定义广播变量</span><br><span class="line">broadcast_var.value调用广播变量值</span><br></pre></td></tr></table></figure>

<h5 id="累加器怎么使用"><a href="#累加器怎么使用" class="headerlink" title="累加器怎么使用"></a>累加器怎么使用</h5><p>累加器有系统自带的三种累加器,也可以自定义累加器</p>
<p>系统自带的累加器,add方法添加元素,value方法获取元素</p>
<p>自定义累加器需要实现AccumulatorV2抽象类并实现相关抽象方法</p>
<h3 id="SparkSql"><a href="#SparkSql" class="headerlink" title="SparkSql"></a>SparkSql</h3><h5 id="sparksql-hive-hive-on-spark-spark-on-hive的区别"><a href="#sparksql-hive-hive-on-spark-spark-on-hive的区别" class="headerlink" title="sparksql,hive,hive on spark,spark on hive的区别"></a>sparksql,hive,hive on spark,spark on hive的区别</h5><ul>
<li>spark sql和hive on spark</li>
</ul>
<p>SparkSQL前身是shark,和hive类似,也是希望将sql转换成底层的代码,rdd,去执行,hive是为了转换成mr,所以也有metastore,解析器,翻译器,优化器,执行器等等,相当于是代替hive的mr引擎,</p>
<p>问题是Hive发展缓慢,导致spark受制于Hive发展速度,所以产生了Hive-On-spark和spark SQL两个分类</p>
<p>Hive-On-spark指的是:Hive的一个发展计划,将spark作为Hive底层执行的引擎之一,所以Hive不止一个引擎,有mr,spark,tez等,使用的是hive的解析器优化器metastore那一套,</p>
<p>sparkSQL是spark生态的成员,不受限于Hive发展,而是兼容Hive,所以sparkSQL是为了简化RDD开发,提供两个编程模型抽象,一个是DataFrame,一个是DataSet</p>
<p>DataFrame执行效率高于RDD,因为执行计划进行优化了</p>
<p>DataSet是DataFrame的扩展,对于DataFrame,查询结果要想获取指定字段,只能通过索引获取,不方便,而DataSet可以通过字段名获取,很方便,这是其中一个优点</p>
<p>spark sql: 使用的上下文环境对象是SparkSession,实际上就是老版本的SQLContext和HiveContext的组合,底层计算实际上还是使用的SparkContext</p>
<p>hive on spark: 使用hive context</p>
<p>都是解析,优化,执行功能</p>
<ul>
<li>spark on hive</li>
</ul>
<p>指的是在spark sql这层api,或者说具体的dataset&#x2F;dataframe 编程模型写代码的时候,操作已经现有的hive当中的表,进行读取写入等操作</p>
<h5 id="RDD-DataFrame和DataSet三者区别"><a href="#RDD-DataFrame和DataSet三者区别" class="headerlink" title="RDD,DataFrame和DataSet三者区别"></a>RDD,DataFrame和DataSet三者区别</h5><p>rdd只关心数据,不关心数据含义,而DataFrame还关心数据的结构信息,类似于二维表结构,带有schema元数据信息,关心字段名称,字段类型</p>
<p>根据catalyst优化器进行了优化,所以比自己编写rdd 一般情况下效率高</p>
<p>而DataSet是扩展,可以通过字段名称而不是索引去操作数据,对结果的处理更方便</p>
<p>dataFrame实际上就是dataset,类型是row类型</p>
<p>这三者都可以相互之间转换</p>
<h5 id="如何创建使用DataFrame-DataSet"><a href="#如何创建使用DataFrame-DataSet" class="headerlink" title="如何创建使用DataFrame&#x2F;DataSet"></a>如何创建使用DataFrame&#x2F;DataSet</h5><p>从已经存在的rdd创建转换</p>
<p>从hive表查询返回</p>
<p>直接读取数据源返回,支持多种丰富的数据源格式</p>
<p>还有样例类或者普通序列方式可以创建,很少使用,Seq</p>
<h5 id="SparkSql的默认数据类型"><a href="#SparkSql的默认数据类型" class="headerlink" title="SparkSql的默认数据类型"></a>SparkSql的默认数据类型</h5><p>内存读取数值,默认int,</p>
<p>数据源文件读取数值,默认bigint,可以和long转换</p>
<h5 id="DSL编码风格是什么"><a href="#DSL编码风格是什么" class="headerlink" title="DSL编码风格是什么"></a>DSL编码风格是什么</h5><p>DataFrame提供DSL去管理结构化表数据,不必创建临时视图</p>
<p>可以在scala,Java,r,python中使用</p>
<p>这样就不需要先创建表再去使用SQL查询,可以直接使用df调用查询算子,例如select</p>
<p>但是需要注意的是,如果参与运算,必须将字段添加$符号,例如$”age”,注意双引号位置;或者使用单引号表示字段而不是字符串,例如’age注意只有一个单引号</p>
<p>注意需要导包import spark.implicits._ 注意spark指的是SparkSession对象名称,而不是spark包名</p>
<p>DSL中还能使用sql不能使用的强类型UDAF等函数</p>
<h5 id="rdd-df-ds三者转换"><a href="#rdd-df-ds三者转换" class="headerlink" title="rdd,df,ds三者转换"></a>rdd,df,ds三者转换</h5><p>rdd转换成df,一般结合样例类进行转换</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//RDD转换成DF,注意先导包</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> sc = spark.sparkContext</span><br><span class="line"><span class="keyword">val</span> rdd = sc.textFile(<span class="string">&quot;input/word.txt&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> df = rdd.toDF()</span><br><span class="line">df.show()</span><br><span class="line">df.printSchema()<span class="comment">//默认是value名称string类型</span></span><br></pre></td></tr></table></figure>

<p>df转换成rdd</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//DF转换成RDD</span></span><br><span class="line"><span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;input/user.json&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> rdd = df.rdd</span><br><span class="line">rdd.collect().foreach(println)<span class="comment">//Row类型</span></span><br></pre></td></tr></table></figure>

<p>rdd转换成ds</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//RDD转换成DS,注意先导包,一般结合样例类使用</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> sc = spark.sparkContext</span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;zhangsan&quot;</span>, <span class="number">30</span>), (<span class="string">&quot;lisi&quot;</span>, <span class="number">40</span>)))</span><br><span class="line"><span class="keyword">val</span> ds1 = rdd1.map(x =&gt; <span class="type">User1</span>(x._1, x._2)).toDS()</span><br><span class="line">ds1.show()</span><br><span class="line">ds1.printSchema()</span><br></pre></td></tr></table></figure>

<p>ds转换成rdd</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//DS转换成RDD</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> sc = spark.sparkContext</span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;zhangsan&quot;</span>, <span class="number">30</span>), (<span class="string">&quot;lisi&quot;</span>, <span class="number">40</span>)))</span><br><span class="line"><span class="keyword">val</span> ds1 = rdd1.map(x =&gt; <span class="type">User2</span>(x._1, x._2)).toDS()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd = ds1.rdd</span><br><span class="line">rdd.collect().foreach(println)</span><br></pre></td></tr></table></figure>

<p>df转换成ds</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//如果是通过rdd转换df再转换ds,那么toDF(&quot;username&quot;,&quot;age&quot;)必须指定属性名称而且和样例类的属性保持一致</span></span><br><span class="line"><span class="comment">//而且还需要注意数据类型是否一致的问题</span></span><br><span class="line"><span class="comment">//DF转换成DS</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> sc = spark.sparkContext</span><br><span class="line"><span class="keyword">val</span> df = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;zhangsan&quot;</span>, <span class="number">30</span>), (<span class="string">&quot;lisi&quot;</span>, <span class="number">49</span>))).toDF(<span class="string">&quot;username&quot;</span>, <span class="string">&quot;age&quot;</span>)</span><br><span class="line">df.show()</span><br><span class="line">df.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> ds = df.as[<span class="type">User4</span>]</span><br><span class="line">ds.show()</span><br><span class="line">ds.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="comment">//直接转换</span></span><br><span class="line"><span class="keyword">val</span> df1 = spark.read.json(<span class="string">&quot;input/user.json&quot;</span>)</span><br><span class="line">df1.show()</span><br><span class="line">df1.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> ds1 = df1.as[<span class="type">User4</span>]</span><br><span class="line">ds1.show()</span><br><span class="line">ds1.printSchema()</span><br></pre></td></tr></table></figure>

<p>ds转换成df</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//DS转换成DF</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> sc = spark.sparkContext</span><br><span class="line"><span class="keyword">val</span> df = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;zhangsan&quot;</span>, <span class="number">30</span>), (<span class="string">&quot;lisi&quot;</span>, <span class="number">49</span>))).toDF(<span class="string">&quot;username&quot;</span>, <span class="string">&quot;age&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> ds = df.as[<span class="type">User5</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> newDF = ds.toDF()</span><br><span class="line">newDF.show()</span><br><span class="line">newDF.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="comment">//直接转换</span></span><br><span class="line"><span class="keyword">val</span> df1 = spark.read.json(<span class="string">&quot;input/user.json&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> ds1 = df1.as[<span class="type">User5</span>]</span><br><span class="line"><span class="keyword">val</span> newDF1 = ds1.toDF()</span><br><span class="line">newDF1.show()</span><br><span class="line">newDF1.printSchema()</span><br></pre></td></tr></table></figure>

<h5 id="view生效范围"><a href="#view生效范围" class="headerlink" title="view生效范围"></a>view生效范围</h5><p>只在当前session生效</p>
<p>可以建设全局生效的view,但是访问时需要添加global_temp.view访问</p>
<p>使用spark.newSession可以创建新的session验证效果</p>
<h5 id="spark-sql的默认文件格式是什么"><a href="#spark-sql的默认文件格式是什么" class="headerlink" title="spark sql的默认文件格式是什么"></a>spark sql的默认文件格式是什么</h5><p>spark默认保存和读取的文件格式是parquet列式存储</p>
<h5 id="spark-sql的默认加载和保存方式是什么"><a href="#spark-sql的默认加载和保存方式是什么" class="headerlink" title="spark sql的默认加载和保存方式是什么"></a>spark sql的默认加载和保存方式是什么</h5><p>使用spark.read.load是通用的读取方式,但是一定得是parquet格式才可以</p>
<p>安装目录有样例文件</p>
<p>使用spark.write.save是通用的保存方式,也同样是默认parquet格式</p>
<p>有csv,json,jdbc,parquet,orc,text,avro等等多种格式</p>
<p>甚至可以在sql里面直接查询文件</p>
<p>可以设置压缩格式直接读取压缩文件</p>
<h5 id="sparksql存储文件savemode加锁了吗-有哪些savemode"><a href="#sparksql存储文件savemode加锁了吗-有哪些savemode" class="headerlink" title="sparksql存储文件savemode加锁了吗,有哪些savemode"></a>sparksql存储文件savemode加锁了吗,有哪些savemode</h5><p>error报错,append追加,overwrite覆盖,ignore忽略</p>
<p>没有加锁,不是原子操作,没有事务性保证</p>
<h5 id="sparksql如何连接Hive"><a href="#sparksql如何连接Hive" class="headerlink" title="sparksql如何连接Hive"></a>sparksql如何连接Hive</h5><p>spark内置了Hive,也可以连接外部Hive</p>
<p>内置Hive会产生metastore_db的目录,只要操作像show tables这样的sql语句就会创建,创建表还会产生spark-warehouse目录,还有一个derby.log文件证明元数据内置derby数据库中,内置Hive用来练习使用</p>
<p>外置Hive的使用:</p>
<p>hive-site.xml的拷贝,以及数据库连接jar包,还有一些相关配置,还有core-site.xml和hdfs-site.xml等拷贝</p>
<h5 id="thriftserver是什么"><a href="#thriftserver是什么" class="headerlink" title="thriftserver是什么"></a>thriftserver是什么</h5><p>Spark Thrift Server 是 Spark 社区基于 HiveServer2 实现的一个 Thrift 服务。旨在无缝兼容HiveServer2。因为 Spark Thrift Server 的接口和协议都和 HiveServer2 完全一致，因此我们部署好 Spark Thrift Server 后，可以直接使用 hive 的 beeline 访问 Spark Thrift Server 执行相关语句。 Spark Thrift Server 的目的也只是取代 HiveServer2，因此它依旧可以和 Hive Metastore进行交互，获取到 hive 的元数据  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/opt/module/spark-yarn/sbin/start-thriftserver.sh --hiveconf hive.server2.thrift.port=10001 --hiveconf hive.server2.thrift.bind.host=hadoop102 --conf spark.sql.warehouse.dir=hdfs://hadoop102:8020/user/hive/warehouse --master yarn</span><br></pre></td></tr></table></figure>

<h3 id="优化部分"><a href="#优化部分" class="headerlink" title="优化部分"></a>优化部分</h3><h5 id="工作中的一点执行时间参考"><a href="#工作中的一点执行时间参考" class="headerlink" title="工作中的一点执行时间参考"></a>工作中的一点执行时间参考</h5><p>文件大小2.3GB,日增,参数设置–master yarn –deploy-mode client –driver-memory 1g –num-executors 3 –executor-cores 2 –executor-memory 4g,三个executor,每一个核数2核4gb,最大并发度2*3&#x3D;6,文件切分按照128,一共切分了19个task,执行时间4min,数据条数2.2千万,建设分桶表和不建设的时间基本一致</p>
<h5 id="sparksql转换rdd的流程"><a href="#sparksql转换rdd的流程" class="headerlink" title="sparksql转换rdd的流程"></a>sparksql转换rdd的流程</h5><p>parser进行语法校验,通过后生成逻辑计划,然后通过catalog存储库来校验表名,列名,类型等信息,通过后使用catalyst优化器进行优化,生成优化后的逻辑执行计划,然后转换成物理执行计划,然后基于cbo选择代价较小的物理计划,最终生成rdd代码层面的job任务开始执行</p>
<h5 id="spark的执行计划"><a href="#spark的执行计划" class="headerlink" title="spark的执行计划"></a>spark的执行计划</h5><p>默认只展示物理执行计划,使用explain算子查询,或者sql查询</p>
<p>一般就查询extended物理计划和逻辑计划</p>
<p>从下往上查看计划,可以发现,会提前预聚合,并且尽量走广播join,列裁剪和谓词下推也会尽量执行</p>
<p>也可以在web界面查看执行计划,但是注意idea本地没有历史服务的话,可以通过死循环查看</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Parsed Logical Plan,即未决断的逻辑计划</span><br><span class="line">Analyzed Logical Plan,即决断的逻辑执行计划</span><br><span class="line">	会展示列名,列裁剪</span><br><span class="line">	会展示列的序号</span><br><span class="line">	会进行类型转换</span><br><span class="line">	列的信息和表格式</span><br><span class="line">Optimized Logical Plan,即优化逻辑执行计划</span><br><span class="line">	例如将关联条件过滤空,谓词下推</span><br><span class="line">Physical Plan,即物理执行计划</span><br><span class="line">	前面的小括号数字代表执行的先后顺序</span><br><span class="line">	缩进量相同代表同时执行</span><br><span class="line">	Project可以理解为列裁剪</span><br><span class="line">	Exchange表示shuffle过程</span><br><span class="line">	Aggregate聚合会存在预聚合</span><br><span class="line">HashAggregate 运算符表示数据聚合，一般 HashAggregate 是成对出现，第一个</span><br><span class="line">HashAggregate 是将执行节点本地的数据进行局部聚合，另一个 HashAggregate 是</span><br><span class="line">将各个分区的数据进一步进行聚合计算</span><br><span class="line">Exchange 运算符其实就是 shuffle，表示需要在集群上移动数据。很多时候</span><br><span class="line">HashAggregate 会以 Exchange 分隔开来</span><br><span class="line">Project 运算符是 SQL 中的投影操作，就是选择列</span><br><span class="line">BroadcastHashJoin 运算符表示通过基于广播方式进行 HashJoin</span><br><span class="line">LocalTableScan 运算符就是全表扫描本地的表</span><br></pre></td></tr></table></figure>

<h5 id="参数调优思路"><a href="#参数调优思路" class="headerlink" title="参数调优思路"></a>参数调优思路</h5><p>参考集群可用资源,在spark-submit提交任务时指定参数</p>
<p>executor个数越多,单个cpu核数越多,能够提高task最大并发,并发&#x3D;executor个数*每一个cpu核数</p>
<p>executor内存越大,能cache数据量越大,减少io,shuffle缓存数据量越大,减少io,避免频繁gc</p>
<p>executor-cores,每个executor的最大核数,一般3-6,这里设置4</p>
<p>并行度设置为并发度也就是cores数量的2-3倍,这里的cores&#x3D;num-executors*executor-cores</p>
<p>spark.default.parallelism,指的是rdd并行度</p>
<p>spark.sql.shuffle.partitions,默认200,sparksql并行度</p>
<p>根据经验,Driver调整较少</p>
<p>结果还要注意yarn配置的容器内存,默认1-8GB,一般设置1-4gb,看数据量大小,具体执行分析</p>
<h5 id="内存模型和估算"><a href="#内存模型和估算" class="headerlink" title="内存模型和估算"></a>内存模型和估算</h5><p>1 Execution内存:shuffle等0.3</p>
<ul>
<li>估算,execution&#x3D;每个executor核数*(数据集大小&#x2F;并行度)</li>
<li>计算单个并行度有多大数据量,假设默认200并行度,数据集100GB,每个并行度task的数据量是512MB</li>
<li>一个executor假设4core,那么需要同时跑4个Task,所以内存需要2GB至少</li>
</ul>
<p>2 Storage内存:缓存数据等0.3,一个executor内部,存储的广播变量,数据缓存cache</p>
<ul>
<li>估算,storage&#x3D;广播变量+cache&#x2F;executor数量,因为cache是分散到所有executor存储的</li>
</ul>
<p>3 other其他内存:0.4代码内存,内部元数据等</p>
<ul>
<li>估算,other&#x3D;自定义的数据结构内存*每个executor的核数</li>
<li>一般不太大不需要改</li>
</ul>
<p>4 预留300MB</p>
<p>比例一般不需要调整,因为可以动态调整</p>
<p>spark.memory.fraction调整比例</p>
<p>spark.memory.storageFraction调整比例</p>
<h5 id="cache缓存估算"><a href="#cache缓存估算" class="headerlink" title="cache缓存估算"></a>cache缓存估算</h5><p>2.3GB文件加载到表,并cache缓存</p>
<p>提交参数是,Driver1GB,executors&#x3D;3,executor-cores&#x3D;2,executor-memory&#x3D;6GB,使用client模式</p>
<p>可以查看估计需要7-8GB左右内存去缓存</p>
<p>如果使用kryo序列化,估计需要1GB左右</p>
<p>对于dataset进行缓存cache调用,实际上不是java序列化也不是kryo序列化,默认MEMORY_AND_DISK  ,大概600mb</p>
<p>所以尽量使用dataset的api进行编程,效率更好</p>
<h5 id="并行度和并发度的区别"><a href="#并行度和并发度的区别" class="headerlink" title="并行度和并发度的区别"></a>并行度和并发度的区别</h5><p>spark.default.parallelism,指的是rdd并行度</p>
<p>spark.sql.shuffle.partitions,默认200,sparksql并行度</p>
<p>并行度指的是总共的Task的数量,</p>
<p>并发度:指的是同时可以执行的Task的数量,这个取决于executor的cores数量,因为一个Task使用一个core,所以并发度就是executor的cores数量,executor-num*executor-core</p>
<p>并行度设置较低,所以Task数量较低,所以数据在Task数据量比较大,CPU容易线程挂起,即单个Task需要的内存过大,导致只有一部分Task能并行运行,剩余的核数全部挂起</p>
<p>并行度设置较高,数据过于分散,一个Task处理很少的数据量,调度开销反而更多</p>
<p>原则:并行度设置为并发度也就是cores数量的2-3倍,这里的cores&#x3D;num-executors*executor-cores</p>
<h5 id="RBO"><a href="#RBO" class="headerlink" title="RBO"></a>RBO</h5><p>在spark sql的逻辑执行计划的优化环节,就是基于rbo进行的优化</p>
<p>即sql层面优化,逻辑计划进行优化,基于RBO优化,有80多种优化规则,例如谓词下推,列裁剪,常量替换等</p>
<p>spark使用catalyst优化器,有81种优化规则,分为27组,有些规则会分配到多个组,所以不考虑重复性,总体有129个优化规则</p>
<p>谓词下推:指的是将过滤条件的谓词尽可能提前执行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">select *</span><br><span class="line">from A join B on A.id=B.id</span><br><span class="line">and A.id&lt;10;   指的是关联过程中的关联条件,会谓词下推优化</span><br><span class="line"></span><br><span class="line">select *</span><br><span class="line">from A join B on A.id=B.id</span><br><span class="line">where A.id&lt;10;  指的是关联以后的临时表where,会谓词下推优化</span><br><span class="line"></span><br><span class="line">即对于内连接,不管这个条件是on还是where,左表限定范围了,那么右表自然也会限定范围,所以都会提前谓词下推过滤,注意都是id</span><br><span class="line"></span><br><span class="line">select *</span><br><span class="line">from A left join B on A.id=B.id</span><br><span class="line">and A.id&lt;10;   可以发现,谓词下推把右表进行了范围过滤,因为左表需要保留所有数据,</span><br><span class="line"></span><br><span class="line">select *</span><br><span class="line">from A left join B on A.id=B.id</span><br><span class="line">where A.id&lt;10;   可以发现,相当于内连接</span><br><span class="line"></span><br><span class="line">对于外连接,条件写在on中,无论这个条件是左表的还是右表的条件,首先都会进行谓词下推,然后都是对右表的条件进行过滤,从结果来看,都是保留全部左表的数据,即所谓正确结果</span><br><span class="line">如果条件写在where中就要关注本身语义了,相当于内连接了,此时两张表都会进行事先的谓词下推过滤,从结果来看不是所谓保留全部左表数据的所谓的正确结果</span><br><span class="line"></span><br><span class="line">select *</span><br><span class="line">from A left join B on A.id=B.id</span><br><span class="line">and B.id&lt;10;   左连接,但是加入右表的范围过滤,先进行右表的谓词下推过滤,因为左表需要保留所有数据,</span><br><span class="line"></span><br><span class="line">select *</span><br><span class="line">from A left join B on A.id=B.id</span><br><span class="line">where B.id&lt;10;   左连接,但是where写了右表的过滤条件,相当于内连接,发现谓词下推两张表都先进行过滤,然后内连接</span><br></pre></td></tr></table></figure>

<h5 id="CBO"><a href="#CBO" class="headerlink" title="CBO"></a>CBO</h5><p>首先analyze表信息和关联相关字段信息,然后开启cbo两个参数,这样有机会可以触发广播join</p>
<p>即代价选择较小的优化手段,先计算所有物理计划的代价,然后选择代价最小的,最终生成物理执行计划</p>
<p>考虑的是执行节点数据本身的代价,以及操作算子的代价,例如join算子的优化</p>
<p>主要可以用来优化join顺序</p>
<p>默认关闭,参数是spark.sql.cbo.enabled</p>
<p>要先进行statistics收集,即先统计表和列的信息,统计行数和字节数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ANALYZE TABLE 表名 COMPUTE STATISTICS,收集行数,表大小(字节数)</span><br><span class="line"></span><br><span class="line">可以从hive元数据库查看:TBLS表,查询TBL_ID,再查看TABLE_PARAMS,根据TBL_ID查看</span><br></pre></td></tr></table></figure>

<p>再进行列信息统计</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ANALYZE TABLE 表名 COMPUTE STATISTICS FOR COLUMNS 列 1,列 2,列 3</span><br></pre></td></tr></table></figure>

<p>或者直接使用</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">desc formatted 表名,在statistics查看</span><br><span class="line">desc formatted 表名 列名  注意一次只能查看一列</span><br></pre></td></tr></table></figure>

<p>重要参数:</p>
<p>spark.sql.cbo.enabled默认false</p>
<p>spark.sql.cbo.joinReorder.enabled默认false,指的是使用CBO来自动调整连续内连接的表关联的顺序,</p>
<p>spark.sql.cbo.joinReorder.dp.threshold默认12,代表CBO自动调整内连接表的关联顺序的表的个数,超过该阈值就不会进行自动调整</p>
<p>使用方式</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SparkConf</span><br><span class="line">...</span><br><span class="line">.set(&quot;spark.sql.cbo.enabled&quot;, &quot;true&quot;)</span><br><span class="line">.set(&quot;spark.sql.cbo.joinReorder.enabled&quot;, &quot;true&quot;)</span><br></pre></td></tr></table></figure>

<p>使用演示</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">	A.cid,</span><br><span class="line">	sum(B.col)</span><br><span class="line">from A,B</span><br><span class="line">where A.id=B.id</span><br><span class="line">and B.col=&#x27;xxx&#x27;</span><br><span class="line">group by A.cid</span><br><span class="line"></span><br><span class="line">未开启CBO的join优化:不会走广播join</span><br><span class="line">开启CBO,会提前过滤,然后满足广播join条件</span><br><span class="line"></span><br><span class="line">需要先进行表分析analyze</span><br><span class="line"></span><br><span class="line">不开启CBO,判断是否广播join,是根据原始表大小判断,不会进行过滤等操作,但是实际上sql中存在提前过滤,开启以后,进行提前过滤,就满足广播join了</span><br></pre></td></tr></table></figure>

<h5 id="广播join"><a href="#广播join" class="headerlink" title="广播join"></a>广播join</h5><p>广播join类比于hive的mapjoin;如果一张表小于条件要求,会先将小表聚合到Driver端,然后广播到各个大表分区中,本地join避免shuffle</p>
<p>spark默认是sortmergejoin</p>
<p>参数:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">spark.sql.autoBroadcastJoinThreshold参数,默认10MB</span><br><span class="line">默认广播join是开启的,设置-1代表禁用广播join,禁用广播就会走sortmergejoin,是要经过shuffle过程,而且要排序,性能很低的</span><br><span class="line"></span><br><span class="line">强行广播,即使用Hint方式,sc指的是小表,可以写别名</span><br><span class="line">select /*+ BROADCASTJOIN(sc) */  或者</span><br><span class="line">select /*+ BROADCAST(sc) */  或者</span><br><span class="line">select /*+ MAPJOIN(sc) */  </span><br><span class="line"></span><br><span class="line">使用dataset api的话</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">                .setAppName(&quot;word count&quot;)</span><br><span class="line">                .setMaster(&quot;local[*]&quot;);</span><br><span class="line">SparkSession spark = InitUtil.initSparkSession(conf);</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; sql1 = spark.sql(&quot;select * from sparktuning.sale_course&quot;);</span><br><span class="line">Dataset&lt;Row&gt; sql2 = spark.sql(&quot;select * from sparktuning.course_shopping_cart&quot;);</span><br><span class="line">org.apache.spark.sql.functions.broadcast(sql1)</span><br><span class="line">        .join(sql2, &quot;courseid&quot;)</span><br><span class="line">        .select(&quot;courseid&quot;)</span><br><span class="line">        .explain();</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="SMB-join"><a href="#SMB-join" class="headerlink" title="SMB join"></a>SMB join</h5><p>概述: 适用于两张大表进行join,指的是sort merge bucket join,所以需要进行分桶</p>
<p>原理是: 首先进行排序,然后根据key进行合并,将相同key的数据放置在同一个桶中,相同key的数据在同一个桶然后进行join,就变成小表之间的join了</p>
<p>要求是: 两张表能分桶,且分桶个数相同,且join的关联列必须是分桶列,也必须是排序列</p>
<p>数据写入分桶表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark</span><br><span class="line">...</span><br><span class="line">.bucketBy(numBuckets, &quot;colName&quot;)</span><br><span class="line">.sortBy(&quot;colName&quot;)</span><br><span class="line">...</span><br><span class="line">.saveAsTable(&quot;databaseName.tableName&quot;)</span><br></pre></td></tr></table></figure>

<p>执行效果差别:通过页面查看,使用和不使用分桶表,都是体现为SortMergeJoin,但是Sort环节的时间发现,分桶join时间缩短</p>
<p>然后观察文件可以发现,hdfs生成文件中,普通的生成文件数量是等同于并行度,即spark.sql.shuffle.partitions数量,分桶的生成文件的数量是等同于桶数量,变相减少了文件数量</p>
<h5 id="数据倾斜"><a href="#数据倾斜" class="headerlink" title="数据倾斜"></a>数据倾斜</h5><p>原因:一般是经过shuffle的时候产生的,涉及到数据的重新分区时,某个key是大key,就产生数据倾斜,可能导致OOM</p>
<p>如何定位大key:首先已经发生了数据倾斜时,此时就进行数据采样,来定位大key</p>
<p>web界面,有一条特别长,其他都短的;或者查看shuffle read的数据量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Dataset&lt;Row&gt; sql1 = spark.sql(&quot;select courseid from sparktuning.course_shopping_cart;&quot;);</span><br><span class="line">        System.out.println(sql1.select(&quot;courseid&quot;)</span><br><span class="line">                .sample(false, 0.1)</span><br><span class="line">                .toJavaRDD()</span><br><span class="line">                .mapToPair(x -&gt; new Tuple2&lt;&gt;(x, 1))</span><br><span class="line">                .reduceByKey((x1, x2) -&gt; (x1 + x2))</span><br><span class="line">                .mapToPair(x -&gt; new Tuple2&lt;&gt;(x._2, x._1))</span><br><span class="line">                .sortByKey(false)</span><br><span class="line">                .take(10));</span><br><span class="line">                </span><br><span class="line">[(500138,[101]), (500022,[103]), (258,[6482]), (254,[2373]), (253,[2770]), (249,[5912]), (246,[2743]), (245,[7628]), (245,[3878]), (245,[3986])]</span><br><span class="line">发现101和103是大key</span><br></pre></td></tr></table></figure>

<ul>
<li>单表数据倾斜优化</li>
</ul>
<p>这种情况不是join产生,而是分组聚合产生,例如dataset的groupby,或者rdd的groupbykey算子等,这种情况实际上不太需要关注,因为本来就会进行预聚合操作,但是有一种场景,倾斜的key大量分布在不同切片中,如果分片很少,那么就不会倾斜了,因为会预聚合,但是分片数量很多,预聚合的效果就不明显了,解决思路</p>
<p>二次聚合,先加盐后去盐:即先给key添加一个随机数前缀或者后缀,然后按照这个新key进行分组聚合,这样大key就被打散,然后分组聚合,然后将新key去盐,去除随机数前缀或者后缀,然后按照key进行分组聚合,这样大key就在不同分区进行分组聚合避免数据倾斜</p>
<ul>
<li><p>join数据倾斜优化</p>
<p>有以下几种解决方式</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">广播join,类似于mapjoin,为了解决大表join小表的问题</span><br><span class="line"></span><br><span class="line">拆分大key,打散大表,扩容小表;适用于join时产生的数据倾斜</span><br><span class="line">逻辑是:</span><br><span class="line">	先进行抽样,找到大key</span><br><span class="line">	假设A表存在数据倾斜,B表不存在数据倾斜</span><br><span class="line">	将A表拆分为skew表和common表,skew表是倾斜key表,common表是不倾斜key表</span><br><span class="line">	将skew表的key全部加上随机数前缀(创建新的一列存储随机数前缀列):封装一个JavaBean,记得实现getset方法,将skew表返回携带随机数前缀的新表</span><br><span class="line">	然后将B表数据量扩大N倍:封装JavaBean,记得实现getset方法,扩容可以使用flatMap实现</span><br><span class="line">	N的取值可以使用分区数量</span><br><span class="line">	然后关联</span><br><span class="line">	from skew join new</span><br><span class="line">	union all</span><br><span class="line">	from common join B</span><br><span class="line">	代价是shuffle增加,适用于顽固倾斜问题</span><br><span class="line">具体代码见IDEA,参考hive优化的场景,是类似的</span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="map端优化"><a href="#map端优化" class="headerlink" title="map端优化"></a>map端优化</h5><p>实际上不需要我们操作什么,因为本身就会执行</p>
<p>那如果使用rdd算子,那么建议使用reducebykey等预聚合算子</p>
<h5 id="小文件优化"><a href="#小文件优化" class="headerlink" title="小文件优化"></a>小文件优化</h5><p>输入小文件优化</p>
<p>对于hive的mr程序,有一个CombineTextInputFormat用来读取小文件使用的</p>
<p>对于spark,需要控制N个文件总大小+N个文件开销&lt;&#x3D;spark.sql.files.maxPartitionBytes参数</p>
<p>文件开销参数的设置?接近小文件大小</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">set(&quot;spark.files.openCostInBytes&quot;, &quot;7194304&quot;)</span><br><span class="line">set(&quot;spark.sql.files.maxPartitionBytes&quot;, &quot;128MB&quot;)</span><br></pre></td></tr></table></figure>

<p>输出小文件优化</p>
<ul>
<li><p>join后的结果插入新表</p>
<p>join后的结果默认生成文件数量就是200,可以调整并行度,或者缩小分区例如coalesce算子,repartition算子</p>
</li>
<li><p>动态分区插入数据</p>
<p>如果没有shuffle,假设对于数据源分成3个Task处理,动态分区有4个,并且极端情况每个分区都在同一个Task出现了,此时产生的小文件数量&#x3D;Task数量*表分区数量&#x3D;12;对于有shuffle情况,有可能是200乘以4&#x3D;800</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">样例sql</span><br><span class="line">insert overwrite table A partition(aa)</span><br><span class="line">select</span><br><span class="line">*</span><br><span class="line">from B</span><br><span class="line">where aa!=大key</span><br><span class="line">distribute by aa;//将小key的数据按照分区字段aa进行分区,相同分区的数据会进入同一个分区中,这样每个分区下就只有一个小文件了</span><br><span class="line"></span><br><span class="line">insert overwrite table A partition(aa)</span><br><span class="line">select</span><br><span class="line">*</span><br><span class="line">from B</span><br><span class="line">where aa=大key</span><br><span class="line">distribute by cast(rand()*5 as int);//将大key分成随机的5个分区,然后写入分区表,这样该分区下就只有五个文件产生,避免产生太多的小文件</span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="Shuffle过程"><a href="#Shuffle过程" class="headerlink" title="Shuffle过程"></a>Shuffle过程</h5><p>1, map端的shuffle叫做shuffle write,reduce端的shuffle叫做shuffle read</p>
<p>有一个缓冲区,大小默认5MB,超过阈值就会尝试2*当前使用的内存,去申请新的内存</p>
<p>​	如果当前使用6MB,默认5MB,则去申请够12MB,即再申请7MB</p>
<p>如果申请不到内存,就会触发溢出写操作,这个参数我们不能设置</p>
<p>2, 溢出写使用的输出流缓冲区的大小默认32KB,可以适当提高来提升效率</p>
<p>3, shuffle涉及到文件序列化,默认每一批次1万条数据量去读写,这个参数我们无法指定</p>
<p>所以我们能调整的参数只有输出流缓冲区大小,kb</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">.set(&quot;spark.shuffle.file.buffer&quot;, &quot;32&quot;)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h5 id="reduce端优化"><a href="#reduce端优化" class="headerlink" title="reduce端优化"></a>reduce端优化</h5><p>reduce个数设置,reduce拉取缓冲区大小设置,减少join后并行度</p>
<ul>
<li><p>合理设置reduce数量</p>
<p>主要是关于并行度,并发度参数的设置,并发度设置&#x3D;executor-cores*num-executor</p>
<p>executor-cores一般设置3-6,设置4</p>
<p>并行度一般是2-3倍的并发度</p>
</li>
<li><p>增大reduce缓冲区,减少拉取次数,讲</p>
<p>reduce将相同分区数据拉到一起的时候,存在一个缓冲区的概念,默认48MB,建议96MB</p>
</li>
<li><p>调节reduce端拉取数据重试次数,不讲</p>
<p>默认3次重试,一般调整到6次</p>
<p>实践发现,针对超级大数据量(数十亿到上百亿)的Shuffle过程,调节该参数可以大幅度提高稳定性</p>
</li>
<li><p>调节reduce端拉取数据的等待间隔,不讲</p>
<p>拉取重试过程中的等待间隔,默认5秒,建议调整60秒</p>
</li>
</ul>
<p>上面三个参数如下配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;spark.reducer.maxSizeInFlight&quot;, &quot;96m&quot;</span><br><span class="line">&quot;spark.shuffle.io.maxRetries&quot;, &quot;6&quot;</span><br><span class="line">&quot;spark.shuffle.io.retryWait&quot;, &quot;60s&quot;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>合理的利用bypass</p>
<p>当shuffleManager是sortshuffleManager的时候,如果shuffle read task的数量小于阈值默认200,并且不需要进行map端合并,则不会进行排序操作,就可以使用bypassMergesortshufflewriter写数据,最后将每个task产生的临时磁盘文件合并成为一个文件,创建单独索引文件</p>
<p>什么时候没有预聚合?预聚合指的是聚合之前先聚合,所以没有聚合自然就没有预聚合,简单的join操作就没有聚合</p>
<p>执行计划看不出来bypass,看执行结果发现不走bypass出现了任务失败情况</p>
<p>不需要设置,因为默认看能不能触发</p>
</li>
</ul>
<h5 id="其他优化"><a href="#其他优化" class="headerlink" title="其他优化"></a>其他优化</h5><ul>
<li><p>调节数据本地化等待时间</p>
<p>一般追求NODE_LOCAL节点本地化即可,如果不是可以吧这个等待时间调大一些</p>
<p>参数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spark.locality.wait  默认3秒,建议6秒,10秒</span><br><span class="line">spark.locality.wait.process  默认3秒,建议60秒</span><br><span class="line">spark.locality.wait.node  默认3秒,建议30秒</span><br><span class="line">spark.locality.wait.rack  默认3秒,建议20秒</span><br><span class="line">注意参数6s,带s</span><br></pre></td></tr></table></figure>
</li>
<li><p>堆外内存参数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.memory.enable.offheap.enable,设置true</span><br></pre></td></tr></table></figure>
</li>
<li><p>调节连接等待时长</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--conf spark.core.connection.ack.wait.timeout=300s</span><br><span class="line">需要在spark-submit设置</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="源码部分"><a href="#源码部分" class="headerlink" title="源码部分"></a>源码部分</h3><h5 id="环境准备阶段源码学习"><a href="#环境准备阶段源码学习" class="headerlink" title="环境准备阶段源码学习"></a>环境准备阶段源码学习</h5><p>以spark on yarn为例说明</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">spark-submit提交任务到RM: 启动一个SparkSubmit的进程,走main方法; 所谓提交指的是封装一些指令提交给rm</span><br><span class="line">类:  org.apache.spark.deploy.SparkSubmit</span><br><span class="line">方法: main                                                                                  	1：submit = new SparkSubmit()</span><br><span class="line">  2：submit.doSubmit(args)</span><br><span class="line">  	...</span><br><span class="line">  	1：解析命令行参数: appArgs = parseArguments(args)</span><br><span class="line">    	创建对象: new SparkSubmitArguments(args)</span><br><span class="line">        1：类的加载过程中，调用了parse方法解析命令行参数，使用正则表达式的方式进行解析,是Java的方法: parse(args.asJava),将--class这样的命令行参数赋值给变量,然后去匹配,具体细节看SparkSubmitArguments的handle方法</span><br><span class="line">        2：类的加载过程中，给action赋默认值SUBMIT, action.getOrElse(SUBMIT),在loadEnvironmentArguments方法中</span><br><span class="line">    2：action方法进行匹配,SUBMIT为提交，触发submit方法: appArgs.action匹配submit(appArgs, uninitLog)方法</span><br><span class="line">	判断是什么集群类型standalone或者其他,然后执行doRunMain方法</span><br><span class="line">    然后判断是否有代理用户,然后执行runMain方法</span><br><span class="line">        1：准备提交环境，获取childMainClass：prepareSubmitEnvironment(args)</span><br><span class="line">            判断是什么集群类型,为childMainClass字符串赋值org.apache.spark.deploy.yarn.YarnClusterApplication,需要添加spark-yarn_2.12依赖查看源码</span><br><span class="line">        2：mainClass = Util.classForName(childMainClass)：通过类名反射获取类的信息</span><br><span class="line">        3：两种情况,创造实例对象,java或者scala版本：SparkApplication app = new xxx</span><br><span class="line">        4：实例对象启动：app.start; 实际上走到了具体实现类YarnClusterApplication类的start方法</span><br><span class="line"></span><br><span class="line">类: org.apache.spark.deploy.yarn.YarnClusterApplication</span><br><span class="line">方法: start方法</span><br><span class="line">    1：实例化过程中：new Client()</span><br><span class="line">    	1：实例化过程中,实例化一个非常重要的对象yarnClient：yarnClient = YarnClient.createYarnClient(),还会创建一个rmClient对象</span><br><span class="line">    2：new Client调用run方法</span><br><span class="line">       获取一个appId,同时调用提交应用程序的方法: appId = submitApplication方法</span><br><span class="line">        	1：connect方法,再加上yarnClient对象调用init、start，此时建立和服务器RM连接</span><br><span class="line">            2：yarnClient对象调用createApplication方法，告诉rm创建一个应用newApp</span><br><span class="line">            3：获取响应的同时,获取app id: newAppResponse = newApp.getNewApplicationResponse</span><br><span class="line">            4：创建容器环境containerContext = createContainerLauncherContext(newAppResponse)</span><br><span class="line">                方法1：包含了一些配置参数</span><br><span class="line">                方法2：包含amClass：集群环境下是org.apache.spark.deploy.yarn.ApplicationMaster；非集群环境下是org.apache.spark.deploy.yarn.ExecutorLauncher</span><br><span class="line">            5：使用app和容器环境，创建提交环境appContext = createApplicationSubmissionContext(newApp,containerContext),appContext包含了一些配置参数</span><br><span class="line">            6：yarnClient等同于建立RM连接，submitApplication等同于提交任务；yarnClient.submitApplication(appContext),</span><br><span class="line">            </span><br></pre></td></tr></table></figure>

<h5 id="启动AM阶段源码"><a href="#启动AM阶段源码" class="headerlink" title="启动AM阶段源码"></a>启动AM阶段源码</h5><p>以spark on yarn为例说明,cluster模式</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">进程方式启动AM: rm让某一个nm以进程的方式启动am</span><br><span class="line">类：org.apache.spark.deploy.yarn.ApplicationMaster</span><br><span class="line">方法：main</span><br><span class="line">	1：解析命令行参数,封装: amArgs = new ApplicationMasterArguments(args)</span><br><span class="line">    	parseArgs解析参数</span><br><span class="line">    2：创建sparkConf和yarnConf，</span><br><span class="line">    3：然后作为参数传入构造器，创建master = new ApplicationMaster(amArgs,sparkConf,yarnConf)</span><br><span class="line">        实例化过程中，client = new YarnRMClient，即AM连接RM的客户端</span><br><span class="line">    4：master.run方法</span><br><span class="line">    	1：集群模式下isClusterMode是true，runDriver方法，会启动一个应用程序，然后一直阻塞等待一个上下文环境对象，所以startUserApplication方法一定要传出一个上下文环境对象,不然下面的ThreadUtils.awaitResult方法会一直阻塞</span><br><span class="line">        	1：调用方法过程中， startUserApplication方法，启动用户应用程序</span><br><span class="line">                1：类加载器，加载--class对应的参数,即用户自定义的主程序类</span><br><span class="line">                2：创建线程启动Driver，包含SparkContext环境对象,AM根据参数启动Driver线程，并初始化SparkContext环境对象，此时开始执行自己的代码的main	</span><br><span class="line">        2：如果sc环境对象不为空，则rpcEnv = sc.env.rpcEnv；rpcEnv指的是通信环境</span><br><span class="line">        3：registerAM方法，即注册AM，保持和Yarn的RM的连接，申请资源,使用YarnRMClient客户端</span><br><span class="line">        4：createAllocator方法,同样使用YarnRMClient客户端</span><br><span class="line">            1：通过allocator = client.createAllocator方法,使用YarnRMClient客户端</span><br><span class="line">            2：allocator.allocateResource方法得到可以分配的资源，即返回资源可用列表               </span><br><span class="line">				1；获取allocatedContainers可用容器</span><br><span class="line">                2：处理可用容器handleAllocatedContainers(allocatedContainers.asScala),做了分类的处理,涉及到机架感知,位置选择等内容, 然后runAllocatedContainers运行已经分配好的容器: 当资源不够的时候,launcherPool线程池，使用线程池启动Executor(YarnCoarseGrainedExecutorBackend类)：实例化对象ExecutorRunnable(该对象里面有一个NMClient,意味着要和NM进行交互关联)，添加相关的构造器参数，并调用run方法启动,run方法明细如下</span><br><span class="line">                	run方法1：实例化对象ExecutorRunnable过程中，产生nmClient对象,用来和某个NM进行关联</span><br><span class="line">                    run方法2：Executor对象的run方法中，nmClient调用init、start方法</span><br><span class="line">                    run方法3：Executor对象的run方法中，还调用startContainer方法启动容器: 向指定NM启动容器，还是进程方式启动：nmClient.startContainer(容器,环境信息)，prepareCommand方法会准备好启动指令; 环境信息中包含命令参数；org.apache.spark.executor.YarnCoarseGrainedExecutorBackend类</span><br><span class="line">        5: resumeDriver方法,等环境都准备好,会让线程继续执行,即自己的开发的程序继续执行  ,查看YarnClusterScheduler类  </span><br></pre></td></tr></table></figure>

<h5 id="启动Executor阶段源码"><a href="#启动Executor阶段源码" class="headerlink" title="启动Executor阶段源码"></a>启动Executor阶段源码</h5><p>以spark on yarn为例说明</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">类: org.apache.spark.executor.YarnCoarseGrainedExecutorBackend</span><br><span class="line">方法: main</span><br><span class="line">	1：run</span><br><span class="line">    	方法1：driver = fetcher.方法，即通过fetcher找到driver建立联系</span><br><span class="line">        方法2：env = SparkEnv.createExecutorEnv通过该方法创建环境</span><br><span class="line">        方法3：env.rpcEnv.setupEndpoint即创建Executor终端，这里是把YarnCoarseGrainedExecutorBackend创建为通信终端，所以我们可以理解为YarnCoarseGrainedExecutorBackend进程就是Executor，但是实际上是用来通信的而不是用来真正计算的; 通信环境的生命周期，constructor、onstart、receive、onstop; setupEndpoint方法具体实现类是NettyRpcEnv</span><br><span class="line">;同时CoarseGrainedExecutorBackend类的onStart方法能够收到消息; 该方法中得到driver,并ask发送消息,消息就是注册Executor;同时Driver线程的SparkContext进行接收注册信息; 使用ask方法发消息,参数是RegisterExecutor注册Executor消息; SparkContext类的SchedulerBackend抽象类的实现类CoarseGrainedSchedulerBackend，包含接收回复等方法</span><br><span class="line">		1: receiveAndReply方法,接收到注册Executor的消息,返回true,自己给自己发消息的</span><br><span class="line">		2: receive方法，自己给自己发消息注册成功了，此时executor = new Executor实例化Executor对象,这个Executor才是真正用来计算的Executor,是一个计算对象</span><br><span class="line">		</span><br></pre></td></tr></table></figure>

<h5 id="整体如下"><a href="#整体如下" class="headerlink" title="整体如下"></a>整体如下</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Yarn,cluster版本：</span><br><span class="line">	1) 执行脚本提交任务，实际是启动一个 SparkSubmit 的 JVM 进程；</span><br><span class="line">    2) SparkSubmit 类中的 main 方法反射调用 YarnClusterApplication 的 main 方法；</span><br><span class="line">    3) YarnClusterApplication 创建 Yarn 客户端，然后向 Yarn 服务器发送执行指令：bin/java </span><br><span class="line">    ApplicationMaster；</span><br><span class="line">    4) Yarn 框架收到指令后会在指定的 NM 中启动 ApplicationMaster；</span><br><span class="line">    5) ApplicationMaster 启动 Driver 线程，执行用户的作业；</span><br><span class="line">    6) AM 向 RM 注册，申请资源；</span><br><span class="line">    7) 获取资源后 AM 向 NM 发送指令：bin/java YarnCoarseGrainedExecutorBackend；</span><br><span class="line">    8) CoarseGrainedExecutorBackend 进程会接收消息，跟 Driver 通信，注册已经启动的</span><br><span class="line">    Executor；然后启动计算对象 Executor 等待接收任务</span><br><span class="line">    9) Driver 线程继续执行完成作业的调度和任务的执行。交叉执行,即Driver线程执行时,注册资源是阻塞的,当执行main方法创建SparkContext以后,注册资源继续执行,完成资源注册初始化后,main方法继续执行,执行到行动算子等</span><br><span class="line">    10) Driver 分配任务并监控任务的执行。</span><br></pre></td></tr></table></figure>

<h5 id="作业-阶段-任务等的关系"><a href="#作业-阶段-任务等的关系" class="headerlink" title="作业,阶段,任务等的关系"></a>作业,阶段,任务等的关系</h5><p>Application对应一个初始化的SparkContext</p>
<p>Job对应一个行动算子</p>
<p>Stage对应宽依赖数量+1</p>
<p>Task对应单个Stage最后RDD的分区个数</p>
<p>由上到下是一对多关系</p>
<h5 id="作业-阶段-任务的源码"><a href="#作业-阶段-任务的源码" class="headerlink" title="作业,阶段,任务的源码"></a>作业,阶段,任务的源码</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">行动算子当中会使用SparkContext对象来调用runJob方法运行任务,经过层层调用后,最终会在DAGScheduler对象处调用runJob方法,最终发送了一个任务提交的消息并由handleJobSubmitted方法进行接收处理,该方法中做了几件事情</span><br><span class="line">	第一件事情,调用createResultStage方法,会计算出一个Job有几个Stage,创建ResultStage对象,该阶段只有一个,调用getOrCreateParentStages方法来根据ShuffleDependency的数量来创建ShuffleMapStage阶段对象的数量,所以说Stage的数量=ShuffleDependency宽依赖的数量+1</span><br><span class="line">	这里需要注意的是如果调用两次相同计算规则的 shuffle 算子，则第二次的算子不会产生 shuffle 过程，自然不会创建新的阶段</span><br><span class="line">	</span><br><span class="line">	第二件事,创建ActiveJob对象,即对应一个Job作业</span><br><span class="line">	第三件事,调用submitStage方法,并传入ResultStage对象,该方法中会判断是否还有上一级阶段,并使用递归调用的方式,一直找到没有上一级阶段的那个阶段,从这里开始向后按照顺序执行,一直执行到ResultStage阶段为止</span><br><span class="line">	这里需要注意的是前一个阶段不执行完成就不会执行后续阶段</span><br><span class="line">	</span><br><span class="line">在第三件事中,具体调用到了submitMissingTasks方法,该方法中做了几件事</span><br><span class="line">	第一件事,任务切分,和任务Task相关,会根据不同阶段类型来创建不同任务类型,例如ShuffleMapStage创建的任务类型是ShuffleMapTask,而ResultStage创建的是ResultTask,至于创建多少个Task对象,根据不同阶段里面的findMissingPartitions方法进行判断,任务数量=每个阶段最后一个RDD的分区数量</span><br><span class="line">	第二件事,任务调度: taskScheduler.submitTasks(new TaskSet()),提交任务到TaskSchedulerImpl中</span><br><span class="line">		1.会将任务Task包装成为TaskSet任务集</span><br><span class="line">		2.TaskSet包装成为TaskSetManager</span><br><span class="line">		3.将TaskSetManager添加到调度器中任务池TaskPool中,调度器有FIFO模式,Fair模式,默认配置来自于配置文件,默认使用FIFO调度模式; rootPool对象调用addSchedulable(传入manager参数)</span><br><span class="line">		4.自己给自己发消息,从池子中取出任务,然后makeOffers方法,任务不为空则启动任务launcherTasks启动任务</span><br><span class="line">		5.取出任务的过程调用resourceOffers,从调度器中取出任务,涉及到FIFO和Fair算法</span><br><span class="line">		6.判断本地化级别,然后决定发给哪里Executor去执行:计算和数据存储级别</span><br><span class="line">			进程本地化:数据和计算在同一进程;最高级别</span><br><span class="line">			节点本地化:数据和计算在同一节点机器</span><br><span class="line">			机架本地化:数据和计算在同一机架</span><br><span class="line">			任意;最低级别</span><br><span class="line">		7.launcherTasks启动任务</span><br><span class="line">		8.找到executorEndpoint终端,发消息send给某个Executor执行,消息是: LauncherTask,当然要把任务序列化传输过去</span><br><span class="line">	第三件事,任务执行:发消息以后Executor端进行接收消息执行,这个就在Executor端执行了</span><br><span class="line">		1.coarseGrainedExecutorBackend类,接收消息</span><br><span class="line">		2.判断是否任务为空,接收任务,反序列化</span><br><span class="line">		3.executor.launcherTask启动任务,将任务数据参数传入</span><br><span class="line">		4.层级关系是:Executor -&gt; ThreadPool -&gt; Thread -&gt; Task,实例化TaskRunner对象,然后线程池threadPool开始执行这个对象任务,每个Task用一个线程执行; tr = new TaskRunner(); threadPool.execute(tr)</span><br><span class="line">		5.每一个具体的任务,都需要重写抽象方法runTask</span><br></pre></td></tr></table></figure>

<h5 id="Netty通信框架"><a href="#Netty通信框架" class="headerlink" title="Netty通信框架"></a>Netty通信框架</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">并不会直接使用Socket进行通信</span><br><span class="line"></span><br><span class="line">spark3版本使用的内部通信框架是Netty框架,各个组件之间的通信依靠EndPoint,一个EndPoint有一个InBox和多个OutBox,具体多少个OutBox取决于你当前的EndPoint要和多少其他的EndPoint进行通信,接收的消息存入的是InBox,而发送出去的消息存入的是OutBox并写入其他EndPoint的InBox,在spark中有两种EndPoint,一个是DriverEndPoint,一个是CoarseGrainedExecutorBackend</span><br><span class="line"></span><br><span class="line">支持AIO异步非阻塞式通信</span><br><span class="line"></span><br><span class="line">Driver通信环境:</span><br><span class="line">SparkContext类，创建Env环境，创建的是NettyRpcEnv环境对象</span><br><span class="line">	NettyRpcEnv类</span><br><span class="line">		服务器TransportServer类,使用Epoll方式模拟AIO</span><br><span class="line">		通信终端RpcEndpoint,receive*,用来收数据</span><br><span class="line">			inbox收件箱,用来存储接收到的数据</span><br><span class="line">		通信终端引用RpcEndpointRef,ask*,用来发数据</span><br><span class="line">			outbox发件箱,根据地址存在多个发件箱,这个地址就是ip地址</span><br><span class="line">		客户端TransportClient,和服务器TransportServer类建立的连接</span><br><span class="line"></span><br><span class="line">Executor通信环境:</span><br><span class="line">CoarseGrainedExecutorBackend类</span><br><span class="line">	NettyRpcEnv类</span><br><span class="line">		服务器TransportServer类,使用Epoll方式模拟AIO</span><br><span class="line">		通信终端RpcEndpoint,receive*,用来收数据</span><br><span class="line">			inbox收件箱,用来存储接收到的数据</span><br><span class="line">		通信终端引用RpcEndpointRef,ask*,用来发数据</span><br><span class="line">			outbox发件箱,根据地址存在多个发件箱,这个地址就是ip地址</span><br><span class="line">		客户端TransportClient,和服务器TransportServer类建立的连接</span><br><span class="line"></span><br><span class="line">Driver发信息就代表Executor收信息,这样的对应关系</span><br></pre></td></tr></table></figure>

<h5 id="不同IO"><a href="#不同IO" class="headerlink" title="不同IO"></a>不同IO</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">java.io, 即BIO, 传统IO, 效率低</span><br><span class="line">java.nio, 即NIO, </span><br><span class="line">java AIO, Windows支持, 异步IO, Linux中使用Epoll来模拟实现AIO, 例如Netty框架</span><br></pre></td></tr></table></figure>

<h5 id="shuffle是什么"><a href="#shuffle是什么" class="headerlink" title="shuffle是什么"></a>shuffle是什么</h5><p>spark的shuffle指的是打乱数据重新组合分区的过程,分为shuffle write和shuffle read两个过程</p>
<p>判断是否经过shuffle,看上游一个分区是否对应下游多个分区</p>
<p>需要注意的是,shuffle前后的RDD的分区数量,可能改变,也可能不变,所以说分区数量不变不代表没有经过shuffle</p>
<p>注意需要等待所有分区的数据都进行了shuffle以后才能继续向下执行后续的RDD,所以数据在内存中无法持续等待,所以需要落盘</p>
<p>磁盘的交互就产生了性能的下降,所以可以通过减少落盘数据量而提升性能,例如预聚合操作(类似于MR的combine归约),另一方面,溢出写磁盘的文件越少性能越高</p>
<h5 id="shuffle写磁盘读磁盘过程"><a href="#shuffle写磁盘读磁盘过程" class="headerlink" title="shuffle写磁盘读磁盘过程"></a>shuffle写磁盘读磁盘过程</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">shuffle写磁盘读磁盘过程</span><br><span class="line">写磁盘:即ShuffleMapStage对应的ShuffleMapTask, 重写runTask方法</span><br><span class="line">    dep.shuffleWriterProcessor.write方法, 指的是Shuffle写磁盘处理器</span><br><span class="line">        1.manager = SparkEnv.get.ShuffleManager:是一个抽象接口,是Shuffle管理器,具体实现是SortShuffleManager一种(早期有两种)</span><br><span class="line">        2.writer = manager.getWriter方法中会匹配三种类型的Writer写磁盘方式,见表格; </span><br><span class="line">        3.writer.write(): 以SortShuffleWriter为例,里面有getWriter和write的具体实现方法,见表格</span><br><span class="line">            1.sorter排序器,需要先进行数据排序,要判断是否支持预聚合,使用的排序方式也不同</span><br><span class="line">            2.sorter.insertAll插入数据然后排序</span><br><span class="line">                1.根据预聚合有不同处理方式:changeValue或者insert方法预聚合或者不预聚合</span><br><span class="line">                  相同key把Value相加update,就是预聚合,即changeValue方法</span><br><span class="line">                2.buffer缓冲区的数据被不断插入数据就会触发溢出写到磁盘临时文件中:maybeSpillCollection方法</span><br><span class="line">                  溢出写的内存大小阈值默认5MB并且数据量%32==0,会尝试申请新的内存,否则会溢出写</span><br><span class="line">                  另外一种情况,强制溢出写数据,阈值是超过int最大值</span><br><span class="line">                  内存溢出写磁盘temp临时文件的buffer缓冲区大小默认32KB</span><br><span class="line">                  临时文件temp不一定存在,因为可能数据量少,没有触发溢写</span><br><span class="line">                  溢写过程的排序先按照分区再按照分区内key排序</span><br><span class="line">            3.sorter.writePartitionedMapOutput,区分是否有溢出写</span><br><span class="line">                没有溢出写,直接操作内存数据</span><br><span class="line">                有溢出写,使用Writer写出到磁盘,也就是合并temp临时文件</span><br><span class="line">                合并使用归并排序mergeSort,先按照分区再按照分区内key排序</span><br><span class="line">            4.commitAllPartitions,具体实现不同,只剩下索引文件和数据文件</span><br><span class="line">            </span><br><span class="line">读磁盘:即ResultStage对应的ResultTask, 真正是在ShuffleRDD中的compute方法中的getReader的read读取,在SortShuffleManager实现类中, 使用BlockStoreShuffleReader类进行读取,调用read方法,读取磁盘文件的内存缓冲区48MB	</span><br></pre></td></tr></table></figure>

<h5 id="shuffle演变的过程"><a href="#shuffle演变的过程" class="headerlink" title="shuffle演变的过程"></a>shuffle演变的过程</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">shuffle演变的过程</span><br><span class="line">    方式1: 上游一个Task生成三份文件,下游三个Task分别去读取对应的文件的数据,但是问题在于一个计算core可能包含多个Task,并且还会有多个core,这样会产生大量的小文件,导致性能急速下降</span><br><span class="line">    方式2: 一个core就生成三份文件,多个Task的数据都写入这三份文件中,然后下游三个Task去对应读取,性能高于方式1;但是下游Task的数量很大,而且core的数量也可能很大,还是会产生大量的小文件</span><br><span class="line">    方式3: 只生成一个文件,但是不同的Task的数据要有对应的索引文件,这样下游的三个Task去读取的时候,就按照索引进行读取(即读取具体的哪一段数据),一个core中的多个Task也写入这一个文件; 这种方式就必须排序,不然没有索引的产生</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="spark的shuffle分类"><a href="#spark的shuffle分类" class="headerlink" title="spark的shuffle分类"></a>spark的shuffle分类</h5><p>spark的shuffle分类,或者说ShuffleWriter的实现类,有三种,SortShuffleWriter,BypassMergeSortShuffleWriter,UnsafeShuffleWriter三种,如下	</p>
<table>
<thead>
<tr>
<th>处理器</th>
<th>写磁盘的对象</th>
<th>使用该对象的条件</th>
</tr>
</thead>
<tbody><tr>
<td>SerializedShuffleHandle</td>
<td>UnsafeShuffleWriter</td>
<td>1.序列化规则中必须支持重定位操作(Java序列化不支持,Kryo序列化支持)<br />2.不能使用预聚合功能<br />3.下游分区数量即Task数量&lt;&#x3D;16777216,这个条件一般都能满足</td>
</tr>
<tr>
<td>BypassMergeSortShuffleHandle</td>
<td>BypassMergeSortShuffleWriter</td>
<td>1.不能使用预聚合功能<br />2.下游分区数量即Task数量&lt;&#x3D;200<br />(分区数量一般该参数配置400到500)</td>
</tr>
<tr>
<td>BaseShuffleHandle</td>
<td>SortShuffleWriter</td>
<td>排序Shuffle写磁盘的方式,不满足上面两种就使用这种</td>
</tr>
</tbody></table>
<h5 id="spark的内存模型是什么样的"><a href="#spark的内存模型是什么样的" class="headerlink" title="spark的内存模型是什么样的"></a>spark的内存模型是什么样的</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">spark的内存模型,或者说Executor的内存模型,分为堆内内存和堆外内存,堆内内存受到jvm的管理,而堆外内存是直接向操作系统申请和释放</span><br><span class="line">堆内内存,由spark-submit参数executor-memory指定,多个task任务共享这个内存</span><br><span class="line">堆外内存默认不开启,可以设置开启并且设置大小,堆外内存没有other部分</span><br><span class="line">堆内和堆外都可以再细分包含storage存储内存和execute执行内存,堆内还包含other其他内存</span><br><span class="line">具体划分的比例如下(堆内为例)</span><br><span class="line">	storage存储内存(指的是Executor内存)</span><br><span class="line">	占比: 30%</span><br><span class="line">		缓存数据cache,persist,广播变量</span><br><span class="line">	execute执行内存</span><br><span class="line">	占比: 30%</span><br><span class="line">		Shuffle过程中的操作占用的内存</span><br><span class="line">	other其他内存</span><br><span class="line">	占比; 40%</span><br><span class="line">		系统占用的内存</span><br><span class="line">		RDD元数据信息占用内存</span><br><span class="line">	预留内存: 300MB, 不包含上面三部分内存</span><br><span class="line">	</span><br><span class="line">存储内存和执行内存可以相互占用,</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="https://nfsp412.github.io/asuka.github.io/2024/10/16/bigdata003/" data-id="cm3aedmzb00070gurgljeaziu" data-title="Spark学习笔记" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/asuka.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag">大数据</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/asuka.github.io/2024/10/21/bigdata007/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">前一篇</strong>
      <div class="article-nav-title">
        
          Clickhouse学习笔记
        
      </div>
    </a>
  
  
    <a href="/asuka.github.io/2024/10/15/bigdata002/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">后一篇</strong>
      <div class="article-nav-title">Hive学习笔记</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/asuka.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag">大数据</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/asuka.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" style="font-size: 10px;">大数据</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/asuka.github.io/archives/2024/10/">十月 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/asuka.github.io/2024/10/24/bigdata005/">Kafka学习笔记</a>
          </li>
        
          <li>
            <a href="/asuka.github.io/2024/10/23/bigdata006/">HBase学习笔记</a>
          </li>
        
          <li>
            <a href="/asuka.github.io/2024/10/21/bigdata004/">Flink学习笔记</a>
          </li>
        
          <li>
            <a href="/asuka.github.io/2024/10/21/bigdata007/">Clickhouse学习笔记</a>
          </li>
        
          <li>
            <a href="/asuka.github.io/2024/10/16/bigdata003/">Spark学习笔记</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 asuka<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/asuka.github.io/" class="mobile-nav-link">Home</a>
  
    <a href="/asuka.github.io/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/asuka.github.io/js/jquery-3.6.4.min.js"></script>



  
<script src="/asuka.github.io/fancybox/jquery.fancybox.min.js"></script>




<script src="/asuka.github.io/js/script.js"></script>





  </div>
</body>
</html>