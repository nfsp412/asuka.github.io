<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Flink学习笔记 | Asuka的个人笔记</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="本文是自己在学习Flink过程中整理的一些基础笔记,仅做记录 概述flink是什么1234大数据流式处理引擎有状态的流式处理低延迟,高吞吐,结果准确,容错性良好,精确一次,高可用,能解决乱序问题能做到每秒百万条数据,毫秒级延迟  flink的分层api1234flink sqlflink cepdatastream(注意区分spark的dataset和dataframe)有状态的流处理proces">
<meta property="og:type" content="article">
<meta property="og:title" content="Flink学习笔记">
<meta property="og:url" content="https://nfsp412.github.io/asuka.github.io/2024/10/21/bigdata004/index.html">
<meta property="og:site_name" content="Asuka的个人笔记">
<meta property="og:description" content="本文是自己在学习Flink过程中整理的一些基础笔记,仅做记录 概述flink是什么1234大数据流式处理引擎有状态的流式处理低延迟,高吞吐,结果准确,容错性良好,精确一次,高可用,能解决乱序问题能做到每秒百万条数据,毫秒级延迟  flink的分层api1234flink sqlflink cepdatastream(注意区分spark的dataset和dataframe)有状态的流处理proces">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-10-21T03:24:38.000Z">
<meta property="article:modified_time" content="2024-11-09T16:21:44.983Z">
<meta property="article:author" content="asuka">
<meta property="article:tag" content="大数据">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/asuka.github.io/atom.xml" title="Asuka的个人笔记" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/asuka.github.io/favicon.png">
  
  
  
<link rel="stylesheet" href="/asuka.github.io/css/style.css">

  
    
<link rel="stylesheet" href="/asuka.github.io/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/asuka.github.io/" id="logo">Asuka的个人笔记</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/asuka.github.io/">Home</a>
        
          <a class="main-nav-link" href="/asuka.github.io/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/asuka.github.io/atom.xml" title="RSS 订阅"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="搜索"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://nfsp412.github.io/asuka.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-bigdata004" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/asuka.github.io/2024/10/21/bigdata004/" class="article-date">
  <time class="dt-published" datetime="2024-10-21T03:24:38.000Z" itemprop="datePublished">2024-10-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Flink学习笔记
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>本文是自己在学习Flink过程中整理的一些基础笔记,仅做记录</p>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><h5 id="flink是什么"><a href="#flink是什么" class="headerlink" title="flink是什么"></a>flink是什么</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">大数据流式处理引擎</span><br><span class="line">有状态的流式处理</span><br><span class="line">低延迟,高吞吐,结果准确,容错性良好,精确一次,高可用,能解决乱序问题</span><br><span class="line">能做到每秒百万条数据,毫秒级延迟</span><br></pre></td></tr></table></figure>

<h5 id="flink的分层api"><a href="#flink的分层api" class="headerlink" title="flink的分层api"></a>flink的分层api</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">flink sql</span><br><span class="line">flink cep</span><br><span class="line">datastream(注意区分spark的dataset和dataframe)</span><br><span class="line">有状态的流处理processfunction</span><br></pre></td></tr></table></figure>

<h5 id="Flink的流批API分别是"><a href="#Flink的流批API分别是" class="headerlink" title="Flink的流批API分别是"></a>Flink的流批API分别是</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">流DataStream</span><br><span class="line">批DataSet</span><br><span class="line">官方推荐流批一体,所以建议都使用流DataStream,可以在提交任务时单独指定批处理</span><br><span class="line">bin/flink run -Dexecution.runtime-mode=BATCH xxx.jar</span><br></pre></td></tr></table></figure>

<h5 id="flink模拟读取无界流数据"><a href="#flink模拟读取无界流数据" class="headerlink" title="flink模拟读取无界流数据"></a>flink模拟读取无界流数据</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">使用nc -lk 7777</span><br><span class="line">使用socketTextStream</span><br></pre></td></tr></table></figure>

<h5 id="如何解释DataFrame的WordCount的结果"><a href="#如何解释DataFrame的WordCount的结果" class="headerlink" title="如何解释DataFrame的WordCount的结果?"></a>如何解释DataFrame的WordCount的结果?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">7&gt; (world,1)</span><br><span class="line">4&gt; (hello,1)</span><br><span class="line">2&gt; (java,1)</span><br><span class="line">4&gt; (hello,2)</span><br><span class="line">4&gt; (hello,3)</span><br><span class="line">10&gt; (flink,1)</span><br></pre></td></tr></table></figure>

<p>体现了流处理,例如hello正是来一条处理一条</p>
<p>前面的编号是?并行度,这里使用IDEA环境下,对应的并行度就是电脑线程数,我这里是12</p>
<h1 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h1><h5 id="集群部署-on-yarn"><a href="#集群部署-on-yarn" class="headerlink" title="集群部署 on yarn"></a>集群部署 on yarn</h5><ul>
<li><p>概述:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">客户端将应用提交给rm,然后rm向nm申请容器资源,会在容器上部署jobmanager和taskmanager的实例,从而启动集群,会根据作业所需要的slot数量动态的分配taskmanager资源</span><br></pre></td></tr></table></figure>
</li>
<li><p>部署流程</p>
<p>略</p>
</li>
<li><p>三种模式的应用</p>
<ul>
<li><p>会话模式</p>
<p>启动集群</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">会话模式,yarn-session.sh</span><br><span class="line">-d 后台运行</span><br><span class="line">-nm 名称</span><br><span class="line">该模式下,yaml配置文件会自动被覆盖,所以可以用standalone模式的安装包</span><br><span class="line">此时8081不再是webUI端口,是yarn随机分配的</span><br><span class="line">启动集群以后如果没有提交作业,没有taskmanager和slot资源,当提交作业以后动态分配资源</span><br><span class="line">相关参数</span><br><span class="line">-jm jm内存</span><br><span class="line">-tm tm内存</span><br><span class="line">-qu yarn队列</span><br><span class="line">-s slots</span><br><span class="line">-j 指定job manager</span><br></pre></td></tr></table></figure>

<p>提交作业</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">提交作业的方式可以在webUI提交,或者命令行提交</span><br><span class="line">作业执行完成后,会资源回收</span><br><span class="line">此时的flink集群就是yarn的一个应用进程,类似于thriftserver服务</span><br><span class="line"></span><br><span class="line">命令行提交作业:bin/flink run -d -c xxx全类名 xxx.jar</span><br><span class="line"></span><br><span class="line">如果此模式下还是想指定到standalone集群,必须在提交作业时使用-m指定集群</span><br><span class="line"></span><br><span class="line">如果提交作业后不能分配到slot资源,可能考虑更换flink-conf.yaml配置文件使用最初默认配置文件,大概率配置了slot但是分配不到资源导致连接超时</span><br></pre></td></tr></table></figure>

<p>取消作业</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">flink list 查看id</span><br><span class="line">flink cancel id</span><br></pre></td></tr></table></figure>

<p>关闭集群</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/tmp/.yarn-properties-root该文件记录了flink作业在yarn集群的id</span><br><span class="line"></span><br><span class="line">关闭session集群的命令行:可以通过webUI关闭集群,或者使用echo &quot;stop&quot; | bin/yarn-session.sh -id application_1700465028835_0002</span><br></pre></td></tr></table></figure>
</li>
<li><p>单作业模式</p>
<p>提交作业</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">单作业模式:直接提交作业,无需预先启动集群资源</span><br><span class="line"></span><br><span class="line">命令行:多一个参数-t yarn-per-job</span><br><span class="line">bin/flink run -d -c xxx全类名 -t yarn-per-job xxx.jar</span><br></pre></td></tr></table></figure>

<p>取消作业</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">如何停止:webUI取消作业,或者命令行使用cancel,停止作业就相当于停止集群资源了</span><br><span class="line"></span><br><span class="line">flink cancel</span><br><span class="line">	bin/flink cancel -t yarn-per-job -Dyarn.application.id=application_1700465028835_0003 f80034defa96f709116e733364ce71b9</span><br><span class="line">	最后要有一个jobid,因为你有可能是yarn session模式存在多个job所以要指定</span><br><span class="line"></span><br><span class="line">如何找到jobid</span><br><span class="line">flink list 比较适用于standalone</span><br><span class="line">	bin/flink list -t yarn-per-job -Dyarn.application.id=application_1700465028835_0003</span><br></pre></td></tr></table></figure>
</li>
<li><p>应用模式</p>
<p>提交作业</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">应用模式,无需启动集群,直接提交作业,并且代码解析放在jobmanager端进行</span><br><span class="line">bin/flink run-application -t yarn-application -c xxx全类名 xxx.jar</span><br><span class="line">直接到后台运行</span><br></pre></td></tr></table></figure>

<p>取消作业</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">如果应用模式:flink cancel -t yarn-application -Dyarn.application.id=xxx jobidxxx</span><br><span class="line"></span><br><span class="line">如何找到jobid</span><br><span class="line">如果应用模式:flink list -t yarn-application -Dyarn.application.id=xxx </span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>异常解决</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">异常:Exception in thread &quot;Thread-5&quot; java.lang.IllegalStateException: Trying to access closed classloader</span><br><span class="line">使用-d会出现,不影响使用</span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="集群部署-on-standalone"><a href="#集群部署-on-standalone" class="headerlink" title="集群部署 on standalone"></a>集群部署 on standalone</h5><ul>
<li><p>概述</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">一般用于开发测试场景,或者作业非常少的情况</span><br></pre></td></tr></table></figure>
</li>
<li><p>部署流程</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">需要配置workers和masters文件</span><br><span class="line">还需要配置flink-conf.yaml</span><br><span class="line">注意:taskmanager.host: hadoop102,该参数需要对应主机进行修改</span><br><span class="line">启动时使用start-cluster,需要到指定job manager节点启动</span><br></pre></td></tr></table></figure>
</li>
<li><p>三种模式的应用</p>
<ul>
<li><p>会话模式</p>
<p>提交作业</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">flink run -m rest_ip:8081 -c xxx全类名 xxx.jar</span><br><span class="line"></span><br><span class="line">注意:</span><br><span class="line">提交成功,但是输出结果还是得去UI界面查看taskmanager</span><br><span class="line">此时ctrl+c不会取消作业,还是得去UI界面取消</span><br><span class="line">Job has been submitted with JobID xxx</span><br></pre></td></tr></table></figure>

<p>取消作业</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">flink list查看id</span><br><span class="line">flink cancel id</span><br></pre></td></tr></table></figure>
</li>
<li><p>单作业模式:standalone不支持</p>
</li>
<li><p>应用模式</p>
<p>提交作业</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">bin/standalone-job.sh start --job-classname xxx全类名</span><br><span class="line"></span><br><span class="line">注意:</span><br><span class="line">jar包必须在lib目录,不然找不到</span><br><span class="line">提交以后到后台执行,进程名称StandaloneApplicationClusterEntryPoint</span><br><span class="line">还需要taskmanager,自己启动:bin/taskmanager.sh start</span><br><span class="line">此时web ui页面能查看,但是你在哪里启动的,对应ip就是哪里,并且此时还没有分配task manager,所以也没有slot显示</span><br><span class="line">注意应该根据你flink-conf.yaml中配置的job manager的地址去对应服务器启动job,因为应用模式启动的就是job manager,所以要去对应的地方启动,此时再启动task manager才能找到对应job manager</span><br></pre></td></tr></table></figure>

<p>取消作业和停止集群</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./bin/standalone-job.sh stop</span><br><span class="line">./bin/taskmanager.sh stop</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h5 id="flink三种部署模式是什么"><a href="#flink三种部署模式是什么" class="headerlink" title="flink三种部署模式是什么?"></a>flink三种部署模式是什么?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">分为会话模式,单作业模式,应用模式</span><br><span class="line">区别在于集群的生命周期以及资源分配的方式,以及main方法在哪里执行,是Client还是jobmanager</span><br><span class="line"></span><br><span class="line">会话模式:</span><br><span class="line">需要先启动集群,保持会话,通过客户端提交作业,所有的作业会竞争集群的资源</span><br><span class="line">适用于单个规模小,执行时间短的大量作业</span><br><span class="line"></span><br><span class="line">单作业模式:</span><br><span class="line">为了更好的隔离资源,使用单作业模式</span><br><span class="line">每个提交的作业,启动一个集群;在提交作业的时候才去启动集群资源;作业执行完成后就会关闭集群释放资源</span><br><span class="line">是实际生产上面的首选模式</span><br><span class="line">代价就是资源必须充沛</span><br><span class="line">需要借助k8s或者yarn执行</span><br><span class="line">perjob在17版本标记过时</span><br><span class="line"></span><br><span class="line">应用模式:</span><br><span class="line">前面两种模式的代码都是在客户端执行,由客户端提交jobmanager,但是需要占用大量网络带宽,也会加重客户端所在节点的资源消耗</span><br><span class="line">不需要客户端,直接把应用提交到jobmanager,为每一个应用创建一个jobmanager,执行结束以后jobmanager就关闭</span><br><span class="line">是新出的用法</span><br><span class="line"></span><br><span class="line">集群生命周期来看:会话模式先启动集群,再提交作业,集群资源不会释放,另外两个提交作业时启动集群,执行完成后关闭集群</span><br><span class="line">main方法执行:会话模式和单作业模式,都是在client解析代码再提交集群,另外的那个是在jobmanager解析代码执行</span><br><span class="line">资源分配:会话模式所有的job去共享资源,另外两个是每个作业单独一份资源</span><br></pre></td></tr></table></figure>

<h5 id="Flink组件架构"><a href="#Flink组件架构" class="headerlink" title="Flink组件架构"></a>Flink组件架构</h5><p>以standalone说明架构图</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">组件:</span><br><span class="line">Client:</span><br><span class="line">	获取代码,解析命令行参数</span><br><span class="line">	生成逻辑流图,即StreamGraph,就是把算子标注出来,并行度标注出来,算子之间的关系标注出来</span><br><span class="line">	转换为作业图,即JobGraph,即合并了算子链</span><br><span class="line">	封装数据,提交给JobManager(通过Actor)</span><br><span class="line">JobManager:</span><br><span class="line">	JobMaster:</span><br><span class="line">		处理单独作业job(一个作业一个)</span><br><span class="line">		从客户端获取任务信息包含jar包,逻辑流图(stream graph 或 data flow),作业图(job graph 进行算子链合并)</span><br><span class="line">		作业图转换成执行图(execution graph 即并行化版本的job graph),即包含了并行度的详细版本</span><br><span class="line">		向RM申请资源</span><br><span class="line">		分发给taskmanager</span><br><span class="line">		检查点工作</span><br><span class="line">	ResourceManager:</span><br><span class="line">		请求slots</span><br><span class="line">		指的是flink的RM而不是YARN的RM(一个集群只有一个)</span><br><span class="line">		资源是TaskManager的任务槽slot</span><br><span class="line">		一个任务task对应一个slot</span><br><span class="line">	Dispatcher:</span><br><span class="line">		启动webui,提交应用</span><br><span class="line">		启动JobMaster</span><br><span class="line">		可以没有</span><br><span class="line">TaskManager:</span><br><span class="line">	数据处理,一个taskmanager包含多个slot,同时一个集群包含多个taskmanager</span><br><span class="line">	向RM进行注册</span><br><span class="line">	提供slot给JobMaster</span><br><span class="line">	和其他TaskManager交换数据</span><br></pre></td></tr></table></figure>

<h5 id="POM文件注意事项"><a href="#POM文件注意事项" class="headerlink" title="POM文件注意事项"></a>POM文件注意事项</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1 建议针对dependence依赖,使用provided</span><br><span class="line">需要在IDEA的run选项的Edit Configuration选项,勾选include dependence with provided scope</span><br><span class="line">或者defaults模板,选择Application,勾选</span><br><span class="line"></span><br><span class="line">2 注意写法:</span><br><span class="line">&lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ServicesResourceTransformer&quot;&gt;</span><br><span class="line">&lt;/transformer&gt;</span><br><span class="line"></span><br><span class="line">3 shade打包注意,需要先clean,再package</span><br></pre></td></tr></table></figure>

<h5 id="历史服务器"><a href="#历史服务器" class="headerlink" title="历史服务器"></a>历史服务器</h5><ul>
<li><p>部署流程</p>
<p>略</p>
</li>
<li><p>注意事项</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">需要在对应服务器启动历史服务</span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="webUI端口是"><a href="#webUI端口是" class="headerlink" title="webUI端口是?"></a>webUI端口是?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">默认端口号8081,可以上传jar包执行,需要配置入口类</span><br></pre></td></tr></table></figure>

<h5 id="并行度有几种设置方式"><a href="#并行度有几种设置方式" class="headerlink" title="并行度有几种设置方式?"></a>并行度有几种设置方式?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">指定并行度有四种方式</span><br><span class="line">优先级从高到低</span><br><span class="line">代码算子指定setParallelism</span><br><span class="line">&gt;</span><br><span class="line">代码全局指定env.setParallelism</span><br><span class="line">&gt;</span><br><span class="line">提交任务指定-p</span><br><span class="line">&gt;</span><br><span class="line">配置文件指定flink-conf.yaml的parallelism.default</span><br></pre></td></tr></table></figure>

<h1 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h1><h5 id="四种流图"><a href="#四种流图" class="headerlink" title="四种流图"></a>四种流图</h5><p>数据流图,类似DAG,web ui可以查看</p>
<p>作业图,经过了算子链合并,</p>
<p>执行图,并行度进行拆分,</p>
<p>物理图,taskmanager根据物理图进行任务执行</p>
<h5 id="算子链"><a href="#算子链" class="headerlink" title="算子链"></a>算子链</h5><p>类比spark,算子种类分为一对一和重分区,类比spark的窄依赖和shuffle</p>
<p>并行度相同而且是一对一,可以连接形成一个大任务,一个大任务使用一个task执行</p>
<p>使用场景:定位问题算子,或者分开两个任务重的算子</p>
<p>大部分场景无需禁用算子链</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">env.disableOperatorChaining();//全局禁用算子链</span><br><span class="line">.disableChaining()//和前后算子断开</span><br><span class="line">.startNewChain()//开启新的算子链,和前面算子断开</span><br></pre></td></tr></table></figure>

<h5 id="共享组"><a href="#共享组" class="headerlink" title="共享组"></a>共享组</h5><p>3 注意,同一个job,不同算子的子任务是可以在同一个slot中运行的,前提是属于同一个slot共享组,默认都是default组,例如下图中map算子的子任务和聚合算子的子任务就可以在同一个slot中运行<br>4 代码中.slotSharingGroup(“AAA”)可以设置指定当前之后的所有算子的共享组</p>
<h5 id="作业提交流程-yarn-单作业模式"><a href="#作业提交流程-yarn-单作业模式" class="headerlink" title="作业提交流程(yarn 单作业模式)"></a>作业提交流程(yarn 单作业模式)</h5><p>on yarn,单作业模式</p>
<p>这种生产常用,以此举例说明</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1 客户端提交任务到yarn的rm,同时上传jar包,配置信息,到hdfs</span><br><span class="line">2 yarn的rm分配container资源,启动job manager,启动的是带有job manager的AM</span><br><span class="line">3 job manager里面的job master组件接收任务,向自身的资源管理器请求资源,</span><br><span class="line">4 自身的资源管理器向yarn集群请求容器资源</span><br><span class="line">5 yarn集群启动带有task manager的容器资源</span><br><span class="line">6 TaskManager 启动之后，向 自身的资源管理器 注册自己的可用任务槽（slots）</span><br><span class="line">7 自身的资源管理器通知 TaskManager 为新的作业提供 slots</span><br><span class="line">8 TaskManager 连接到对应的 JobMaster，提供 slots</span><br><span class="line">9 JobMaster 将需要执行的任务分发给 TaskManager</span><br><span class="line">10 TaskManager 执行任务，互相之间可以交换数据</span><br></pre></td></tr></table></figure>

<h5 id="yarn动态申请资源时申请TM数量如何计算"><a href="#yarn动态申请资源时申请TM数量如何计算" class="headerlink" title="yarn动态申请资源时申请TM数量如何计算?"></a>yarn动态申请资源时申请TM数量如何计算?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">申请的TM数量=job并行度/每个TM的slot数量,向上取整</span><br></pre></td></tr></table></figure>

<h5 id="slot和并行度的关系"><a href="#slot和并行度的关系" class="headerlink" title="slot和并行度的关系"></a>slot和并行度的关系</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">指的是一个TaskManager进程划分为多少个slot使用</span><br><span class="line">1 由于slot均分隔离内存,但是不隔离CPU,所以一般可以配置为CPU核心数</span><br><span class="line">2 配置方式: 在yaml文件中,taskmanager.numberOfTaskSlots参数配置,默认1,即一个taskmanager只有一个slot,这里指的是单个tm配置而不是总数,所以每个服务器的配置都是可以不同的</span><br><span class="line">3 注意,同一个job,不同算子的子任务是可以在同一个slot中运行的,前提是属于同一个slot共享组,默认都是default组,例如下图中map算子的子任务和聚合算子的子任务就可以在同一个slot中运行</span><br><span class="line">4 代码中.slotSharingGroup(&quot;AAA&quot;)可以设置指定当前之后的所有算子的共享组</span><br><span class="line">5 注意一个slot中运行至少一个子任务(或者是算子链形成的一个大的子任务)</span><br><span class="line">6 注意,一个slot中的多个子任务,都是在同时运行的,即使这些子任务之间看起来有先后的顺序,也并不是顺序执行的,如下图map算子子任务和聚合算子子任务实际上并不是先后执行,而是在同一个slot中同时运行的,这也符合flink的代码不动数据流动的思想</span><br><span class="line">7 slot的数量可以配置为cpu核心数,并行度的指定如果对接Kafka可以指定为Kafka分区partition数 </span><br><span class="line"></span><br><span class="line">任务槽slot是静态概念,指的是taskmanager并发的能力,使用taskmanager.numberOfTaskSlots设置</span><br><span class="line">并行度是动态的概念,也就是taskmanager实际使用时的并发能力,通过参数parallelism.default设置</span><br><span class="line"></span><br><span class="line">假设一个三个taskmanager,每个taskmanager有3个slot,则一共9个slot</span><br><span class="line">表示集群最多能并行执行9个同一算子的子任务</span><br><span class="line">我们的wordcount代码,使用了4个算子,而合并算子链以后是3个任务</span><br><span class="line">如果我们默认值,则并行度是1,则流程序总的并行度即为最大算子并行度,也是1,所以只有3个任务,使用一个slot即可</span><br><span class="line"></span><br><span class="line">如果程序并行度大于slot数量,则程序不能运行</span><br><span class="line"></span><br><span class="line">如果是yarn模式,是动态申请资源的,申请的TM数量=job并行度/每个TM的slot数量,向上取整</span><br><span class="line"></span><br><span class="line">页面展示的slot少了几个即并行度指定了几个</span><br><span class="line"></span><br><span class="line">并行度:一个任务拆分成多个子任务分发不同节点进行分布式并行计算,那么子任务的个数就是并行度;一个流的并行度就是所有算子最大的并行度</span><br><span class="line"></span><br><span class="line">任务并行和数据并行</span><br><span class="line"></span><br><span class="line">并行度优先级:算子&gt;env环境&gt;命令行参数-p&gt;配置文件yaml</span><br></pre></td></tr></table></figure>

<h5 id="背压是什么意思"><a href="#背压是什么意思" class="headerlink" title="背压是什么意思?"></a>背压是什么意思?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数据量过大、处理能力不足，短时间堆了大量数据处理不完，产生“背压”（back pressure）。</span><br></pre></td></tr></table></figure>

<h5 id="事件时间是什么意思"><a href="#事件时间是什么意思" class="headerlink" title="事件时间是什么意思?"></a>事件时间是什么意思?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">事件时间，是指每个事件在对应的设备上发生的时间，也就是数据生成的时间。</span><br><span class="line">数据一旦产生，这个时间自然就确定了，所以它可以作为一个属性嵌入到数据中。这其实</span><br><span class="line">就是这条数据记录的“时间戳”（Timestamp）。</span><br></pre></td></tr></table></figure>

<h5 id="水位线是什么意思"><a href="#水位线是什么意思" class="headerlink" title="水位线是什么意思?"></a>水位线是什么意思?</h5><p>水位线 &#x3D; 观察到的最大事件时间 – 最大延迟时间 – 1 毫秒  </p>
<ul>
<li><p>水位线是插入到数据流中的一个标记，可以认为是一个特殊的数据</p>
</li>
<li><p>水位线主要的内容是一个时间戳，用来表示当前事件时间的进展</p>
</li>
<li><p>水位线是基于数据的时间戳生成的</p>
</li>
<li><p>水位线的时间戳必须单调递增，以确保任务的事件时间时钟一直向前推进</p>
</li>
<li><p>水位线可以通过设置延迟，来保证正确处理乱序数据</p>
</li>
<li><p>一个水位线 Watermark(t)，表示在当前流中事件时间已经达到了时间戳 t, 这代表 t 之 前的所有数据都到齐了，之后流中不会出现时间戳 t’ ≤ t 的数据</p>
</li>
<li><p>广播给下游所有并行任务</p>
<p>如果上游有多个并行任务,下游应该选择其中最小的水位线时间戳,即上游多个并行任务都发来自己的水位线,然后下游任务就选择最小的,一旦上游有更新,就判断是否需要更新水位线,如果需要,就发送下游</p>
<p>合流操作类似,也是选择最小的</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">水位线，就是基于事件时间提出的概念</span><br><span class="line">一个数据产生的时刻，就是流处理中事件触发的时间点，这就是“事件时间”</span><br><span class="line">如果我们想要统计一段时间内的数据，需要划分时间窗口，这时只要判断一下时间戳就可以知道数据属于哪个窗口了。</span><br><span class="line">明确了一个数据的所属窗口，还不能直接进行计算。因为窗口处理的是有界数据，我们需要等窗口的数据都到齐了，才能计算出最终的统计结果</span><br><span class="line">我们直接用数据的时间戳来指示当前的时间进展，窗口的关闭自然也是以数据的时间戳等于窗口结束时间为准，这就相当于可以不受网络传输延迟的影响了</span><br><span class="line">但在分布式系统中，这种驱动方式又会有一些问题。因为数据本身在处理转换的过程中会变化，如果遇到窗口聚合这样的操作，其实是要攒一批数据才会输出一个结果，那么下游的数据就会变少，时间进度的控制就不够精细了</span><br><span class="line">另外，数据向下游任务传递时，一般只能传输给一个子任务（除广播外），这样其他的并行子任务的时钟就无法推进了</span><br><span class="line">所以我们应该把时钟也以数据的形式传递出去，告诉下游任务当前时间的进展；而且这个时钟的传递不会因为窗口聚合之类的运算而停滞</span><br><span class="line">这种用来衡量事件时间（Event Time）进展的标记，就被称作“水位线”（Watermark）。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">具体实现上，水位线可以看作一条特殊的数据记录，它是插入到数据流中的一个标记点，</span><br><span class="line">主要内容就是一个时间戳，用来指示当前的事件时间。而它插入流中的位置，就应该是在某个</span><br><span class="line">数据到来之后；这样就可以从这个数据中提取时间戳，作为当前水位线的时间戳了</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">实际应用中，如果当前数据量非常大，可能会有很多数据的时间戳是相同的，这时每来一</span><br><span class="line">条数据就提取时间戳、插入水位线就做了大量的无用功。而且即使时间戳不同，同时涌来的数</span><br><span class="line">据时间差会非常小（比如几毫秒），往往对处理计算也没什么影响。所以为了提高效率，一般</span><br><span class="line">会每隔一段时间生成一个水位线</span><br><span class="line">如何判断?就是判断最近一次数据的事件时间是多少</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">水位线的周期性生成，周期时间是指处理时间（系统时间），而不是事件时间。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">一个 7 秒时产生的数据，生成时间自然要比 9 秒的数据早；但</span><br><span class="line">是经过数据缓存和传输之后，处理任务可能先收到了 9 秒的数据，之后 7 秒的数据才姗姗来迟。</span><br><span class="line">这时如果我们希望插入水位线，来指示当前的事件时间进展，又该怎么做呢</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">只有数据的时间戳比当前时钟大，才能推动时钟前进，这时才插入水位线</span><br><span class="line">如果考虑到大量数据同时到来的处理效率，我们同样可以周期性地生成水位线。这时只需要保存一下之前所有数据中的最大时间戳，需要插入水位线时，就直接以它作为时间戳生成新的水位线，</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这样做尽管可以定义出一个事件时钟，却也会带来一个非常大的问题：我们无法正确处理“迟到”的数据</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">试着多等几秒，也就是把时钟调得更慢一些。最终的目的，就是要让窗口能够把所有迟到数据都收进来，得到正确的计算结果</span><br><span class="line">例如:为了让窗口能够正确收集到迟到的数据，我们也可以等上 2 秒；也就是用当前已有数据的最大时间戳减去 2 秒，就是要插入的水位线的时间戳，这样的话，9 秒的数据到来之后，事件时钟不会直接推进到 9 秒，而是进展到了 7 秒</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">另外需要注意的是，这里一个窗口所收集的数据，并不是之前所有已经到达的数据。因为数据属于哪个窗口，是由数据本身的时间戳决定的，一个窗口只会收集真正属于它的那些数据。也就是说，尽管水位线 W(20)之前有时间戳为 22 的数据到来，10~20 秒的窗口中也不会收集这个数据，进行计算依然可以得到正确的结果</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">水位线看起来比时间戳延迟一些,是因为水位线也是一个时间戳,也是事件驱动的,当第一条数据携带时间戳1000到来时,此时如果打印查看水位线,发现还没到200毫秒触发水位线的时间点,所以还是最小值</span><br><span class="line">当过了200毫秒,此时水位线就变成999了,但是我们看不到这个数字</span><br><span class="line">只有当第二条数据携带时间戳11000到来时,此时打印水位线才能看到999</span><br><span class="line">但是此时还是没有触发第二次水位线的更改,因为水位线都是在数据到来以后根据数据里的时间戳进行触发更改的</span><br><span class="line">所以过了200毫秒,水位线更改为10999,但是我们还是看不到这个数字</span><br><span class="line">只有下一条到来时才能继续看到</span><br></pre></td></tr></table></figure>

<h5 id="窗口是什么"><a href="#窗口是什么" class="headerlink" title="窗口是什么?"></a>窗口是什么?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">窗口,是将无限数据切割成有限的“数据块”进行处理</span><br><span class="line"></span><br><span class="line">窗口,应该理解为桶,来一条数据判断在哪个窗口,然后进入对应的桶</span><br><span class="line"></span><br><span class="line">事件时间窗口的关闭按照水位线的进展来判断</span><br><span class="line"></span><br><span class="line">窗口结束时,触发计算和关闭窗口,这两个操作是可以分开的</span><br><span class="line"></span><br><span class="line">窗口是动态创建的,有对应数据进桶时,进行创建</span><br><span class="line"></span><br><span class="line">窗口同一时间可以有多个,还能够重叠</span><br></pre></td></tr></table></figure>

<h5 id="什么是状态"><a href="#什么是状态" class="headerlink" title="什么是状态"></a>什么是状态</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">像聚合算子,窗口算子,实际上都是有状态的</span><br><span class="line">将这些数据存储在内存中,有容错性,按照key可以进行隔离,避免一个slot多个key之间的状态混乱</span><br><span class="line">可以分布式扩展,状态的重组调整</span><br><span class="line">分为按键分区状态和算子状态</span><br><span class="line">算子状态,只针对当前子任务,和其他并行子任务隔离,需要实现CheckpointedFunction接口,和本地变量一样去使用</span><br><span class="line">按键分区状态,按照key进行隔离+每个子任务和其他子任务隔离</span><br><span class="line">例如value状态,list状态,map状态,聚合状态等等</span><br></pre></td></tr></table></figure>

<h5 id="状态后端是什么"><a href="#状态后端是什么" class="headerlink" title="状态后端是什么"></a>状态后端是什么</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">状态的存储,访问,维护,都是由状态后端这个组件负责</span><br><span class="line">1负责本地状态管理</span><br><span class="line">2负责检查点写入远程,持久化存储</span><br></pre></td></tr></table></figure>

<h5 id="检查点是什么"><a href="#检查点是什么" class="headerlink" title="检查点是什么?"></a>检查点是什么?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">对状态进行持久化保存的快照机制叫作“检查点”（Checkpoint）  </span><br><span class="line"></span><br><span class="line">Flink 对状态进行持久化的方式，就是将当前所有分布式状态进行“快照”保存，写入一个“检查点”（checkpoint）或者保存点（savepoint）保存到外部存储系统中。具体的存储介质，一般是分布式文件系统（distributed file system）</span><br></pre></td></tr></table></figure>

<h1 id="DataStream"><a href="#DataStream" class="headerlink" title="DataStream"></a>DataStream</h1><h2 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h2><h5 id="Source有哪些类型"><a href="#Source有哪些类型" class="headerlink" title="Source有哪些类型"></a>Source有哪些类型</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">集合读取,elements,collection,sequence</span><br><span class="line">文件读取,textFile</span><br><span class="line">从文件读取,使用FileSource(新)</span><br><span class="line">socket读取</span><br><span class="line">kafka读取,即flink作为消费者消费kafka数据</span><br><span class="line">自定义数据源,并行数据源,实现SourceFunction接口,实现run和cancel方法.实现ParallelSourceFunction还能指定并行度</span><br></pre></td></tr></table></figure>

<h5 id="flink的kafkasource的offset消费策略"><a href="#flink的kafkasource的offset消费策略" class="headerlink" title="flink的kafkasource的offset消费策略?"></a>flink的kafkasource的offset消费策略?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">消费策略:OffsetsInitializer类</span><br><span class="line">earliest:一定从最早开始消费</span><br><span class="line">latest:一定从最新开始消费</span><br><span class="line">timestamp:指定消费时间</span><br><span class="line">默认是earliest</span><br><span class="line"></span><br><span class="line">和kafka消费策略不同</span><br><span class="line">earliest:如果有offset,从offset消费,如果没有,从最早消费</span><br><span class="line">latest:如果有offset,从offset消费,如果没有,从最新消费</span><br></pre></td></tr></table></figure>

<h2 id="transfer"><a href="#transfer" class="headerlink" title="transfer"></a>transfer</h2><h5 id="flatmap算子如何使用"><a href="#flatmap算子如何使用" class="headerlink" title="flatmap算子如何使用"></a>flatmap算子如何使用</h5><p>flatmap算子可以返回一条,多条数据,可以实现过滤转换等丰富功能,使用collect收集器向下游发送数据</p>
<p>所以使用场景多</p>
<p>和spark的flatmap或者scala的是不同的</p>
<h5 id="分区算子"><a href="#分区算子" class="headerlink" title="分区算子"></a>分区算子</h5><p>flink使用keyby将数据进行逻辑分区,相同key的数据发送到同一个分区,即同一个slot进行处理,</p>
<p>由于不需要等待所有数据都到齐在计算,所以就不需要使用shuffle</p>
<p>通过key的hash对分区数量取模得到</p>
<h5 id="min和minBy的区别"><a href="#min和minBy的区别" class="headerlink" title="min和minBy的区别?"></a>min和minBy的区别?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">minBy()：与 min()类似，在输入流上针对指定字段求最小值。不同的是，min()只计算指定字段的最小值，其他字段会保留最初第一个数据的值；而 minBy()则会返回包含字段最小值的整条数据。</span><br></pre></td></tr></table></figure>

<h5 id="如何实现全局分区"><a href="#如何实现全局分区" class="headerlink" title="如何实现全局分区"></a>如何实现全局分区</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keyby(x-&gt;true)</span><br></pre></td></tr></table></figure>

<h5 id="flink的富函数类有哪些方法"><a href="#flink的富函数类有哪些方法" class="headerlink" title="flink的富函数类有哪些方法"></a>flink的富函数类有哪些方法</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">使用RichxxxFunction类</span><br><span class="line">获取运行时上下文环境,上下文里面可以获取索引号,状态,任务名称等</span><br><span class="line">有生命周期方法open,调用一次,注意是每个算子子任务执行一次,即并行度设置多个,那么会执行多次的,按照slot区分的,</span><br><span class="line">close方法,做一些清理工作,调用一次,注意是每个算子子任务执行一次,在cancel时执行,异常退出不会执行</span><br></pre></td></tr></table></figure>

<h5 id="flink的物理分区方法有哪些"><a href="#flink的物理分区方法有哪些" class="headerlink" title="flink的物理分区方法有哪些"></a>flink的物理分区方法有哪些</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">物理分区指的是数据真正分配去哪里的意思,逻辑分区指的是按照key进行逻辑划分,是可能数据倾斜的</span><br><span class="line">当数据前后并行度不同的时候实际上就是已经发生了物理分区的</span><br><span class="line"></span><br><span class="line">默认forward,并行度不改变时默认使用</span><br><span class="line">rebalance,轮询分区,按照先后顺序分发数据,nextChannelToSendTo=(nextChannelToSendTo+1)%下游算子并行度</span><br><span class="line">	能够解决数据源的数据倾斜问题</span><br><span class="line">	如果前后的并行度不同,默认调用rebalance方法</span><br><span class="line">shuffle,随机分区,random.nextInt(下游算子并行度)</span><br><span class="line">rescale,类似于rebalance,区别在于分成几个小组,在组内进行轮询发送数据</span><br><span class="line">	应用在数据源存在多个并行数据源的情况下</span><br><span class="line">	注意和rebalance区别</span><br><span class="line">broadcast,数据发送给下游所有子任务</span><br><span class="line">global,全部发送到第一个子任务,并行度的设置就没用了</span><br><span class="line">自定义分区partitionCustom</span><br><span class="line">KeyGroupStreamPartitioner根据key进行两次hash来获取分发的位置</span><br></pre></td></tr></table></figure>

<h5 id="如何上传hdfs提交作业"><a href="#如何上传hdfs提交作业" class="headerlink" title="如何上传hdfs提交作业?"></a>如何上传hdfs提交作业?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">将flink相关依赖事先上传hdfs,这样无需每一次提交作业时,AM即jobmanager都去获取上传一次依赖,jar包也需要上传给jobmanager</span><br><span class="line"></span><br><span class="line">上传lib和plugins到hdfs</span><br><span class="line">[root@hadoop101 ~]# hadoop fs -mkdir /flink-dist</span><br><span class="line">[root@hadoop101 ~]# hadoop fs -put /opt/module/flink-1.17.1/plugins/ /flink-dist</span><br><span class="line">[root@hadoop101 ~]# hadoop fs -put /opt/module/flink-1.17.1/lib/ /flink-dist</span><br><span class="line"></span><br><span class="line">上传jar包到hdfs</span><br><span class="line">[root@hadoop101 ~]# hadoop fs -mkdir /flink-jars</span><br><span class="line">[root@hadoop101 ~]# hadoop fs -put /opt/datas/FlinkModule-1.0-SNAPSHOT.jar /flink-jars</span><br><span class="line"></span><br><span class="line">指定位置提交作业</span><br><span class="line">[root@hadoop101 ~]# flink run-application -t yarn-application -Dyarn.provided.lib.dirs=&quot;hdfs://hadoop101:8020/flink-dist&quot; -c 全类名 hdfs://hadoop101:8020/jar包路径</span><br></pre></td></tr></table></figure>

<h5 id="一个main方法可以生成几个job"><a href="#一个main方法可以生成几个job" class="headerlink" title="一个main方法可以生成几个job?"></a>一个main方法可以生成几个job?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">env.execute();</span><br><span class="line">该方法可以执行多次</span><br><span class="line">但是执行第一个以后就会阻塞</span><br><span class="line">但是如果使用</span><br><span class="line">env.executeAsync();</span><br><span class="line">一个main方法就可以生成多个job</span><br><span class="line"></span><br><span class="line">适用于同一个Source,但是多套不同处理逻辑,还要求写在一起</span><br></pre></td></tr></table></figure>

<h5 id="flink如何实现批处理"><a href="#flink如何实现批处理" class="headerlink" title="flink如何实现批处理"></a>flink如何实现批处理</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">使用datastream开发,执行job时,参数指定BATCH模式</span><br><span class="line"> -Dexecution.runtime-mode=BATCH</span><br></pre></td></tr></table></figure>

<h5 id="flink如何使用POJO类"><a href="#flink如何使用POJO类" class="headerlink" title="flink如何使用POJO类?"></a>flink如何使用POJO类?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">要求是:类是public,属性是public(或者提供getset方法),无参构造器,属性可以序列化;没有非静态的内部类;字段或者属性是非final的;</span><br></pre></td></tr></table></figure>

<h5 id="IDEA如何创建带web界面任务"><a href="#IDEA如何创建带web界面任务" class="headerlink" title="IDEA如何创建带web界面任务"></a>IDEA如何创建带web界面任务</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">常用于测试</span><br><span class="line">无需打包提交集群运行</span><br><span class="line">引入依赖</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-runtime-web&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">使用createLocalEnvironmentWithWebUI</span><br><span class="line"></span><br><span class="line">使用localhost:8081查看</span><br></pre></td></tr></table></figure>

<h5 id="如何解决泛型擦除问题"><a href="#如何解决泛型擦除问题" class="headerlink" title="如何解决泛型擦除问题?"></a>如何解决泛型擦除问题?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// InvalidTypesException 泛型擦除</span><br><span class="line">.returns(Types.TUPLE(Types.STRING,Types.INT))</span><br></pre></td></tr></table></figure>

<h5 id="PVUV指的是什么"><a href="#PVUV指的是什么" class="headerlink" title="PVUV指的是什么"></a>PVUV指的是什么</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PV指的是页面浏览量,即点击次数,不进行去重,就是求count</span><br><span class="line">UV指的是独立访客数,要针对用户去重,再统计次数</span><br></pre></td></tr></table></figure>

<h2 id="常见思路"><a href="#常见思路" class="headerlink" title="常见思路"></a>常见思路</h2><h5 id="如何统计所有用户中访问频次最高的那个"><a href="#如何统计所有用户中访问频次最高的那个" class="headerlink" title="如何统计所有用户中访问频次最高的那个"></a>如何统计所有用户中访问频次最高的那个</h5><p>就是当前访问量最大的用户是谁  </p>
<p>先keyby按照用户分区,然后使用reduce统计每个用户访问频次,然后按照true全局分区,使用reduce统计最大的pv即可</p>
<h5 id="统计-10-秒钟的-url-浏览量，每-5-秒钟更新一次"><a href="#统计-10-秒钟的-url-浏览量，每-5-秒钟更新一次" class="headerlink" title="统计 10 秒钟的 url 浏览量，每 5 秒钟更新一次"></a>统计 10 秒钟的 url 浏览量，每 5 秒钟更新一次</h5><p>使用滑动窗口,窗口大小10秒,步长5秒,(和业务确认是否是这种需求场景)</p>
<p>增量聚合来一条统计一条,全窗口展示包装结果</p>
<h5 id="统计每个用户的-pv，隔一段时间（10s）输出一次结果"><a href="#统计每个用户的-pv，隔一段时间（10s）输出一次结果" class="headerlink" title="统计每个用户的 pv，隔一段时间（10s）输出一次结果"></a>统计每个用户的 pv，隔一段时间（10s）输出一次结果</h5><p>注意不带窗口,所以使用状态实现</p>
<p>使用值状态,一个保存当前的pv值,一个保存定时器的时间戳</p>
<p>首先pv值状态每一条数据就累计一次</p>
<p>然后每次来一条数据就判断定时器时间戳状态是否为空,为空就注册10秒钟定时器,然后更新状态的值变成定时器时间戳</p>
<p>在onTimer方法中,输出结果,并且清空定时器时间戳状态,注意pv值状态不能清空</p>
<h5 id="计算了每个-url-在-10-秒滚动窗口的-pv-指标，每隔-1-秒钟触发一次窗口的计算"><a href="#计算了每个-url-在-10-秒滚动窗口的-pv-指标，每隔-1-秒钟触发一次窗口的计算" class="headerlink" title="计算了每个 url 在 10 秒滚动窗口的 pv 指标，每隔 1 秒钟触发一次窗口的计算"></a>计算了每个 url 在 10 秒滚动窗口的 pv 指标，每隔 1 秒钟触发一次窗口的计算</h5><p>设置10秒滚动窗口,然后设置定时器算子,获取一条数据就先判断状态,如果为空就设置定时器</p>
<p>按照窗口的大小10秒,按照1秒间隔设置定时器</p>
<p>然后在事件时间触发的方法里面进行触发</p>
<p>这样只要数据将事件时间推进到能够触发的时候就会进行触发</p>
<p>当窗口关闭时进行销毁</p>
<h5 id="统计最近10-秒钟内最热门的2个-url-链接，并且每-5-秒钟更新一次"><a href="#统计最近10-秒钟内最热门的2个-url-链接，并且每-5-秒钟更新一次" class="headerlink" title="统计最近10 秒钟内最热门的2个 url 链接，并且每 5 秒钟更新一次"></a>统计最近10 秒钟内最热门的2个 url 链接，并且每 5 秒钟更新一次</h5><p>就是topn场景</p>
<p>这里使用滑动窗口,10秒开窗,并且间隔5秒更新窗口</p>
<p>先正常按照窗口计算,将url,窗口的开始结束时间,统计值,包装pojo类汇总输出,</p>
<p>然后按照true进行分区,再接一个process处理</p>
<p>按照窗口结束时间设置定时器,将来到的数据存入列表状态里面</p>
<p>然后触发定时器的时候,就收集到了窗口内部的所有数据,再排序topn即可</p>
<p>定时器针对相同key相同时间戳去重,</p>
<h5 id="app的支付和第三方的支付进行5秒内的匹配"><a href="#app的支付和第三方的支付进行5秒内的匹配" class="headerlink" title="app的支付和第三方的支付进行5秒内的匹配"></a>app的支付和第三方的支付进行5秒内的匹配</h5><p>用cep</p>
<h5 id="如何实现两条流inner-join-不分窗口"><a href="#如何实现两条流inner-join-不分窗口" class="headerlink" title="如何实现两条流inner join,不分窗口"></a>如何实现两条流inner join,不分窗口</h5><p>两条流按照关联字段key进行分区</p>
<p>使用connect连接到一起</p>
<p>process中,定义两个列表状态,分别存储的是两条流的各自数据</p>
<p>来一条数据就加入列表状态,然后遍历另外一个列表状态进行匹配输出,另外一个处理方法一样</p>
<h5 id="动态规则配置的实现"><a href="#动态规则配置的实现" class="headerlink" title="动态规则配置的实现"></a>动态规则配置的实现</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">在connect时,另一个流是一个广播流,从而实现动态规则配置需求</span><br><span class="line">场景就是,实时变动的规则,用单独的流获取,然后广播下游,下游收到数据会保存为状态,即广播状态</span><br><span class="line">广播状态和广播流的使用</span><br><span class="line">广播状态是一个映射kv类型</span><br><span class="line"></span><br><span class="line">DataStream调用broadcast方法得到广播流,传入描述器,然后将要处理的数据流和广播流进行connect连接,得到广播连接流BroadcastConnectedStream,在这里可以使用广播状态</span><br><span class="line"></span><br><span class="line">注意得到广播流传入的描述器,和connect连接后内部实现类中使用的广播状态,是同一个名字</span><br></pre></td></tr></table></figure>

<h5 id="如何实现10秒滚动窗口统计每个url的pv值"><a href="#如何实现10秒滚动窗口统计每个url的pv值" class="headerlink" title="如何实现10秒滚动窗口统计每个url的pv值"></a>如何实现10秒滚动窗口统计每个url的pv值</h5><p>使用状态</p>
<p>按照url进行keyby</p>
<p>然后在process中,定义map状态</p>
<p>key是窗口的开始时间,value是对应的pv值</p>
<p>每次来一条数据,,注册这个开始时间+10秒-1的定时器,即窗口结束时间的定时器</p>
<p>然后判断是哪个窗口开始时间的,更新pv值+1</p>
<p>定时器触发后onTimer中,输出内容,并且移除该窗口开始时间key对应的值,相当于销毁窗口</p>
<h5 id="算子状态的应用案例"><a href="#算子状态的应用案例" class="headerlink" title="算子状态的应用案例"></a>算子状态的应用案例</h5><p>我们数据源输出到下游系统时,为了能够实现检查点,</p>
<p>,对状态进行持久化保存的快照机制叫作“检查点”（Checkpoint）  </p>
<p>实现了checkpointed Function接口,在snapshotState  方法中,将缓存的数据写入列表状态,即检查点钟</p>
<p>,在initializeState  方法中,如果是判断是故障恢复,即isRestored  ,将状态的数据又写入到缓存数据的集合中</p>
<h2 id="Sink"><a href="#Sink" class="headerlink" title="Sink"></a>Sink</h2><h5 id="sink有哪些类型"><a href="#sink有哪些类型" class="headerlink" title="sink有哪些类型"></a>sink有哪些类型</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">自定义sink,实现SinkFunction类,实现invoke方法</span><br><span class="line">redis,依赖添加bahir,实现RedisSink</span><br><span class="line">es,ElasticsearchSink类</span><br><span class="line">文件系统,使用StreamingFileSink,无需添加额外依赖,可以指定行类型文件或者列存储类型,还可以指定文件的滚动策略,</span><br><span class="line">hbase,需要自定义</span><br><span class="line">kakfa,此时flink作为生产者,该kafka实现了两阶段提交,实现了事务性保证,实现精确一次生产</span><br><span class="line">输出mysql,即jdbc,使用JdbcSink</span><br></pre></td></tr></table></figure>

<h2 id="时间和窗口"><a href="#时间和窗口" class="headerlink" title="时间和窗口"></a>时间和窗口</h2><h5 id="水位线的周期时间指的是"><a href="#水位线的周期时间指的是" class="headerlink" title="水位线的周期时间指的是?"></a>水位线的周期时间指的是?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">指的是生成水位线本身的周期触发时间,是处理时间,默认200毫秒</span><br><span class="line">以对于水位线的周期性生成，周期时间是指处理时间（系统时间），而不是事件时间。默认200毫秒</span><br><span class="line">env.getConfig().setAutoWatermarkInterval(100);单位毫秒</span><br><span class="line">也有基于数据时间或者数据内容生成水位线的策略,断点式</span><br></pre></td></tr></table></figure>

<h5 id="水位线生成的算子和位置"><a href="#水位线生成的算子和位置" class="headerlink" title="水位线生成的算子和位置"></a>水位线生成的算子和位置</h5><p>推荐在数据源中生成水位线,离数据源越近越好</p>
<p>数据源DataStreamSource调用assignTimestampsAndWatermarks设置水位线</p>
<p>也可以在数据源本身当中发送水位线</p>
<p>注意如果在自定义数据源发送水位线,就不能再调用该方法</p>
<h5 id="窗口有哪些分类"><a href="#窗口有哪些分类" class="headerlink" title="窗口有哪些分类?"></a>窗口有哪些分类?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">按照驱动类型分类</span><br><span class="line">时间窗口:按照时间进行截取数据</span><br><span class="line">	事件时间窗口</span><br><span class="line">	系统时间窗口</span><br><span class="line">countWindow计数窗口:按照数据个数进行截取数据</span><br><span class="line"></span><br><span class="line">按照窗口中数据分配的原则进行分类</span><br><span class="line">TumblingEventTimeWindows滚动窗口:将数据均匀的切片划分窗口,参数只有一个,就是定义窗口大小的参数</span><br><span class="line">	这个参数可以是时间,也可以是数据个数</span><br><span class="line">	不存在重叠</span><br><span class="line">	是一种特殊的滑动窗口</span><br><span class="line">SlidingEventTimeWindows滑动窗口:和滚动窗口一样,大小是固定的,所以这是一个参数;但是还有一个参数代表滑动的步长,即下一个窗口开始的时间间隔即为步长</span><br><span class="line">	如果步长等于窗口大小,就变成滚动窗口了</span><br><span class="line">	如果步长大于窗口大小,就变成不相连的窗口了</span><br><span class="line">	一个数据可能属于多个窗口,有重叠的情况下</span><br><span class="line">EventTimeSessionWindows会话窗口:指的是数据间隔多久时间没到,就关闭窗口,参数就是这个间隔时间</span><br><span class="line">	一旦有数据超过这个间隔时间,那么就应该属于下一个会话窗口了,前一个窗口也应该关闭,这意味着会话窗口是不会重叠或者连在一起的</span><br><span class="line">	这个间隔可以静态设置,也可以设置提取器动态设置</span><br><span class="line">	实际上的间隔应该是大于等于我们设置的间隔的,因为就是有可能两个数据的间隔大于这个间隔size</span><br><span class="line">	长度不确定,窗口的起始结束时间不确定</span><br><span class="line">	对于乱序数据,flink是针对每一个数据开启一个会话窗口,然后判断两个数据的窗口之间的距离,是不是小于size,是的话要合并merge</span><br><span class="line">GlobalWindows全局窗口:指的是分组聚合的分组条件key,将相同key的所有数据都划分到一个窗口中去计算</span><br><span class="line">	问题是没有结束,如何计算?需要使用触发器</span><br><span class="line">	计数窗口,实际上就是全局窗口实现的,触发器就规定结束时的数据个数</span><br><span class="line">	</span><br><span class="line">按照key分类</span><br><span class="line">按键分区窗口:指的是先分组keyby,再开窗计算,这样就并行计算,也是符合大多数计算场景的,window</span><br><span class="line">非按键分区窗口:所有数据到同一个task执行,并行度是1,不推荐,windowAll</span><br></pre></td></tr></table></figure>

<h5 id="窗口聚合函数有哪些"><a href="#窗口聚合函数有哪些" class="headerlink" title="窗口聚合函数有哪些"></a>窗口聚合函数有哪些</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">reduce方法和不进行窗口聚合的reduce方法一样,传入ReduceFunction实现类,重写reduce方法,针对两个数据进行聚合,得到的结果再和下一条数据进行聚合,问题在于输入和输出的数据类型一样</span><br><span class="line">aggregate,传入AggregateFunction重写四个方法,三个参数类型都可以不同,输入,累加器,输出都可以不同,第一个方法初始化累加器,第二个方法进行累加器的更新,第三个方法输出结果,第四个方法用来累加器合并(一般在会话窗口时使用),核心就是有状态的流处理</span><br><span class="line"></span><br><span class="line">apply,要被弃用了,用来处理全窗口的数据的,批处理思路,没有累加器,是因为所有数据都必须攒齐了再计算,</span><br><span class="line">process方法,代替apply,传入ProcessWindowFunction,泛型1输入,泛型2输出,泛型3是key,泛型4是窗口信息,重写process方法,</span><br><span class="line">该方法参数1是key,参数2是上下文环境对象,参数3是所有数据的迭代器,参数4是out写出对象</span><br><span class="line">能够获取更丰富的窗口信息,有上下文环境对象</span><br><span class="line"></span><br><span class="line">使用aggregate方法时,传参AggregateFunction和ProcessWindowFunction,这样就相当于结合二者了,代表着还是增量聚合而不是全窗口聚合,但是还能够获取窗口的上下文环境对象;操作的方法是,在AggregateFunction中,将累加器结果通过getResult方法传入ProcessWindowFunction的process方法的element迭代器对象参数,然后使用上下文环境对象对这个结果进行更丰富的操作</span><br></pre></td></tr></table></figure>

<h5 id="flink如何处理迟到数据"><a href="#flink如何处理迟到数据" class="headerlink" title="flink如何处理迟到数据?"></a>flink如何处理迟到数据?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">水位线,一般设置毫秒级别</span><br><span class="line"></span><br><span class="line">允许延迟allowedLateness算子,指的是窗口不关闭,数据可以继续进入窗口触发计算,只有水位线推进到窗口结束时间加延迟时间在关闭,会至少先触发一次窗口计算,然后后面继续更新结果,这证明窗口的触发计算和清除销毁是可以分开的</span><br><span class="line">注意迟到数据每一次到来一条就会触发一次计算的,因为本身已经满足触发条件了</span><br><span class="line">这就是Lambda架构的思想</span><br><span class="line">分钟级别</span><br><span class="line"></span><br><span class="line">侧输出流,窗口关闭处理结果已经统计,但是额外再针对侧输出流数据进行计算再进行合并,就回归正确了</span><br></pre></td></tr></table></figure>

<h5 id="聚合函数有哪些"><a href="#聚合函数有哪些" class="headerlink" title="聚合函数有哪些"></a>聚合函数有哪些</h5><p>如果是不开窗,直接分组聚合,那么可以使用min,minBy,max,maxBy,sum,reduce,process</p>
<p>如果进行了开窗,可以使用额外的apply,aggregate</p>
<h5 id="窗口时间和水位线时间"><a href="#窗口时间和水位线时间" class="headerlink" title="窗口时间和水位线时间"></a>窗口时间和水位线时间</h5><p>水位线&#x3D;观察的最大事件时间-延迟-1毫秒</p>
<p>窗口时间,左开右闭的</p>
<p>如果窗口5秒,那么5秒数据到来时可以触发窗口计算和关闭,此时的水位线是4999,而窗口的数据不包含这条五秒钟的数据</p>
<p>窗口内最大时间戳,指的也不是水位线,指的就是单纯的窗口结束时间-1毫秒</p>
<p>水位线和事件时间相关,指的是当前事件时间-延迟-1毫秒</p>
<h5 id="全局窗口和非按键分区窗口"><a href="#全局窗口和非按键分区窗口" class="headerlink" title="全局窗口和非按键分区窗口"></a>全局窗口和非按键分区窗口</h5><p>全局窗口指的是GlobalWindows  ,调用的window算子中传入的窗口函数的一种,按照key将数据全部发送的各自窗口中,没有窗口的结束,所以要结合触发器算子使用,例如计数窗口就是这样实现的</p>
<p>非按键分区窗口:所有数据到同一个task执行,并行度是1,不推荐,windowAll算子</p>
<h1 id="Process-Function"><a href="#Process-Function" class="headerlink" title="Process Function"></a>Process Function</h1><h5 id="ProcessFunction有哪些分类"><a href="#ProcessFunction有哪些分类" class="headerlink" title="ProcessFunction有哪些分类?"></a>ProcessFunction有哪些分类?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">基于DataStream调用ProcessFunction 常用</span><br><span class="line">基于KeyedStream调用KeyedProcessFunction 常用</span><br><span class="line">基于WindowedStream调用ProcessWindowFunction,他也是我们提到的全窗口函数 常用</span><br><span class="line">基于DataStream直接开窗WindowAll得到AllWindowedStream,调用ProcessAllWindowFunction</span><br><span class="line">connect合流以后得到ConnectedStreams调用  CoProcessFunction</span><br><span class="line">interval join间隔连接以后得到IntervalJoined调用  ProcessJoinFunction</span><br><span class="line">基于BroadcastConnectedStream调用  BroadcastProcessFunction</span><br><span class="line">	指的是未进行KeyBy的DataStream和广播流BroadcastStream做连接,得到广播连接流</span><br><span class="line">基于BroadcastConnectedStream调用 KeyedBroadcastProcessFunction</span><br><span class="line">	指的是进行KeyBy的KeyedStream和广播流BroadcastStream做连接,得到广播连接流</span><br></pre></td></tr></table></figure>

<h5 id="定时服务是什么"><a href="#定时服务是什么" class="headerlink" title="定时服务是什么?"></a>定时服务是什么?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">在KeyedProcessFunction中,可以使用定时器和定时服务</span><br><span class="line">使用定时服务timer service来注册或者删除定时器,</span><br><span class="line">定时器分为事件时间和处理时间两种语义</span><br><span class="line">当定时器设置的时间触发时,在onTimer中可以收到并进行处理</span><br><span class="line">定时器会按照key和设置的时间戳进行去除,所以重复设置也只会在onTimer中触发一次</span><br><span class="line">时间戳按照毫秒进行区分,</span><br><span class="line">由于onTimer和processElement方法存在同步,所以不用担心出现并发修改问题</span><br><span class="line">定时器有容错性,可以保存在检查点</span><br></pre></td></tr></table></figure>

<h5 id="为什么水位线看起来比时间戳延迟一些"><a href="#为什么水位线看起来比时间戳延迟一些" class="headerlink" title="为什么水位线看起来比时间戳延迟一些"></a>为什么水位线看起来比时间戳延迟一些</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">水位线看起来比时间戳延迟一些,是因为水位线也是一个时间戳,也是事件驱动的,当第一条数据携带时间戳1000到来时,此时如果打印查看水位线,发现还没到200毫秒触发水位线的时间点,所以还是最小值</span><br><span class="line">当过了200毫秒,此时水位线就变成999了,但是我们看不到这个数字</span><br><span class="line">只有当第二条数据携带时间戳11000到来时,此时打印水位线才能看到999</span><br><span class="line">但是此时还是没有触发第二次水位线的更改,因为水位线都是在数据到来以后根据数据里的时间戳进行触发更改的</span><br><span class="line">所以过了200毫秒,水位线更改为10999,但是我们还是看不到这个数字</span><br><span class="line">只有下一条到来时才能继续看到</span><br></pre></td></tr></table></figure>

<h2 id="合流操作"><a href="#合流操作" class="headerlink" title="合流操作"></a>合流操作</h2><h5 id="flink如何进行合流操作"><a href="#flink如何进行合流操作" class="headerlink" title="flink如何进行合流操作?"></a>flink如何进行合流操作?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">union,数据流的合并,要求数据类型相同,可以合并多条流</span><br><span class="line">	水位线:按照最小的为准</span><br><span class="line">	可以使用map.flatmap,filter,process等</span><br><span class="line">connect,数据流的连接,数据类型可以不同,返回的是ConnectedStreams,不能直接打印,一次只能连接一条流,对应的map,flatmap,process算子,使用的是CoMapFunction,CoFlatMapFunction,coProcessFunction等</span><br></pre></td></tr></table></figure>

<h5 id="flink如何实现分流操作"><a href="#flink如何实现分流操作" class="headerlink" title="flink如何实现分流操作?"></a>flink如何实现分流操作?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">process结合侧输出流</span><br><span class="line">实际应用中,例如ETL,只选择自己需要的数据去Kafka消费,其他数据通过侧输出流发送到Kafka其他标签</span><br><span class="line">或者在侧输出流提示告警信息</span><br><span class="line"></span><br><span class="line">使用侧输出流,是底层processFunction的处理方式</span><br><span class="line"></span><br><span class="line">使用ctx.output输出侧输出流数据</span><br><span class="line">使用out.collect输出主流数据</span><br><span class="line"></span><br><span class="line">定义侧输出流标签时,使用&#123;&#125;方便提取泛型</span><br><span class="line"></span><br><span class="line">使用getSideOutput来获取指定的侧输出流的数据</span><br></pre></td></tr></table></figure>

<h5 id="flink的窗口join有哪些"><a href="#flink的窗口join有哪些" class="headerlink" title="flink的窗口join有哪些"></a>flink的窗口join有哪些</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">可以理解为基于时间的合流connect操作</span><br><span class="line">实际上是更为顶层化的算子,不必从底层开始写</span><br><span class="line"></span><br><span class="line">分类:窗口联结window join,间隔连接Interval join,窗口组联结coGroup</span><br><span class="line"></span><br><span class="line">窗口联结使用方式:</span><br><span class="line">	两条DataStream流,调用join,然后使用where,where指定第一条流的key,equalTo,equalTo指定第二条流的key,进行连接,然后使用window开窗,使用apply设置窗口函数</span><br><span class="line">	就相当于select * from tb1 join tb2 on tb1.id=tb2.id;</span><br><span class="line">	apply窗口函数去实现JoinFunction类或者FlatJoinFunction类</span><br><span class="line">	Window和apply之间可以使用触发器,允许延迟等算子</span><br><span class="line">	每一对匹配的数据进入apply的实现类的join方法,有返回值,flat版本的实现类可以没有返回值,更灵活</span><br><span class="line">	做笛卡尔积</span><br><span class="line">	</span><br><span class="line">间隔连接使用方式:</span><br><span class="line">	间隔连接指的是针对数据开辟前后的一段时间间隔,看在这段时间内是否有另一条流的数据过来匹配上,</span><br><span class="line">	默认是闭区间,边界值能够匹配上</span><br><span class="line">	先keyby,然后intervalJoin,里面的流也要keyby,再between确定范围,在process处理,实现ProcessJoinFunction类</span><br><span class="line">	每检测到一组匹配的数据就调用一次processElement方法</span><br><span class="line">	keyby intervaljoin between process</span><br><span class="line">	仅仅支持事件时间,内连接</span><br><span class="line">	</span><br><span class="line">窗口组联结使用方式:</span><br><span class="line">	是更加通用的window join,window join底层使用的就是cogroup</span><br><span class="line">	可以实现外连接满外连接,更通用</span><br><span class="line">	使用方式把join换成coGroup即可</span><br><span class="line">	把窗口内数据放入集合中,一次性调用一次coGroup方法</span><br></pre></td></tr></table></figure>

<h2 id="状态编程"><a href="#状态编程" class="headerlink" title="状态编程"></a>状态编程</h2><h5 id="按键分区状态有哪些-有哪些方法"><a href="#按键分区状态有哪些-有哪些方法" class="headerlink" title="按键分区状态有哪些,有哪些方法"></a>按键分区状态有哪些,有哪些方法</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">必须基于keyed stream</span><br><span class="line"></span><br><span class="line">必须状态描述器 参数1状态数据的名称,参数2状态保存数据的类型</span><br><span class="line"></span><br><span class="line">值状态ValueState接口,value获取值,update更新值,还需要有状态描述器ValueStateDescriptor	</span><br><span class="line">	每一个key的状态从null开始不断进行更新</span><br><span class="line">	value和update方法</span><br><span class="line"></span><br><span class="line">列表状态ListState接口,add添加值,update更新,get获取迭代器,状态描述器ListStateDescriptor</span><br><span class="line">	每一个key的状态从空列表开始,列表会一直追加状态数据</span><br><span class="line">	add和get方法,update</span><br><span class="line"></span><br><span class="line">映射状态MapState接口,get获取key对应value,put更新,remove移除,等</span><br><span class="line">	get和put方法,remove,等等</span><br><span class="line">	key,value形式存储数据</span><br><span class="line"></span><br><span class="line">归约状态ReducingState,描述器是ReducingStateDescriptor,类似于值状态,但是需要针对所有值进行归约</span><br><span class="line">	add和get方法进行使用</span><br><span class="line">	输入和输出类型必须一致</span><br><span class="line"></span><br><span class="line">聚合状态AggregatingState,和归约状态区别在于,是更加一般化的聚合函数</span><br><span class="line">	add和get方法使用</span><br><span class="line">	输入和输出类型可以不一致,和中间值也可以不一致</span><br></pre></td></tr></table></figure>

<h5 id="本地变量和按键分区状态变量的区别"><a href="#本地变量和按键分区状态变量的区别" class="headerlink" title="本地变量和按键分区状态变量的区别?"></a>本地变量和按键分区状态变量的区别?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">本地变量无法根据key进行隔离操作</span><br><span class="line">状态变量则能够进行key进行隔离操作</span><br></pre></td></tr></table></figure>

<h5 id="ReducingState和AggregatingState的区别"><a href="#ReducingState和AggregatingState的区别" class="headerlink" title="ReducingState和AggregatingState的区别?"></a>ReducingState和AggregatingState的区别?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">AggregatingState是更加一般化通用化的聚合状态</span><br><span class="line">输入,中间值,输出,都可以不一致</span><br></pre></td></tr></table></figure>

<h5 id="状态的生存时间TTL"><a href="#状态的生存时间TTL" class="headerlink" title="状态的生存时间TTL"></a>状态的生存时间TTL</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">即TTL,状态可能会越来越多,那么就需要进行clear清理,或者设置TTL</span><br><span class="line">在open方法中,创建了状态以后,就可以设置TTL</span><br><span class="line">setUpdateType,代表什么时候更新状态的TTL,默认是创建状态和写操作时会更新TTL</span><br><span class="line">setStateVisibility,由于状态的清除并不是实时操作的,所以到了过期时间以后还是可能继续存在,那么此时能不能获取状态呢?,默认是从不返回过期值,也就是只要过期就认为被清除状态,不能获取</span><br><span class="line"></span><br><span class="line">只支持处理时间</span><br><span class="line">针对每一项状态进行设置,例如集合或者映射状态</span><br></pre></td></tr></table></figure>

<h5 id="算子状态有哪些"><a href="#算子状态有哪些" class="headerlink" title="算子状态有哪些"></a>算子状态有哪些</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">一个算子并行子任务上面定义的状态,和key无关</span><br><span class="line">一般用在source和sink上面,例如自定义Sink或者source时,为了保证故障恢复使用算子状态</span><br><span class="line">例如kafka消费,维护的偏移量,保证精确一次</span><br><span class="line"></span><br><span class="line">类型有哪些:</span><br><span class="line">列表状态ListState</span><br><span class="line">	不会按照key分开,所以一个并行子任务只有一个列表状态</span><br><span class="line">	并行度改变时,按照轮询方式,将所有子任务合并成一个大列表的状态,然后分发</span><br><span class="line">	算子状态没有键组的概念,所以一个并行子任务只有一个大的状态,所以没办法按照键组的形式进行重分配,所以算子状态中,没有值状态的概念,因为一旦只有一个值状态,那就没办法进行重新分配了,只有最基本的列表状态,才能在合并一起以后再进行重新分配</span><br><span class="line">	</span><br><span class="line">联合列表状态UnionListState</span><br><span class="line">	区别在于,并行度缩放时,分配策略不同</span><br><span class="line">	会把整个状态的大列表广播给下游所有子任务,子任务自行选择使用哪些丢弃哪些,资源和效率考虑不建议使用</span><br><span class="line">	</span><br><span class="line">广播状态BroadcastState</span><br><span class="line">	必须基于广播流使用</span><br><span class="line">	每一个并行子任务,都有一份相同的广播状态</span><br><span class="line"></span><br><span class="line">对于算子状态的使用,发生故障恢复时,比较麻烦,因为没有key,不知道去下游哪个分区,发生故障重启之后，我们不能保证某个数据跟之前一样，进入到同一个并行子任务、访问同一个状态,所以需要实现CheckpointedFunction更为底层的接口</span><br></pre></td></tr></table></figure>

<h5 id="状态后端的分类"><a href="#状态后端的分类" class="headerlink" title="状态后端的分类"></a>状态后端的分类</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">默认哈希表状态后端</span><br><span class="line"></span><br><span class="line">1哈希表状态后端</span><br><span class="line">状态存储在内存,把状态当做对象存储JVM的堆上,普通状态,窗口数据,触发器,都按照kv存储,底层是一个哈希表</span><br><span class="line">检查点的保存,一般放在分布式文件系统</span><br><span class="line">性能最佳,耗费内存</span><br><span class="line"></span><br><span class="line">2内嵌RocksDB状态后端</span><br><span class="line">kv类型数据库,持久化到该数据库,存储本地硬盘</span><br><span class="line">适用于状态很大,窗口周期长,键值对状态很大的场景</span><br><span class="line">支持增量检查点的状态后端</span><br><span class="line"></span><br><span class="line">最大的区别在于本地状态存放哪里,第一种存储内容,第二种存储数据库RocksDB</span><br></pre></td></tr></table></figure>

<h1 id="容错机制"><a href="#容错机制" class="headerlink" title="容错机制"></a>容错机制</h1><h5 id="检查点和保存点的区别"><a href="#检查点和保存点的区别" class="headerlink" title="检查点和保存点的区别?"></a>检查点和保存点的区别?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">保存点是手动触发的,建议使用uid方法为每一个算子指定id</span><br><span class="line">检查点是自动的</span><br></pre></td></tr></table></figure>

<h5 id="flink-kafka如何保证精确一次"><a href="#flink-kafka如何保证精确一次" class="headerlink" title="flink+kafka如何保证精确一次"></a>flink+kafka如何保证精确一次</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">flink端,开启检查点并设置精确一次</span><br><span class="line"></span><br><span class="line">flink kafka consumer端,没有特殊的设置</span><br><span class="line">	如果读一个设置了精确一次的主题,需要添加ConsumerConfig.ISOLATION_LEVEL_CONFIG为read_committed</span><br><span class="line"></span><br><span class="line">flink kafka producer端,</span><br><span class="line">	ProducerConfig.TRANSACTIONAL_ID_CONFIG事务id</span><br><span class="line">	FlinkKafkaProducer.Semantic.EXACTLY_ONCE精确一次</span><br><span class="line">	ProducerConfig.TRANSACTION_TIMEOUT_CONFIG小于15min</span><br></pre></td></tr></table></figure>

<h1 id="Table-API-SQL"><a href="#Table-API-SQL" class="headerlink" title="Table API &amp; SQL"></a>Table API &amp; SQL</h1><h5 id="表环境有什么用处"><a href="#表环境有什么用处" class="headerlink" title="表环境有什么用处"></a>表环境有什么用处</h5><p>表环境,指的是tableEnvironment</p>
<p>可以注册catalog和表,useCatalog,useDatabase,createTemporaryView  </p>
<p>可以执行sql查询语句,executeSql,sqlQuery</p>
<p>可以注册udf函数,createTemporaryFunction</p>
<p>可以将dataStream和表table之间进行转换,fromDataStream,toDataStream等</p>
<p>设置参数,例如ttl之类,getConfig</p>
<h5 id="创建表-视图的语法"><a href="#创建表-视图的语法" class="headerlink" title="创建表&#x2F;视图的语法"></a>创建表&#x2F;视图的语法</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">1 创建连接器表,即输入source和输出sink的表</span><br><span class="line">create table if not exists xxx.xxx.xxx (</span><br><span class="line">    --常规列</span><br><span class="line">    user string,</span><br><span class="line">    --元数据列</span><br><span class="line">    record_time timestamp_ltz(3) metadata from &#x27;timestamp&#x27;,--kafka标记的时间戳</span><br><span class="line">    --计算列</span><br><span class="line">    cost as f1 * f2, --f1,f2是两个常规列</span><br><span class="line">    --定义水位线</span><br><span class="line">    watermark for ts as ts - interval &#x27;xxx&#x27; second,</span><br><span class="line">    --主键</span><br><span class="line">    primary key(xxx) not enforce</span><br><span class="line">)</span><br><span class="line">with(</span><br><span class="line">	k=v,...</span><br><span class="line">) ;</span><br><span class="line"></span><br><span class="line">2 like创建表,快速复制表使用</span><br><span class="line">create table xxx (增加的列)</span><br><span class="line">with (增加的/覆盖的属性kv)</span><br><span class="line">like xxx;</span><br><span class="line"></span><br><span class="line">3 ctas创建表,一般用来创建中间过程当中的临时视图view</span><br><span class="line">create view xxx</span><br><span class="line">with (xxx)//一般没加这个</span><br><span class="line">as select xxx from xxx;</span><br></pre></td></tr></table></figure>

<h5 id="表和流的转换"><a href="#表和流的转换" class="headerlink" title="表和流的转换"></a>表和流的转换</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">流转换成表,两种方式,追加查询和更新更新查询,更新查询当中的编码有insert,update_before,update_after,delete,要结合row类型和rowKind使用</span><br><span class="line">使用fromDataStream,转换成Table表,再使用createTemporaryView</span><br><span class="line">推荐直接使用:或者直接使用createTemporaryView,第一个依然是注册的表名，而第二个可以直接就是DataStream。之后仍旧可以传入多个参数，用来指定表中的字段</span><br><span class="line">使用$需要导包,$(&quot;id&quot;)这样使用,可以使用as</span><br><span class="line">fromChangelogStream将一个更新日志流转换成表,要求流中的数据类型只能是 Row,而且每一个数据都需要指定当前行的更新类型（RowKind）详见代码</span><br><span class="line"></span><br><span class="line">表转换成流,仅追加流,撤回流,更新插入流,区别在于更新插入流需要有一个主键key,根据key判断是插入还是更新</span><br><span class="line">toDataStream 仅插入流</span><br><span class="line">toChangelogStream 有更新操作的表(更新日志流),仅插入流</span><br><span class="line">在api代码中使用,是没有办法调用相关方法转换成更新插入流的,这种流一般是提供给外部系统使用的,例如Kafka就能够实现更新插入流</span><br></pre></td></tr></table></figure>

<h5 id="Row类型如何使用"><a href="#Row类型如何使用" class="headerlink" title="Row类型如何使用"></a>Row类型如何使用</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Row.of定义Row</span><br><span class="line">RowKind定义编码类型</span><br><span class="line"></span><br><span class="line">可以在创建dataStream时使用,例如下面</span><br><span class="line">Row.ofKind(RowKind.INSERT, &quot;Alice&quot;, 12)</span><br></pre></td></tr></table></figure>

<h5 id="事件时间和处理时间如何定义"><a href="#事件时间和处理时间如何定义" class="headerlink" title="事件时间和处理时间如何定义"></a>事件时间和处理时间如何定义</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">事件时间,定义表时指定</span><br><span class="line">ts BIGINT,</span><br><span class="line">ts_ltz AS TO_TIMESTAMP_LTZ(ts, 3),</span><br><span class="line">WATERMARK FOR ts_ltz AS time_ltz - INTERVAL &#x27;5&#x27; SECOND</span><br><span class="line"></span><br><span class="line">处理时间,定义表时指定</span><br><span class="line">ts AS PROCTIME()</span><br></pre></td></tr></table></figure>

<h5 id="窗口如何定义"><a href="#窗口如何定义" class="headerlink" title="窗口如何定义"></a>窗口如何定义</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">  id,</span><br><span class="line">  window_start, --窗口开始时间</span><br><span class="line">  window_end,</span><br><span class="line">  sum(vc) as s</span><br><span class="line">from table(</span><br><span class="line">  cumulate (table ws, descriptor(et), interval &#x27;5&#x27; minute, interval &#x27;1&#x27; hour)累计</span><br><span class="line">  TUMBLE(table ws,descriptor(et),interval &#x27;1&#x27; hour)滚动</span><br><span class="line">  hop(table ws,descriptor(et),interval &#x27;5&#x27; minutes,interval &#x27;1&#x27; hour)滑动</span><br><span class="line">)</span><br><span class="line">group by id,</span><br><span class="line">window_start,window_end --可选</span><br></pre></td></tr></table></figure>

<h5 id="连接器种类有哪些"><a href="#连接器种类有哪些" class="headerlink" title="连接器种类有哪些"></a>连接器种类有哪些</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">输出</span><br><span class="line">&#x27;connector&#x27;=&#x27;print&#x27;</span><br><span class="line"></span><br><span class="line">快速生成数据集</span><br><span class="line">&#x27;connector&#x27;=&#x27;datagen&#x27;,</span><br><span class="line">&#x27;rows-per-second&#x27;=&#x27;1&#x27;, --每秒条数</span><br><span class="line">&#x27;fields.ts.kind&#x27;=&#x27;sequence&#x27;, --序列类型</span><br><span class="line">&#x27;fields.ts.start&#x27;=&#x27;1&#x27;,</span><br><span class="line">&#x27;fields.ts.end&#x27;=&#x27;1000000&#x27;,</span><br><span class="line">&#x27;fields.vc.kind&#x27;=&#x27;random&#x27;, --随机数类型</span><br><span class="line">&#x27;fields.vc.min&#x27;=&#x27;1&#x27;,</span><br><span class="line">&#x27;fields.vc.max&#x27;=&#x27;100&#x27;</span><br><span class="line">&#x27;fields.dim.length&#x27;=&#x27;1&#x27; --字段值长度</span><br><span class="line"></span><br><span class="line">连接jdbc</span><br><span class="line">&#x27;connector&#x27;=&#x27;jdbc&#x27;,</span><br><span class="line">&#x27;url&#x27;=&#x27;jdbc:mysql://hadoop101:3306/customerdb&#x27;,</span><br><span class="line">&#x27;table-name&#x27;=&#x27;customers&#x27;</span><br><span class="line"></span><br><span class="line">连接kafka</span><br><span class="line">  &#x27;connector&#x27; = &#x27;kafka&#x27;,</span><br><span class="line">  &#x27;topic&#x27; = &#x27;user_behavior&#x27;,</span><br><span class="line">  &#x27;properties.bootstrap.servers&#x27; = &#x27;localhost:9092&#x27;,</span><br><span class="line">  &#x27;properties.group.id&#x27; = &#x27;testGroup&#x27;,</span><br><span class="line">  &#x27;scan.startup.mode&#x27; = &#x27;earliest-offset&#x27;,</span><br><span class="line">  &#x27;format&#x27; = &#x27;csv&#x27;</span><br><span class="line">  </span><br><span class="line">连接upsert kafka</span><br><span class="line">创建表时必须指定主键,并且不支持offset,因为都是得从头读取的</span><br><span class="line">主键的选择就可以选择分组字段,支持联合主键</span><br><span class="line">  &#x27;connector&#x27; = &#x27;upsert-kafka&#x27;,</span><br><span class="line">  &#x27;properties.bootstrap.servers&#x27; = &#x27;hadoop102:9092&#x27;,</span><br><span class="line">  &#x27;topic&#x27; = &#x27;ws2&#x27;,</span><br><span class="line">  &#x27;key.format&#x27; = &#x27;json&#x27;,</span><br><span class="line">  &#x27;value.format&#x27; = &#x27;json&#x27;</span><br><span class="line"></span><br><span class="line">文件系统</span><br><span class="line">  &#x27;connector&#x27; = &#x27;filesystem&#x27;,</span><br><span class="line">  &#x27;path&#x27; = &#x27;hdfs://hadoop102:8020/data/t3&#x27;,</span><br><span class="line">  &#x27;format&#x27; = &#x27;csv&#x27;</span><br><span class="line"></span><br><span class="line">jdbc</span><br><span class="line">建表时指定主键则按照upsert操作,没有指定主键则按照insert操作</span><br><span class="line">&#x27;connector&#x27;=&#x27;jdbc&#x27;,</span><br><span class="line">    &#x27;url&#x27; = &#x27;jdbc:mysql://hadoop102:3306/test?useUnicode=true&amp;characterEncoding=UTF-8&#x27;,</span><br><span class="line">    &#x27;username&#x27; = &#x27;root&#x27;,</span><br><span class="line">    &#x27;password&#x27; = &#x27;000000&#x27;,</span><br><span class="line">    &#x27;connection.max-retry-timeout&#x27; = &#x27;60s&#x27;,</span><br><span class="line">    &#x27;table-name&#x27; = &#x27;ws2&#x27;,</span><br><span class="line">    &#x27;sink.buffer-flush.max-rows&#x27; = &#x27;500&#x27;,</span><br><span class="line">    &#x27;sink.buffer-flush.interval&#x27; = &#x27;5s&#x27;,</span><br><span class="line">    &#x27;sink.max-retries&#x27; = &#x27;3&#x27;,</span><br><span class="line">    &#x27;sink.parallelism&#x27; = &#x27;1&#x27;</span><br><span class="line"></span><br><span class="line">连接hbase做lookup join</span><br><span class="line">指定主键,指定rowkey,指定f1 row&lt;&gt;等</span><br><span class="line">&#x27;connector&#x27;=&#x27;hbase14&#x27;</span><br><span class="line">&#x27;lookup.cache-type&#x27;=&#x27;None&#x27;</span><br><span class="line">&#x27;lookup.parallelism&#x27;=&#x27;1&#x27;</span><br><span class="line">&#x27;lookup.cache.ttl&#x27;=&#x27;60000&#x27;</span><br><span class="line">&#x27;lookup.cache-max-rows&#x27;=&#x27;10000&#x27;</span><br><span class="line">&#x27;table-name&#x27;=&#x27;xx:xxx&#x27;</span><br><span class="line">&#x27;zookeeper.znode.parent&#x27;=&#x27;/xxx&#x27;</span><br><span class="line">&#x27;zookeeper.quorum&#x27;=&#x27;xxx:2181,...&#x27;</span><br></pre></td></tr></table></figure>

<h5 id="sql客户端如何使用"><a href="#sql客户端如何使用" class="headerlink" title="sql客户端如何使用?"></a>sql客户端如何使用?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">启动了flink集群后(yarn或者standalone模式)</span><br><span class="line">embedded可以省略</span><br><span class="line">使用bin/sql-client embedded -s yarn-session (基于yarn session模式下)</span><br></pre></td></tr></table></figure>

<h5 id="sql客户端的常用配置"><a href="#sql客户端的常用配置" class="headerlink" title="sql客户端的常用配置"></a>sql客户端的常用配置</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">结果的显式模式:changelog,tableau,table(默认)</span><br><span class="line">set sql-client.execution.result-mode=changelog;</span><br><span class="line"></span><br><span class="line">执行环境的设置: streaming(默认) batch</span><br><span class="line">set execution.runtime-mode=streaming;</span><br><span class="line"></span><br><span class="line">设置并行度</span><br><span class="line">set parallelism.default=1;</span><br><span class="line"></span><br><span class="line">设置状态的ttl时间,单位毫秒</span><br><span class="line">set table.exec.state.ttl=1000;</span><br></pre></td></tr></table></figure>

<h5 id="如何执行sql文件"><a href="#如何执行sql文件" class="headerlink" title="如何执行sql文件"></a>如何执行sql文件</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">使用bin/sql-client embedded -s yarn-session (基于yarn session模式下) -i sql文件路径</span><br></pre></td></tr></table></figure>

<h5 id="topn和去重的实现方式有啥区别"><a href="#topn和去重的实现方式有啥区别" class="headerlink" title="topn和去重的实现方式有啥区别"></a>topn和去重的实现方式有啥区别</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">6 支持固定格式实现topN</span><br><span class="line">topN对应的查询是更新查询,因为topN的数据一直在动态变化,所以涉及到先删除再插入</span><br><span class="line">SELECT </span><br><span class="line">	user, url, ts, row_num</span><br><span class="line">FROM (</span><br><span class="line">    SELECT *,</span><br><span class="line">        ROW_NUMBER() OVER (</span><br><span class="line">        PARTITION BY user</span><br><span class="line">        ORDER BY CHAR_LENGTH(url) desc 可以非时间字段,可以降序,固定写法不能改变</span><br><span class="line">        ) AS row_num</span><br><span class="line">	FROM EventTable)</span><br><span class="line">WHERE row_num &lt;= 2</span><br><span class="line">	</span><br><span class="line">7 支持duplication去重</span><br><span class="line">语法格式和topN一样,区别在于order by的字段是时间类型字段</span><br><span class="line">所以和topN的区分主要看order by的字段,是时间类型字段则代表去重,如果是非时间类型字段则代表topN</span><br><span class="line">建议使用asc去重,这样可以避免-U+U</span><br><span class="line">SELECT </span><br><span class="line">	user, url, ts, row_num</span><br><span class="line">FROM (</span><br><span class="line">    SELECT *,</span><br><span class="line">        ROW_NUMBER() OVER (</span><br><span class="line">        PARTITION BY user</span><br><span class="line">        ORDER BY ts asc 可以非时间字段,可以降序,固定写法不能改变</span><br><span class="line">        ) AS row_num</span><br><span class="line">	FROM EventTable)</span><br><span class="line">WHERE row_num = 1</span><br></pre></td></tr></table></figure>

<h5 id="lookup-join如何书写"><a href="#lookup-join如何书写" class="headerlink" title="lookup join如何书写"></a>lookup join如何书写</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">即维表关联</span><br><span class="line">10 支持lookup join,相当于流数据和外部系统(例如hbase,redis,mysql等)进行join</span><br><span class="line">示例:</span><br><span class="line">	select</span><br><span class="line">	*</span><br><span class="line">	from tb1</span><br><span class="line">	join tb2_dim for system_time as of tb1.proc_time as tb2</span><br><span class="line">	on tb1.id=tb2.id</span><br></pre></td></tr></table></figure>

<h5 id="如何自定义udtf函数"><a href="#如何自定义udtf函数" class="headerlink" title="如何自定义udtf函数"></a>如何自定义udtf函数</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">继承TableFunction,一般泛型设置Row类型,返回也是Row类型</span><br><span class="line">一般使用@FunctionHint注解指定类型信息</span><br><span class="line">在代码中createTemporaryFunction注册udtf函数</span><br><span class="line">或者add jar,然后create table function xxx with xxx</span><br><span class="line">然后结合侧视图使用</span><br><span class="line">from xxx left join lateral table(xx(xx)) as xx(xx,xx)</span><br></pre></td></tr></table></figure>

<h5 id="sql查询语法"><a href="#sql查询语法" class="headerlink" title="sql查询语法"></a>sql查询语法</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line">1 distinct语句</span><br><span class="line">状态实现,所以必须设置ttl,否则状态越来越大</span><br><span class="line"></span><br><span class="line">2 支持count(*)</span><br><span class="line">实现方式就是更新查询,即先删除再增加</span><br><span class="line"></span><br><span class="line">3 支持分组聚合查询,使用更新查询方式</span><br><span class="line"></span><br><span class="line">4 支持grouping sets,rollup,cube</span><br><span class="line">group by grouping sets (</span><br><span class="line">	(xx,xx,xx),</span><br><span class="line">	(...)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">5 支持开窗函数</span><br><span class="line">select xxx() over(partition by xxx order by xxx range/rows xxx)</span><br><span class="line">注意order by的字段只能是时间戳,而且只能升序</span><br><span class="line">range范围表示按照时间区间 range between interval &#x27;1&#x27; second preceding and current row</span><br><span class="line">rows范围表示按照行数 rows between xx and xx</span><br><span class="line">示例</span><br><span class="line">select</span><br><span class="line">  id,</span><br><span class="line">  et,</span><br><span class="line">  vc,</span><br><span class="line">  count(vc) over(</span><br><span class="line">    partition by id </span><br><span class="line">    order by et </span><br><span class="line">    range between interval &#x27;10&#x27; second preceding and current row</span><br><span class="line">  ) as cnt</span><br><span class="line">from ws;</span><br><span class="line">或者写作:</span><br><span class="line">select</span><br><span class="line">  id,</span><br><span class="line">  et,</span><br><span class="line">  vc,</span><br><span class="line">  count(vc) over w as cnt</span><br><span class="line">from ws</span><br><span class="line">window w as (</span><br><span class="line">  partition by id </span><br><span class="line">  order by et </span><br><span class="line">  range between interval &#x27;10&#x27; second preceding and current row</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">6 支持固定格式实现topN</span><br><span class="line">topN对应的查询是更新查询,因为topN的数据一直在动态变化,所以涉及到先删除再插入</span><br><span class="line">select</span><br><span class="line">	*</span><br><span class="line">from (</span><br><span class="line">	select</span><br><span class="line">		*,</span><br><span class="line">		row_number() over(partition by xxx order by xxx ) as rownum</span><br><span class="line">	from xxx</span><br><span class="line">)</span><br><span class="line">where </span><br><span class="line">	rownum &lt;= N</span><br><span class="line">	</span><br><span class="line">7 支持duplication去重</span><br><span class="line">语法格式和topN一样,区别在于order by的字段是时间类型字段</span><br><span class="line">所以和topN的区分主要看order by的字段,是时间类型字段则代表去重,如果是非时间类型字段则代表topN</span><br><span class="line">建议使用asc去重,这样可以避免-U+U</span><br><span class="line"></span><br><span class="line">8 支持常规join</span><br><span class="line">针对inner join,查询方式为追加查询,动态表转换成流是仅追加流,因为只有匹配到的数据才会输出,并且不会有撤回的情况,编码是+[L,R]</span><br><span class="line">针对左连接/右连接,主表的一边到达以后,无论从表是否有匹配数据,都会直接先输出+[L,R]或者+[L,null],如果右边到达了能够匹配的数据,则先撤销-[L,null]再插入+[L,R]</span><br><span class="line">full join类似</span><br><span class="line">注意事项</span><br><span class="line">	可以是不等值join,但是不推荐使用,压力大</span><br><span class="line">	等值join数据进行hash shuffle,按照关联条件发送下游</span><br><span class="line">	状态会无限增大,因为所有数据都需要存储在状态中,所以必须设置合适的ttl</span><br><span class="line">	撤销编码使用-D,插入编码使用+I,这里没有使用-U和+U</span><br><span class="line"></span><br><span class="line">9 支持间隔连接join</span><br><span class="line">针对间隔连接,全部都是+I,这个和inner join编码一致,不存在删除撤回	</span><br><span class="line">示例:</span><br><span class="line">	select</span><br><span class="line">	*</span><br><span class="line">	from ws,ws1</span><br><span class="line">	where ws.id=ws1.id</span><br><span class="line">	and ws.et between ws1.et-interval &#x27;2&#x27; second and ws1.et+interval &#x27;2&#x27; second;</span><br><span class="line"></span><br><span class="line">10 支持lookup join,相当于流数据和外部系统(例如hbase,redis,mysql等)进行join</span><br><span class="line">示例:</span><br><span class="line">	select</span><br><span class="line">	*</span><br><span class="line">	from tb1</span><br><span class="line">	join tb2_dim for system_time as of tb1.proc_time as tb2</span><br><span class="line">	on tb1.id=tb2.id</span><br><span class="line">	</span><br><span class="line">11 支持自定义函数udf,udtf,udaf</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://nfsp412.github.io/asuka.github.io/2024/10/21/bigdata004/" data-id="cm3ae5bae00042kur417650xs" data-title="Flink学习笔记" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/asuka.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag">大数据</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/asuka.github.io/2024/10/23/bigdata006/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">前一篇</strong>
      <div class="article-nav-title">
        
          HBase学习笔记
        
      </div>
    </a>
  
  
    <a href="/asuka.github.io/2024/10/21/bigdata007/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">后一篇</strong>
      <div class="article-nav-title">Clickhouse学习笔记</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/asuka.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag">大数据</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/asuka.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" style="font-size: 10px;">大数据</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/asuka.github.io/archives/2024/10/">十月 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/asuka.github.io/2024/10/24/bigdata005/">Kafka学习笔记</a>
          </li>
        
          <li>
            <a href="/asuka.github.io/2024/10/23/bigdata006/">HBase学习笔记</a>
          </li>
        
          <li>
            <a href="/asuka.github.io/2024/10/21/bigdata004/">Flink学习笔记</a>
          </li>
        
          <li>
            <a href="/asuka.github.io/2024/10/21/bigdata007/">Clickhouse学习笔记</a>
          </li>
        
          <li>
            <a href="/asuka.github.io/2024/10/16/bigdata003/">Spark学习笔记</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 asuka<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/asuka.github.io/" class="mobile-nav-link">Home</a>
  
    <a href="/asuka.github.io/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/asuka.github.io/js/jquery-3.6.4.min.js"></script>



  
<script src="/asuka.github.io/fancybox/jquery.fancybox.min.js"></script>




<script src="/asuka.github.io/js/script.js"></script>





  </div>
</body>
</html>