<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Kafka学习笔记 | Asuka的个人笔记</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="本文是自己在学习Kafka过程中整理的一些基础笔记,仅做记录 概述消息队列是什么12345678基于文件或者数据库都是离线计算,但是实际上可能走消息队列更多所以一般结合消息队列加flume实时读取加spark或者flink功能解耦合,限流削峰,异步处理,消息驱动(即来了消息才驱动后续代码功能执行)消息传递的两种模式点对点发布订阅  Kafka是什么12Kafka是一个分布式的基于发布&#x2F;订阅模式的消">
<meta property="og:type" content="article">
<meta property="og:title" content="Kafka学习笔记">
<meta property="og:url" content="https://nfsp412.github.io/asuka.github.io/2024/10/24/bigdata005/index.html">
<meta property="og:site_name" content="Asuka的个人笔记">
<meta property="og:description" content="本文是自己在学习Kafka过程中整理的一些基础笔记,仅做记录 概述消息队列是什么12345678基于文件或者数据库都是离线计算,但是实际上可能走消息队列更多所以一般结合消息队列加flume实时读取加spark或者flink功能解耦合,限流削峰,异步处理,消息驱动(即来了消息才驱动后续代码功能执行)消息传递的两种模式点对点发布订阅  Kafka是什么12Kafka是一个分布式的基于发布&#x2F;订阅模式的消">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-10-24T03:24:38.000Z">
<meta property="article:modified_time" content="2024-11-09T16:28:55.469Z">
<meta property="article:author" content="asuka">
<meta property="article:tag" content="大数据">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/asuka.github.io/atom.xml" title="Asuka的个人笔记" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/asuka.github.io/favicon.png">
  
  
  
<link rel="stylesheet" href="/asuka.github.io/css/style.css">

  
    
<link rel="stylesheet" href="/asuka.github.io/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/asuka.github.io/" id="logo">Asuka的个人笔记</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/asuka.github.io/">Home</a>
        
          <a class="main-nav-link" href="/asuka.github.io/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/asuka.github.io/atom.xml" title="RSS 订阅"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="搜索"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://nfsp412.github.io/asuka.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-bigdata005" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/asuka.github.io/2024/10/24/bigdata005/" class="article-date">
  <time class="dt-published" datetime="2024-10-24T03:24:38.000Z" itemprop="datePublished">2024-10-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Kafka学习笔记
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>本文是自己在学习Kafka过程中整理的一些基础笔记,仅做记录</p>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><h5 id="消息队列是什么"><a href="#消息队列是什么" class="headerlink" title="消息队列是什么"></a>消息队列是什么</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">基于文件或者数据库都是离线计算,但是实际上可能走消息队列更多</span><br><span class="line">所以一般结合消息队列加flume实时读取加spark或者flink</span><br><span class="line"></span><br><span class="line">功能解耦合,限流削峰,异步处理,消息驱动(即来了消息才驱动后续代码功能执行)</span><br><span class="line"></span><br><span class="line">消息传递的两种模式</span><br><span class="line">点对点</span><br><span class="line">发布订阅</span><br></pre></td></tr></table></figure>

<h5 id="Kafka是什么"><a href="#Kafka是什么" class="headerlink" title="Kafka是什么"></a>Kafka是什么</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Kafka是一个分布式的基于发布/订阅模式的消息队列（MessageQueue），主要应用于大数据实时处理领域</span><br><span class="line">Kafka是 一个开源的 分 布式事件流平台 （Event StreamingPlatform），被数千家公司用于高性能数据管道、流分析、数据集成和关键任务应用</span><br></pre></td></tr></table></figure>

<h5 id="发布订阅的含义"><a href="#发布订阅的含义" class="headerlink" title="发布订阅的含义"></a>发布订阅的含义</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">发布/订阅：消息的发布者不会将消息直接发送给特定的订阅者，而是将发布的消息分为不同的类别，订阅者只接收感兴趣的消息</span><br></pre></td></tr></table></figure>

<h5 id="消息队列产品有哪些"><a href="#消息队列产品有哪些" class="headerlink" title="消息队列产品有哪些"></a>消息队列产品有哪些</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">目前企业中比较常见的消息队列产品主要有 Kafka、ActiveMQ 、RabbitMQ 、RocketMQ 等</span><br><span class="line">在大数据场景主要采用 Kafka 作为消息队列。在 JavaEE 开发中主要采用 ActiveMQ、RabbitMQ、RocketMQ</span><br></pre></td></tr></table></figure>

<h5 id="消息队列的应用场景有哪些"><a href="#消息队列的应用场景有哪些" class="headerlink" title="消息队列的应用场景有哪些"></a>消息队列的应用场景有哪些</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">缓存/消峰、解耦和异步通信。</span><br><span class="line">缓冲/消峰：有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况</span><br><span class="line">解耦：允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束</span><br><span class="line">异步通信：允许用户把一个消息放入队列，但并不立即处理它，然后在需要的时候再去处理它们</span><br></pre></td></tr></table></figure>

<h5 id="消息队列有哪些模式"><a href="#消息队列有哪些模式" class="headerlink" title="消息队列有哪些模式"></a>消息队列有哪些模式</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">点对点模式 消费者主动拉取数据，消息收到后清除消息</span><br><span class="line">发布/订阅模式 可以有多个topic主题（浏览、点赞、收藏、评论等） 消费者消费数据之后，不删除数据 每个消费者相互独立，都可以消费到数据</span><br><span class="line"></span><br><span class="line">主题的数量不同</span><br><span class="line">消费以后是否删除数据</span><br><span class="line">消费者的数量</span><br></pre></td></tr></table></figure>

<h1 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h1><h5 id="Kafka的重要组件有哪些"><a href="#Kafka的重要组件有哪些" class="headerlink" title="Kafka的重要组件有哪些"></a>Kafka的重要组件有哪些</h5><p>broker,partition,消费者组,replication&lt;&#x3D;broker-1,leader副本,follower副本,master broker就是controller,</p>
<p>生产者,消费者,broker三者的网络客户端client,broker额外包含服务端server</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">避免IO热点问题,分布式扩展,所以有多个broker</span><br><span class="line">同时topic也分布式扩展到多个broker上面</span><br><span class="line">所以产生了将一个topic切分成为多个partition</span><br><span class="line">所以生产者可以指定partition消费者也可以指定partition</span><br><span class="line">产生的消费者组,让多个消费者在一个组内,那么组内每个消费者可以只消费指定某个partition即可,整个组构成了完整整体去消费一个topic</span><br><span class="line">但是某个broker宕机,消费这个topic就不完全,所以产生了备份,并且备份放在其他broker上面去</span><br><span class="line">follower备份的数量应该&lt;=broker数量-1,即三个broker,备份2份</span><br><span class="line">这里备份称之为副本replication,</span><br><span class="line">副本只能有一个提供读取操作,其他的是用来备份的</span><br><span class="line">leader副本有读写能力,follower副本只用来备份</span><br><span class="line">为了监控broker的运行情况,选择某一个broker作为master(zk选举机制),也就是controller</span><br><span class="line">为了HA高可用,任何一个节点都可以作为standby备份的controller</span><br><span class="line">生产者,消费者,broker,都包含网络客户端,而broker还有socket server网络服务器,用来接收生产者和消费者的连接请求</span><br><span class="line">broker中还有replication manager,用来管理副本的,还有log manager用来管理日志,还有zk client用来访问zk的,还有事务协调器TransactionCoordinator,用来管理事务</span><br><span class="line"></span><br><span class="line">producer生产者,java语言编写</span><br><span class="line">consumer消费者,java语言编写</span><br><span class="line">topic主题 存储数据的地方</span><br><span class="line">partition分区 一个主题分为多个分区存储,负载均衡,提高并行度</span><br><span class="line">broker 代表服务器节点名称,scala语言编写</span><br><span class="line">消费者组 每一个消费者负责一个分区的数据的消费,共同组成消费者组</span><br><span class="line">	注意一个分区的数据只能由一个消费者进行消费</span><br><span class="line">HA 每个分区增加副本冗余存储,保证安全</span><br><span class="line">	和hdfs副本机制不同,Kafka存在leader和follower,消费者只能消费leader</span><br><span class="line">	当leader挂掉以后,follower可能成为leader</span><br><span class="line">zk 存储Kafka的服务器的存货情况,存储leader的信息</span><br><span class="line">	2.8.0版本以后可以不使用zk</span><br></pre></td></tr></table></figure>

<h5 id="命令行有哪些"><a href="#命令行有哪些" class="headerlink" title="命令行有哪些"></a>命令行有哪些</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">启动:</span><br><span class="line">bin/kafka-server-start.sh -daemon config/server.properties</span><br><span class="line">停止:</span><br><span class="line">bin/kafka-server-stop.sh</span><br><span class="line">创建topic:</span><br><span class="line">bin/kafka-topics.sh --bootstrap-server ip:port --topic first --create --partitions 1 --replication-factor 3</span><br><span class="line">删除topic:</span><br><span class="line">bin/kafka-topics.sh --bootstrap-server ip:port --topic first --delete</span><br><span class="line">查看topic详细信息:</span><br><span class="line">bin/kafka-topics.sh --bootstrap-server ip:port --topic first --describe</span><br><span class="line">修改分区数量:只能增大分区</span><br><span class="line">bin/kafka-topics.sh --bootstrap-server ip:port --topic first --alter --partitions 3</span><br><span class="line">查询所有主题:</span><br><span class="line">bin/kafka-topics.sh --bootstrap-server ip:port --list</span><br><span class="line"></span><br><span class="line">bin/kafka-topics.sh </span><br><span class="line">	--bootstrap-server hadoop101:9092,hadoop102:9092,hadoop103:9092</span><br><span class="line">		测试环境连接一个即可,生产环境都连接</span><br><span class="line">	--topic</span><br><span class="line">	--partitions 指定分区</span><br><span class="line">	--replication-factor 指定副本数</span><br><span class="line">	--create</span><br><span class="line">	--delete</span><br><span class="line">	--alter</span><br><span class="line">	--describe</span><br><span class="line">	--list</span><br><span class="line"></span><br><span class="line">生产者生产数据:</span><br><span class="line">bin/kafka-console-producer.sh --bootstrap-server ip:port --topic first</span><br><span class="line">消费者消费数据:</span><br><span class="line">bin/kafka-console-consumer.sh --bootstrap-server ip:port --topic first</span><br><span class="line">历史数据一次性消费:</span><br><span class="line">bin/kafka-console-consumer.sh --bootstrap-server ip:port --topic first --from-beginning</span><br></pre></td></tr></table></figure>

<h5 id="生产者发送原理是"><a href="#生产者发送原理是" class="headerlink" title="生产者发送原理是"></a>生产者发送原理是</h5><p>main-send-kv的序列化-分区器进行数据分区-缓冲区-sender线程发送数据到broker集群-反向ack校验</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">简单来说:</span><br><span class="line">main线程</span><br><span class="line">send方法发送数据</span><br><span class="line">拦截器列表</span><br><span class="line">key序列化器</span><br><span class="line">value序列化器</span><br><span class="line">分区器</span><br><span class="line">元数据缓存MetadataCache</span><br><span class="line">数据缓冲区RecordAccumulat</span><br><span class="line">Sender发送数据线程</span><br><span class="line">网络客户端,在途请求缓冲区,默认5</span><br><span class="line"></span><br><span class="line">结合源码来说:</span><br><span class="line">生产者KafkaProducer类调用send方法发送数据</span><br><span class="line">数据是封装为ProducerRecord</span><br><span class="line">底层先调用拦截器ProducerInterceptors对象的onSend方法</span><br><span class="line">	onSend方法里面是将拦截器这个列表循环遍历,去执行每一个拦截器ProducerInterceptor接口的onSend方法的具体实现</span><br><span class="line">最终将一个ProducerRecord转换成另一个ProducerRecord</span><br><span class="line">然后再调用doSend发送</span><br><span class="line">该方法里面会使用key序列化器和value序列化器进行序列化,必须传入,否则这里直接抛出异常了</span><br><span class="line">然后执行分区器的方法partition,返回int类型分区编号</span><br><span class="line">	先判断是否在ProducerRecord中设置了分区编号,设置了直接使用,不会做任何正确性判断</span><br><span class="line">	再判断是否有自定义分区器</span><br><span class="line">	再判断key不为空而且不忽略key即参数partitioner.ignore,keys默认false,就根据key进行分区</span><br><span class="line">	再判断key为空,最终会根据分区负载情况粘性分区,即尽可能选择同一个分区,阈值batch.size=16kb</span><br><span class="line">数据校验</span><br><span class="line">	max.request.size=1mb</span><br><span class="line">	buffer.memory=32mb</span><br><span class="line">数据缓冲区,即参数buffer.memory=32mb对应的数据缓冲区</span><br><span class="line">	当batch.size满了或者是开启了一个新的batch,就唤醒了sender线程</span><br><span class="line">以broker的id作为key,以拉取数据的请求request作为value,构建一个拉取数据的队列;这个队列最多默认5个请求</span><br><span class="line"></span><br><span class="line">主线程main</span><br><span class="line">send方法,发送数据</span><br><span class="line">拦截器</span><br><span class="line">序列化器,自己实现的序列化方式</span><br><span class="line">分区器,进行数据分区</span><br><span class="line">	一个分区会创建一个队列</span><br><span class="line">	在内存中完成,队列大小32MB</span><br><span class="line">	传输数据的批次大小16KB</span><br><span class="line">sender线程,从队列读取数据发往kafka集群</span><br><span class="line">	batch.size 满16KB一批次,sender就拉取一次</span><br><span class="line">	linger.ms 默认0毫秒,即不等待16KB批次参数</span><br><span class="line">	以broker的id作为key,以拉取数据的请求request作为value,构建一个拉取数据的队列</span><br><span class="line">	这个队列最多默认5个请求</span><br><span class="line">selector作为链路打通,还要接收kafka集群接收数据的ack应答</span><br><span class="line">	0 生产者发送的数据,不需要等待数据落盘</span><br><span class="line">	1 生产者发送的数据,需要leader收到数据后应答</span><br><span class="line">	-1 生产者发送的数据,需要leader和isr队列所有节点收齐全数据后应答</span><br><span class="line">反馈成功:清理请求队列的请求,清理队列中数据</span><br><span class="line">反馈失败:默认一直重试</span><br></pre></td></tr></table></figure>

<h5 id="分区器源码"><a href="#分区器源码" class="headerlink" title="分区器源码"></a>分区器源码</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">执行分区器的方法partition,返回int类型分区编号</span><br><span class="line">	先判断是否在ProducerRecord中设置了分区编号,设置了直接使用,不会做任何正确性判断</span><br><span class="line">	再判断是否有自定义分区器</span><br><span class="line">	再判断key不为空而且不忽略key即参数partitioner.ignore,keys默认false,就根据key进行分区</span><br><span class="line">	再判断key为空,最终会根据分区负载情况粘性分区,即尽可能选择同一个分区,阈值batch.size=16kb</span><br><span class="line">	</span><br><span class="line">ProducerRecord类,使用不同的构造器</span><br><span class="line">设置分区就发送到对应分区</span><br><span class="line">没设置分区,如果有key,就按照key的哈希code值和分区数取余</span><br><span class="line">如果也没有key,使用粘性分区,指的是随机选择分区后,尽量一直使用这个分区,等batch.size满,再更换分区</span><br><span class="line"></span><br><span class="line">DefaultPartitioner类</span><br><span class="line"></span><br><span class="line">生产中可以把topic名字即表名字设置为key,所有表的数据可以进入相同分区</span><br></pre></td></tr></table></figure>

<h5 id="zk中存储了哪些信息"><a href="#zk中存储了哪些信息" class="headerlink" title="zk中存储了哪些信息"></a>zk中存储了哪些信息</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">有brokers节点,包含了id,topic等</span><br><span class="line">还有cluster节点,包含controller等</span><br><span class="line"></span><br><span class="line">例如</span><br><span class="line">/brokers/ids</span><br><span class="line">/brokers/topics</span><br><span class="line">/controller</span><br><span class="line"></span><br><span class="line">ls / 查看节点信息</span><br><span class="line">ls /kafka 查看kafka信息</span><br><span class="line"></span><br><span class="line">ls /kafka/brokers/ids 存储了节点信息</span><br><span class="line">ls /kafka/brokers/topics/主题名称/partitions/分区编号/state 查看主题分区下的leader和ISR</span><br><span class="line">offset保存在kafka集群中自己的主题中,不再保存在zk,避免网络传输</span><br><span class="line">ls /kafka/controller 注册该节点,选举leader</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="异步发送"><a href="#异步发送" class="headerlink" title="异步发送"></a>异步发送</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">指的是数据发送到ProducerAccumulator中,无需等确认发送到kafka broker 集群就返回</span><br><span class="line">代码中使用send就是异步发送,还能够使用callback回调</span><br></pre></td></tr></table></figure>

<h5 id="选举流程"><a href="#选举流程" class="headerlink" title="选举流程"></a>选举流程</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">简单来说</span><br><span class="line">所有的brokers启动后,去zk抢注临时节点,抢注成功的成为controller节点</span><br><span class="line">其他剩余brokers设置监听</span><br><span class="line">当controller宕机,节点消失,剩余brokers继续抢注</span><br><span class="line"></span><br><span class="line">详细来说</span><br><span class="line">broker创建的时候会做几件事情</span><br><span class="line">1 创建/brokers/ids/xxx</span><br><span class="line">2 监听/controller节点</span><br><span class="line">3 创建/controller节点</span><br><span class="line">4 创建成功就选举成功成为controller,监听/brokers/ids/的变化,来判断其余brokers的增减</span><br><span class="line">创建失败原因(zk无法创建相同节点)</span><br><span class="line">成为controller后</span><br><span class="line">1 controller由于监听了/brokers/ids/的变化,会收到通知brokers发生了变化</span><br><span class="line">2 controller会连接所有的集群broker,传递消息包含集群的topic,controller的id等内容</span><br><span class="line">如果controller发生宕机</span><br><span class="line">1 controller发生宕机时,所有brokers都会收到通知,是因为broker都会监听/controller</span><br><span class="line">2 所有brokers抢注/controller</span><br><span class="line">3 创建成功的成为controller,增加监听/brokers/ids/的变化</span><br><span class="line">4 此时会发生一次集群内容同步,controller会连接所有的集群broker,传递消息包含集群的topic,controller的id等内容</span><br><span class="line"></span><br><span class="line">broker启动以后,在zk注册启动成功,在/kafka/brokers/ids记录信息</span><br><span class="line">broker的Controller类去zk抢注册/kafka/controller节点,成为leader</span><br><span class="line">Controller选出以后,监听/kafka/brokers/ids节点</span><br><span class="line">Controller选举Leader:</span><br><span class="line">	ISR中存活为前提,按照AR中顺序靠前优先</span><br><span class="line">Controller将Leader信息上传zk,即/kafka/brokers/topics/主题名称/partitions/分区编号/state</span><br><span class="line">其他的Controller节点会同步这个节点信息</span><br><span class="line"></span><br><span class="line">数据按照Segment存储,默认1GB,存在.log文件和.index索引文件</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="如何进行节点的服役和退役"><a href="#如何进行节点的服役和退役" class="headerlink" title="如何进行节点的服役和退役"></a>如何进行节点的服役和退役</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">节点服役,直接安装启动即可,但是需要进行主题的负载均衡操作</span><br><span class="line"></span><br><span class="line">退役节点,先进行负载均衡,但是--broker-list指定的是不退役的节点id</span><br></pre></td></tr></table></figure>

<h5 id="副本的知识"><a href="#副本的知识" class="headerlink" title="副本的知识"></a>副本的知识</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">默认1,生产一般配置2</span><br><span class="line">分为Leader和follower,</span><br><span class="line">分区中所有的副本统称为AR</span><br><span class="line">即AR=ISR+OSR</span><br><span class="line">ISR表示和Leader进行同步的follower集合</span><br><span class="line">	参数表示follower超时踢出ISR的时间默认30秒</span><br><span class="line">OSR表示延迟过多的副本follower</span><br></pre></td></tr></table></figure>

<h5 id="Leader的选举规则是"><a href="#Leader的选举规则是" class="headerlink" title="Leader的选举规则是"></a>Leader的选举规则是</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">选举规则:ISR中存活,AR中排序靠前优先,按照AR中顺序轮询</span><br></pre></td></tr></table></figure>

<h5 id="Follower挂掉后的处理流程"><a href="#Follower挂掉后的处理流程" class="headerlink" title="Follower挂掉后的处理流程?"></a>Follower挂掉后的处理流程?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Leader和Follower的数据没有对齐,是因为Leader先获取数据,所以数据量大于Follower</span><br><span class="line">LEO,指的是每个副本的最后的一个offset,实际上是最新的offset+1</span><br><span class="line">HW,指的是所有的副本中,最小的那个LEO</span><br><span class="line">消费者能看到的最大的offset,是HW减去1的那个数据</span><br><span class="line"></span><br><span class="line">挂掉后:</span><br><span class="line">踢出ISR</span><br><span class="line">期间正常接收数据</span><br><span class="line">Follower恢复后,读取本地磁盘上一次的HW,将log中高于HW的数据截取,从HW开始同步Leader的数据,然后等到该Follower的LEO大于等于该分区的HW时,就可以重新加入ISR</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="Leader挂掉后的处理流程"><a href="#Leader挂掉后的处理流程" class="headerlink" title="Leader挂掉后的处理流程"></a>Leader挂掉后的处理流程</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">从ISR中选举一个新的Leader</span><br><span class="line">其余Follower将高出Leader的数据截取掉,所以能保证数据一致性,但是不能保证数据不丢失或者不重复</span><br></pre></td></tr></table></figure>

<h5 id="分区副本如何进行分配"><a href="#分区副本如何进行分配" class="headerlink" title="分区副本如何进行分配"></a>分区副本如何进行分配</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">分区数量大于broker节点数量:尽量均匀的方式进行分配</span><br><span class="line"></span><br><span class="line">0123,1230,2301这样分配</span><br><span class="line"></span><br><span class="line">可以手动的调整</span><br></pre></td></tr></table></figure>

<h5 id="文件存储机制"><a href="#文件存储机制" class="headerlink" title="文件存储机制"></a>文件存储机制</h5><p>底层存储按照topic+分区号设置文件目录,包含index,log等</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">broker-&gt;topic-&gt;partition-&gt;log.dirs-&gt;segment</span><br><span class="line"></span><br><span class="line">Partition是一个物理上的概念</span><br><span class="line">	一个分区对应一个log文件</span><br><span class="line">	生产的数据会不断追加到log文件末尾</span><br><span class="line">	每个Partition分成多个segment文件存储,每个segment包含log文件,index文件,timeindex文件等</span><br><span class="line">	以当前segment第一条消息的offset命名文件名</span><br><span class="line">topic是一个逻辑上的概念,一个topic分成多个分区</span><br><span class="line">	文件夹命名是主题名称+分区编号</span><br><span class="line"></span><br><span class="line">所以真正磁盘上这样存储的:</span><br><span class="line">	first-0</span><br><span class="line">		xxx.index</span><br><span class="line">		xxx.log</span><br><span class="line">		xxx.timeindex</span><br><span class="line"></span><br><span class="line">使用系统工具kafka-run-class.sh kafka.tools.DumpLogSegments --files xxx.index查看</span><br><span class="line"></span><br><span class="line">index文件:</span><br><span class="line">index是稀疏索引,每4KB约数据,写入一条索引,参数log.index.interval.bytes</span><br><span class="line">格式是:</span><br><span class="line">相对offset,position位置</span><br><span class="line">注意position位置指的是log文件中的position位置</span><br><span class="line"></span><br><span class="line">log文件:</span><br><span class="line">格式是 :</span><br><span class="line">baseoffset,lastoffset,position等等内容</span><br><span class="line"></span><br><span class="line">默认7天会删除主题topic数据,可以设置</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><h5 id="生产者如何提高吞吐量"><a href="#生产者如何提高吞吐量" class="headerlink" title="生产者如何提高吞吐量?"></a>生产者如何提高吞吐量?</h5><p>sender线程发送数据到broker集群时,涉及到缓冲区大小buffer,数据可以设置压缩,linger.ms等待时间,默认不等待batch.size时间,batch,size增大</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">四个参数</span><br><span class="line">增加linger.ms等待时间,5-100毫秒</span><br><span class="line">batch.size可以修改16KB到32KB</span><br><span class="line">数据压缩,compression.type例如使用snappy</span><br><span class="line">缓冲区大小默认32MB可以修改为64MB</span><br><span class="line"></span><br><span class="line">properties.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 1024 * 1024 * 64);</span><br><span class="line">properties.put(ProducerConfig.BATCH_SIZE_CONFIG, 1024 * 32);</span><br><span class="line">properties.put(ProducerConfig.LINGER_MS_CONFIG, 5);</span><br><span class="line">properties.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, &quot;snappy&quot;);</span><br></pre></td></tr></table></figure>

<h5 id="生产者如何保证数据不丢不重"><a href="#生产者如何保证数据不丢不重" class="headerlink" title="生产者如何保证数据不丢不重?"></a>生产者如何保证数据不丢不重?</h5><p>如何保证数据不丢失?</p>
<p>ack设置-1,分区partition的副本replication数量&gt;&#x3D;2,并且isr队列中最小副本数也要&gt;&#x3D;2</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">replica.lag.time.max.ms参数默认30秒,超时会将follower踢出isr队列</span><br><span class="line">	这个参数的意思是,在acks=-1时,如果某个follower迟迟没给响应,那么延迟性会很高,此时我们设置这个参数,把这个follower剔除isr队列,不再校验这个存储的情况,所以isr中必须得保证最小的副本数&gt;=2才能保证数据不丢失,即最少得有一个follower在isr中,</span><br><span class="line"></span><br><span class="line">数据可靠性=</span><br><span class="line">    ack应答设置-1 +</span><br><span class="line">	分区副本数&gt;=2 + 即最少一个follower</span><br><span class="line">	isr中最小副本数&gt;=2 即最少一个follower,min.insync.replicas参数默认1,min.insync.replicas怎么设置的?</span><br><span class="line">	一般还把重试次数设置小一点</span><br><span class="line"></span><br><span class="line">properties.put(ProducerConfig.ACKS_CONFIG,&quot;all&quot;);</span><br><span class="line">properties.put(ProducerConfig.RETRIES_CONFIG, 3);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>如何保证数据不重复?</p>
<p>数据不丢失的保证下,开启幂等性,然后设置事务id</p>
<p>能保证单分区数据有序,并且不重复</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">ack=-1时,可能产生数据重复问题,在ack应答时leader挂掉,所以会重复接受数据,但是概率低,但是有概率</span><br><span class="line"></span><br><span class="line">保证精确一次,或者说保证数据不重复</span><br><span class="line">幂等性+上面的数据可靠性</span><br><span class="line">或者事务+上面的数据可靠性</span><br><span class="line"></span><br><span class="line">开启幂等性:enable.idempotence开启true,默认true</span><br><span class="line">幂等性还能保证单分区内有序</span><br><span class="line"></span><br><span class="line">使用事务:先开启幂等性,再操作代码</span><br><span class="line"></span><br><span class="line">出现重复的情况是数据已经保存,但是ack响应的时候没成功响应导致</span><br><span class="line">乱序问题是因为一次性可以发送5个batch数据但是某个发送失败后会重试所以排到后面去了</span><br><span class="line">但是注意如果生产者重启,那么分区编号的那个值会改变,这样还是可能造成重复数据</span><br><span class="line">这个时候再使用事务保证,添加配置transactional.id设置事务id</span><br><span class="line"></span><br><span class="line">properties.put(ProducerConfig.ACKS_CONFIG, &quot;all&quot;);</span><br><span class="line">properties.put(ProducerConfig.RETRIES_CONFIG, 3);</span><br><span class="line">properties.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);</span><br><span class="line">properties.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, &quot;trans1&quot;);</span><br></pre></td></tr></table></figure>

<h5 id="幂等性的判断条件是"><a href="#幂等性的判断条件是" class="headerlink" title="幂等性的判断条件是?"></a>幂等性的判断条件是?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;PID,partition,seqnumber&gt;</span><br><span class="line">PID是kafka集群每次重启都会分配一个新的,所以不能保证多会话不重复</span><br><span class="line">partition表示分区号,所以只能保证单分区内不重复</span><br><span class="line">seqnumber是单调增序列</span><br><span class="line">给每个数据增加一个唯一性标识,即seqnumber,还有一个分区编号标识</span><br></pre></td></tr></table></figure>

<h5 id="事务的原理"><a href="#事务的原理" class="headerlink" title="事务的原理?"></a>事务的原理?</h5><p>通过事务协调器实现,</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">每一个broker节点还包含了:</span><br><span class="line">事务协调器</span><br><span class="line">存储事务信息的特殊主题topic</span><br><span class="line"></span><br><span class="line">多个broker有多个事务协调器,那么具体使用哪一个呢?</span><br><span class="line">根据事务id(手动指定,全局唯一值)的哈希code和50取余,计算该事务属于哪个broker和事务协调器</span><br><span class="line"></span><br><span class="line">1生产者请求pid</span><br><span class="line">2事务协调器返回pid</span><br><span class="line">3发数据</span><br><span class="line">4发数据以后,提交commit到事务协调器</span><br><span class="line">5这个请求消息,持久化到存储事务信息的特殊主题topic中,方便回滚之类</span><br><span class="line">6返回持久化成功</span><br><span class="line">7事务协调器向分区leader发送commit提交请求</span><br><span class="line">8返回数据已经处理成功消息</span><br><span class="line">9事务成功的信息持久化到存储事务信息的特殊主题topic中</span><br></pre></td></tr></table></figure>

<h5 id="如何保证数据单分区内有序"><a href="#如何保证数据单分区内有序" class="headerlink" title="如何保证数据单分区内有序?"></a>如何保证数据单分区内有序?</h5><p>开启幂等性,然后设置发送数据的队列个数&lt;&#x3D;5</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">假设kafka集群有两个broker节点</span><br><span class="line">那么生产者端,可以每个broker缓存5个请求,即sender线程发送数据到集群的请求队列(见上文)</span><br><span class="line"></span><br><span class="line">1版本保证数据单分区有序:设置max.in.flight.requests.per.connection=1</span><br><span class="line">1版本以后保证数据单分区有序:未开启幂等性,设置参数为1,</span><br><span class="line">	开启幂等性:设置小于等于5,因为最多缓存五个,会在内存进行重新排序</span><br><span class="line">乱序问题是因为一次性可以发送5个batch数据但是某个发送失败后会重试所以排到后面去了</span><br><span class="line"></span><br><span class="line">properties.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);</span><br><span class="line">properties.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 5);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="如何对主题进行负载均衡"><a href="#如何对主题进行负载均衡" class="headerlink" title="如何对主题进行负载均衡?"></a>如何对主题进行负载均衡?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">创建json文件</span><br><span class="line">&#123;</span><br><span class="line">	&quot;topics&quot;:[</span><br><span class="line">		&#123;&quot;topic&quot;:&quot;first&quot;&#125;,xxx</span><br><span class="line">	],</span><br><span class="line">	&quot;version&quot;:1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">进行负载均衡计划</span><br><span class="line">kafka-reassign-partitions.sh --bootstrap-server xxx --topics-to-move-json-file xxx --broker-list &quot;0,1,2,3&quot; --generate</span><br><span class="line"></span><br><span class="line">创建副本存储计划的json</span><br><span class="line">将刚才新的执行计划的内容复制</span><br><span class="line"></span><br><span class="line">执行计划</span><br><span class="line">kafka-reassign-partitions.sh --bootstrap-server xxx --reassignment-json-file xxx --execute</span><br><span class="line"></span><br><span class="line">验证是否成功</span><br><span class="line">kafka-reassign-partitions.sh --bootstrap-server xxx --reassignment-json-file xxx --verify</span><br></pre></td></tr></table></figure>

<h5 id="如何手动调整分区副本存储的策略"><a href="#如何手动调整分区副本存储的策略" class="headerlink" title="如何手动调整分区副本存储的策略?"></a>如何手动调整分区副本存储的策略?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">例如将所有副本都存储在broker0和1节点:</span><br><span class="line">创建json</span><br><span class="line">&#123;</span><br><span class="line">	&quot;version&quot;:1,</span><br><span class="line">	&quot;partition&quot;:[</span><br><span class="line">		&#123;&quot;topic&quot;:主题名称a,&quot;partition&quot;:分区号0,&quot;replicas&quot;:[0,1]&#125;,</span><br><span class="line">        &#123;&quot;topic&quot;:主题名称a,&quot;partition&quot;:分区号1,&quot;replicas&quot;:[0,1]&#125;,</span><br><span class="line">        &#123;&quot;topic&quot;:主题名称a,&quot;partition&quot;:分区号2,&quot;replicas&quot;:[1,0]&#125;,</span><br><span class="line">        &#123;&quot;topic&quot;:主题名称a,&quot;partition&quot;:分区号3,&quot;replicas&quot;:[1,0]&#125;</span><br><span class="line">	]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">执行计划</span><br><span class="line">执行计划</span><br><span class="line">kafka-reassign-partitions.sh --bootstrap-server xxx --reassignment-json-file xxx --execute</span><br><span class="line"></span><br><span class="line">验证是否成功</span><br><span class="line">kafka-reassign-partitions.sh --bootstrap-server xxx --reassignment-json-file xxx --verify</span><br></pre></td></tr></table></figure>

<h5 id="Leader-Partition自动平衡"><a href="#Leader-Partition自动平衡" class="headerlink" title="Leader Partition自动平衡"></a>Leader Partition自动平衡</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">本身会进行自动平衡,但是broker节点宕机以后,可能会导致不平衡</span><br><span class="line">使用自动再平衡机制参数,auto.leader.rebalance.enable默认true</span><br><span class="line">比例阈值参数,leader.imbalance,per.broker.percentage默认10%</span><br><span class="line">检查时间间隔参数leader.imbalance.check.interval.seconds默认300秒</span><br><span class="line"></span><br><span class="line">实际生产上,不建议开启自动再平衡</span><br></pre></td></tr></table></figure>

<h5 id="如何增加副本数量"><a href="#如何增加副本数量" class="headerlink" title="如何增加副本数量?"></a>如何增加副本数量?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">即需要后续增加副本数量</span><br><span class="line">不能通过命令行方式修改副本数</span><br><span class="line">创建json文件</span><br><span class="line">&#123;</span><br><span class="line">	&quot;version&quot;:1,</span><br><span class="line">	&quot;partitions&quot;:[</span><br><span class="line">		&#123;&quot;topic&quot;:主题名称a,&quot;partition&quot;:分区号0,&quot;replicas&quot;:[0,1,2]&#125;,</span><br><span class="line">        &#123;&quot;topic&quot;:主题名称a,&quot;partition&quot;:分区号1,&quot;replicas&quot;:[0,1,2]&#125;,</span><br><span class="line">        &#123;&quot;topic&quot;:主题名称a,&quot;partition&quot;:分区号2,&quot;replicas&quot;:[0,1,2]&#125;</span><br><span class="line">	]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">执行计划</span><br><span class="line">kafka-reassign-partitions.sh --bootstrap-server xxx --reassignment-json-file xxx --execute</span><br><span class="line"></span><br><span class="line">验证是否成功</span><br><span class="line">kafka-reassign-partitions.sh --bootstrap-server xxx --reassignment-json-file xxx --verify</span><br></pre></td></tr></table></figure>

<h5 id="文件清除策略"><a href="#文件清除策略" class="headerlink" title="文件清除策略"></a>文件清除策略</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">log.retention.hours默认7天清除数据</span><br><span class="line">log.retention.minutes默认没设置,设置了就使用该参数,分钟级别</span><br><span class="line">log.retention.ms默认没设置,设置了就使用该参数,毫秒级别</span><br><span class="line">检测是否到达时间的参数log.retention.check.intervel.ms,默认5分钟</span><br><span class="line"></span><br><span class="line">建议3天清除,也有保存7小时的</span><br><span class="line"></span><br><span class="line">清除的策略:</span><br><span class="line">删除delete</span><br><span class="line">	log.cleanup.policy默认使用该策略,delete</span><br><span class="line">		按照时间判断,默认方式,即按照segment中所有数据的最大时间戳作为该segment时间戳来判断是否该删除,这样即使一个segment中有一部分没有超时,也不会有问题,因为是按照最大时间戳,也就是最后一条数据的时间戳来判断的</span><br><span class="line">		按照大小判断,参数log.retention.bytes默认-1代表无穷大,代表关闭,一般不会打开的,</span><br><span class="line">压缩compact</span><br><span class="line">	log.cleanup.policy为compact</span><br><span class="line">	针对kv类型数据可以使用,即对于相同key的不同value的数据,只保留最后一个版本</span><br><span class="line">	使用较少</span><br></pre></td></tr></table></figure>

<h5 id="kafka如何高效读写数据"><a href="#kafka如何高效读写数据" class="headerlink" title="kafka如何高效读写数据?"></a>kafka如何高效读写数据?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">分布式,使用分区方式并行读写</span><br><span class="line">读取数据使用稀疏索引,快速定位数据</span><br><span class="line">顺序写入磁盘,追加数据到log末尾</span><br><span class="line">使用页缓存+零拷贝技术</span><br><span class="line">	页缓存,使用底层操作系统的页缓存,就相当于内存缓存加速的</span><br><span class="line">	零拷贝,指的是数据处理不经过集群,不走应用层,直接走网卡传输数据,效率高</span><br></pre></td></tr></table></figure>

<h1 id="消费"><a href="#消费" class="headerlink" title="消费"></a>消费</h1><h5 id="kafka的消费方式是"><a href="#kafka的消费方式是" class="headerlink" title="kafka的消费方式是?"></a>kafka的消费方式是?</h5><p>主动拉取</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">拉取pull</span><br><span class="line">推送push</span><br><span class="line">kafka使用主动拉取的方式进行消费</span><br></pre></td></tr></table></figure>

<h5 id="消费者的工作流程"><a href="#消费者的工作流程" class="headerlink" title="消费者的工作流程"></a>消费者的工作流程</h5><p>创建客户端进行网络交互-主动拉取-存入queue队列-kv反序列化-处理数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">一个消费者可以消费多个分区数据</span><br><span class="line">多个消费者可以消费同一个分区,因为消费者之间是相互隔离的</span><br><span class="line">但是存在消费者组的概念,一个分区只能由一个消费者组的一个消费者进行消费</span><br><span class="line">offset存储在__consumer_offsets中</span><br><span class="line"></span><br><span class="line">前置是消费者组和coordinator</span><br><span class="line"></span><br><span class="line">消费者创建consumer network client</span><br><span class="line">消费者发送消费请求,send fetches</span><br><span class="line">	相关参数:fetch.min.bytes每个批次最小拉取大小,默认1字节</span><br><span class="line">	fetch.max.wait.ms一批数据未达到最小拉取值的超时时间默认500毫秒</span><br><span class="line">	fetch.max.bytes最大每批次抓取大小,默认50MB</span><br><span class="line">拉取来的数据,存入消息队列queue,</span><br><span class="line">具体消费者,从该queue队列获取数据,max.poll.records最大条数,默认500</span><br><span class="line">	先进行反序列化,</span><br><span class="line">	经过拦截器,</span><br><span class="line">	处理数据</span><br></pre></td></tr></table></figure>

<h5 id="消费者组的概念"><a href="#消费者组的概念" class="headerlink" title="消费者组的概念?"></a>消费者组的概念?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">由多个消费者组成,所有消费者的groupid相同则属于同一组</span><br><span class="line">组内消费者消费不同分区数据</span><br><span class="line">消费者组互相之间隔离</span><br><span class="line"></span><br><span class="line">如何消费者组内的消费者数量大于分区数量?会存在闲置的消费者</span><br><span class="line"></span><br><span class="line">coordinator,每个broker节点都有这个组件,消费者组的groupid的哈希code值和50取余,因为__consumer_offsets的分区数量是50</span><br><span class="line">就得到了这个消费者组所对应的broker节点</span><br><span class="line">组内的所有消费者,发送join group请求,到coordinator</span><br><span class="line">coordinator随机选择一个消费者作为leader,把需要消费的topic都发给这个leader</span><br><span class="line">然后这个leader,或者说consumer消费者,会制定一个消费的计划,谁来消费哪个分区的计划</span><br><span class="line">把这个计划发送给coordinator,然后分发给各个consumer</span><br><span class="line">这个coordinator和所有消费者之间会保持一个心跳默认3s,一旦超时,session.timeout.ms=45s,会移除该消费者,并进行重平衡,或者消费者处理时间超时,max.poll.interval.ms=5分钟,也会同样移除,重平衡</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="消费者和分区对应关系"><a href="#消费者和分区对应关系" class="headerlink" title="消费者和分区对应关系?"></a>消费者和分区对应关系?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">即消费者组内的消费者,具体应该消费哪个分区的数据呢?</span><br><span class="line">参数partition.assignment.strategy</span><br><span class="line">默认策略是Range+CooperativeSticky</span><br></pre></td></tr></table></figure>

<h5 id="Range分配策略"><a href="#Range分配策略" class="headerlink" title="Range分配策略"></a>Range分配策略</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">同一个topic的分区,按照分区编号排序,并且对消费者按照字母顺序排序</span><br><span class="line">分区数量/消费者数量,决定每一个消费者应该消费几个分区的数据,除不尽的按顺序多分配消费者上,然后每个消费者消费的分区数量知道以后,再进行分区的分配</span><br><span class="line"></span><br><span class="line">如果topic很多,很容易产生数据倾斜问题</span><br><span class="line"></span><br><span class="line">如果某个消费者挂掉,会整体分配给别人,</span><br><span class="line"></span><br><span class="line">再平衡:会把宕机数据分配其他节点,全部给其中一个消费者</span><br></pre></td></tr></table></figure>

<h5 id="RoundRobin分配策略"><a href="#RoundRobin分配策略" class="headerlink" title="RoundRobin分配策略"></a>RoundRobin分配策略</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">针对所有的topic来说</span><br><span class="line">使用轮询的方式,把所有主题的所有的分区放在一起,把所有的消费者放在一起,然后按照哈希code排序,然后轮询分</span><br><span class="line"></span><br><span class="line">再平衡:轮询分给其他所有节点</span><br></pre></td></tr></table></figure>

<h5 id="Sticky分配策略"><a href="#Sticky分配策略" class="headerlink" title="Sticky分配策略"></a>Sticky分配策略</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">和Range区别在于,是随机的,所以每一次分配的情况都不同</span><br><span class="line"></span><br><span class="line">再平衡:打散给到其余所有消费者</span><br></pre></td></tr></table></figure>

<h5 id="offset维护位置"><a href="#offset维护位置" class="headerlink" title="offset维护位置"></a>offset维护位置</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">维护在__consumer_offsets主题中,0.9版本以前存储在zk中</span><br><span class="line">使用kv格式存储数据,key是消费者组id+topic+分区编号,value是当前offset的值</span><br><span class="line">会间隔一段时间进行compact压缩,即按照key进行压缩保留最新的</span><br><span class="line"></span><br><span class="line">如何查看?</span><br><span class="line">consumer.properties文件</span><br><span class="line">exclude.internal.topics默认false,设置为true才能查看系统主题</span><br><span class="line">不需要重启服务</span><br><span class="line"></span><br><span class="line">命令行查看:</span><br><span class="line">kafka-console-consumer.sh --topic __consumer_offsets --bootstrap-server hadoop101:9092 --consumer.config config/consumer.properties --formatter &quot;kafka.coordinator.group.GroupMetadataManager\$OffsetsMessageFormatter&quot; --from-beginning</span><br></pre></td></tr></table></figure>

<h5 id="提交offset功能"><a href="#提交offset功能" class="headerlink" title="提交offset功能"></a>提交offset功能</h5><p>生产中使用异步提交,手动设置提交</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">自动提交</span><br><span class="line">希望使用者专注于业务实现</span><br><span class="line">enable.auto.commit是否自动提交offset,默认true</span><br><span class="line">auto.commit.interval.ms自动提交的时间间隔,默认5s</span><br><span class="line"></span><br><span class="line">手动提交</span><br><span class="line">分为同步提交和异步提交</span><br><span class="line">同步提交kafkaConsumer.commitSync();</span><br><span class="line">异步提交kafkaConsumer.commitAsync();</span><br><span class="line"></span><br><span class="line">实际生产中,使用异步提交方式</span><br></pre></td></tr></table></figure>

<h5 id="指定offset消费的模式"><a href="#指定offset消费的模式" class="headerlink" title="指定offset消费的模式"></a>指定offset消费的模式</h5><p>lastest最新,earliest最早</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">参数是auto.offset.reset</span><br><span class="line">默认latest,从最新的偏移量消费</span><br><span class="line">earliest相当于--from-beginning,从头消费</span><br><span class="line">none未找到消费者组的先前偏移量,则抛出异常</span><br><span class="line"></span><br><span class="line">还能从指定offset进行消费</span><br><span class="line">需要指定哪个分区从哪个offset进行消费,所以要先获取分区的信息</span><br><span class="line">可能没能消费到数据,为什么?因为从订阅主题到获取分区分配的信息可能延迟了,导致获取的分区信息为空</span><br><span class="line">所以还需要进行一个为空判断</span><br><span class="line">每次消费完,需要更改组id</span><br><span class="line"></span><br><span class="line">指定时间段进行重新消费</span><br><span class="line">即把时间段转换成offset,然后还是使用指定offset的方式消费</span><br></pre></td></tr></table></figure>

<h5 id="offset如何使用"><a href="#offset如何使用" class="headerlink" title="offset如何使用"></a>offset如何使用</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">auto.offset.reset=earliest/latest/none</span><br><span class="line">默认是latest即从最新数据开始消费</span><br><span class="line">earliest即命令行的--from-begining从头开始消费</span><br><span class="line"></span><br><span class="line">properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,&quot;latest&quot;);</span><br></pre></td></tr></table></figure>

<h5 id="消费者如何保证不漏不重"><a href="#消费者如何保证不漏不重" class="headerlink" title="消费者如何保证不漏不重?"></a>消费者如何保证不漏不重?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">重复消费:自动提交offset引起,自动提交offset间隔中,消费者挂掉,重启consumer后从上一次offset消费,导致重复消费</span><br><span class="line">漏消费:设置offset手动提交引起,数据还没有落盘时,消费者挂掉,那么offset已经提交但是数据没有处理,</span><br><span class="line"></span><br><span class="line">解决方式:使用事务,即将消费过程和提交offset绑定事务,下游系统必须支持事务(例如下游还是kafka或者rdbms)</span><br></pre></td></tr></table></figure>

<h5 id="消费者如何提高吞吐量"><a href="#消费者如何提高吞吐量" class="headerlink" title="消费者如何提高吞吐量"></a>消费者如何提高吞吐量</h5><p>增加分区,同时增加消费者数据量,二者相等</p>
<p>每批次拉取数据量的条数和大小,增加</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">即如何提高消费者的吞吐量</span><br><span class="line">如果是kafka消费能力不足,可以增加分区,同时增加消费者数量,让消费者数量=分区数量</span><br><span class="line">	增加linger.ms等待时间,5-100毫秒</span><br><span class="line">	batch.size可以修改16KB到32KB</span><br><span class="line">	数据压缩,compression.type例如使用snappy</span><br><span class="line">	缓冲区大小默认32MB可以修改为64MB</span><br><span class="line">如果是下游处理不足,提高每批次拉取数据量,以及上限值,50MB和500条</span><br><span class="line">	max.poll.records最大条数,默认500</span><br><span class="line">	fetch.max.bytes最大每批次抓取大小,默认50MB</span><br><span class="line">	</span><br><span class="line">properties.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 1000);</span><br><span class="line">properties.put(ConsumerConfig.FETCH_MAX_BYTES_CONFIG, 1024 * 1024 * 100);</span><br></pre></td></tr></table></figure>

<h1 id="Kraft模式"><a href="#Kraft模式" class="headerlink" title="Kraft模式"></a>Kraft模式</h1><h5 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">不依赖zk,设置三台controller进行集群管理</span><br><span class="line">元数据存储在controller中</span><br></pre></td></tr></table></figure>

<h1 id="调优"><a href="#调优" class="headerlink" title="调优"></a>调优</h1><h5 id="硬件配置"><a href="#硬件配置" class="headerlink" title="硬件配置"></a>硬件配置</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">假设场景</span><br><span class="line">每天数据条数:100万日活*100条=1亿条(中型公司)</span><br><span class="line">每秒速度:1亿/(24*3600)=1150条/秒</span><br><span class="line">1条数据:0.5k-2k,取值1k</span><br><span class="line">所以每秒平均数据量:1150*1=1MB/秒</span><br><span class="line">每秒峰值数据量:假设20倍,所以20MB/秒</span><br><span class="line"></span><br><span class="line">评估服务器数量:</span><br><span class="line">数量=2*(生产者峰值*副本数/100)+1</span><br><span class="line">=2*(20*2/100)+1</span><br><span class="line">除不尽往上进位</span><br><span class="line">=3台</span><br><span class="line"></span><br><span class="line">磁盘选择:</span><br><span class="line">按照顺序读写,所以机械硬盘即可</span><br><span class="line">1亿*1k=100GB/天</span><br><span class="line">100GB*2副本*保存3天/0.7(预留0.3)=1TB</span><br><span class="line">所以三台总计1TB即可</span><br><span class="line"></span><br><span class="line">内存选择:</span><br><span class="line">内存=kafka堆内存+页缓存</span><br><span class="line">堆内存=10-15GB即可,修改start脚本</span><br><span class="line">页缓存=一般将segment的1GB的25%放置页缓存即可*分区总数中leader总数量(假设3台服务器对应3分区)/集群broker台数=256MB</span><br><span class="line">所以一台服务器=10GB+256MB即可</span><br><span class="line"></span><br><span class="line">如何查看内存使用情况:</span><br><span class="line">jstat -gc 进程号 ls 10</span><br><span class="line">主要查看YGC次数</span><br><span class="line">jmap -heap 进程号</span><br><span class="line"></span><br><span class="line">CPU核数</span><br><span class="line">num.io.threads=8写磁盘的线程数,需要占比总C的50%</span><br><span class="line">num.replica.fetchers=1副本拉取线程,占比总C的50%的三分之一</span><br><span class="line">num.network.threads=3数据传输线程数,占比总C的50%的三分之二</span><br><span class="line">建议32C,分成24+8</span><br><span class="line">参数分别是12,4,8</span><br><span class="line">剩余8留给系统本身</span><br><span class="line"></span><br><span class="line">网络</span><br><span class="line">网络带宽=峰值吞吐量=20MB/s</span><br><span class="line">千兆网卡,1000Mbps,即1000/8=125MB/s</span><br></pre></td></tr></table></figure>

<h5 id="生产者配置"><a href="#生产者配置" class="headerlink" title="生产者配置"></a>生产者配置</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">提高吞吐量:</span><br><span class="line">	32MB-&gt;64MB</span><br><span class="line">	16KB-&gt;32KB</span><br><span class="line">	0-&gt;5到100ms</span><br><span class="line">	压缩snappy</span><br><span class="line"></span><br><span class="line">数据可靠:</span><br><span class="line">	ack=-1</span><br><span class="line">	副本&gt;=2 --replicate-factor=3 </span><br><span class="line">	ISR&gt;=2(默认1) min.insync.replicas=2</span><br><span class="line"></span><br><span class="line">数据去重:</span><br><span class="line">	幂等性开启</span><br><span class="line">	代码使用事务</span><br><span class="line"></span><br><span class="line">数据乱序解决:</span><br><span class="line">	幂等性开启</span><br><span class="line">	ack次数默认5</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="broker配置"><a href="#broker配置" class="headerlink" title="broker配置"></a>broker配置</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">自动再平衡</span><br><span class="line">	建议关闭</span><br><span class="line">	auto.leader.rebalance.enable 建议false</span><br><span class="line"></span><br><span class="line">数据存储时间:</span><br><span class="line">	建议7天-&gt;3天</span><br><span class="line"></span><br><span class="line">线程三个参数设置:</span><br><span class="line">	查看上面的硬件配置</span><br><span class="line"></span><br><span class="line">自动创建主题:</span><br><span class="line">	建议关闭</span><br><span class="line">	auto.create.topics.enable=false</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="消费者配置"><a href="#消费者配置" class="headerlink" title="消费者配置"></a>消费者配置</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">消费者事务:</span><br><span class="line">	使用事务</span><br><span class="line">消费者提高吞吐量:</span><br><span class="line">	增加分区数量--partitions</span><br><span class="line">	max.poll.records一次性拉取数据条数500增加</span><br><span class="line">	fetch.max.bytes默认50MB增加</span><br><span class="line">    </span><br></pre></td></tr></table></figure>

<h5 id="精确一次配置"><a href="#精确一次配置" class="headerlink" title="精确一次配置"></a>精确一次配置</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">ack=-1</span><br><span class="line">分区&gt;=2,建议3,--replication-factor=3</span><br><span class="line">ISR&gt;=2,建议2,min.insync.replicas=2</span><br><span class="line"></span><br><span class="line">幂等性</span><br><span class="line">事务代码</span><br><span class="line">手动提交offset,enable.auto.commit=false,异步提交</span><br><span class="line">输出目的地支持事务</span><br><span class="line"></span><br><span class="line">手动提交offset+消费过程绑定事务,详见后续</span><br></pre></td></tr></table></figure>

<h5 id="吞吐量配置"><a href="#吞吐量配置" class="headerlink" title="吞吐量配置"></a>吞吐量配置</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">32MB-&gt;64MB</span><br><span class="line">16KB-&gt;32KB</span><br><span class="line">0-&gt;5到100ms</span><br><span class="line">压缩snappy</span><br><span class="line"></span><br><span class="line">增加分区数量--partitions</span><br><span class="line">max.poll.records一次性拉取数据条数500增加</span><br><span class="line">fetch.max.bytes默认50MB增加</span><br></pre></td></tr></table></figure>

<h5 id="分区数规划"><a href="#分区数规划" class="headerlink" title="分区数规划"></a>分区数规划</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">假设想要的吞吐量是100MB/s</span><br><span class="line">上面说生产者20MB/s吞吐</span><br><span class="line">那么分区数量设置100/20=5</span><br><span class="line"></span><br><span class="line">做压测</span><br><span class="line"></span><br><span class="line">一般3-10分区数</span><br><span class="line"></span><br><span class="line">注意分区数量可以大于broker数量</span><br></pre></td></tr></table></figure>

<h5 id="单条日志大于1MB"><a href="#单条日志大于1MB" class="headerlink" title="单条日志大于1MB"></a>单条日志大于1MB</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">该问题的解决:</span><br><span class="line">message.max.bytes默认1MB,调大,针对broker</span><br><span class="line">max.request.size,默认1MB,调大,针对topic</span><br><span class="line">replica.fetch.max.bytes,默认1MB,调大</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="服务器宕机"><a href="#服务器宕机" class="headerlink" title="服务器宕机"></a>服务器宕机</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">尝试重启</span><br><span class="line">增加资源</span><br><span class="line">服役新节点并负载均衡</span><br></pre></td></tr></table></figure>

<h5 id="压测"><a href="#压测" class="headerlink" title="压测"></a>压测</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">生产者压测,kafka-producer-perf-test.sh</span><br><span class="line">	创建压测主题</span><br><span class="line">	kafka-producer-perf-test.sh --topic test --record-size 1024 --num-records 1000000 --throughput 10000 --producer-props.bootstrap.servers=xxx batch.size=xxx linger.ms=xxx</span><br><span class="line">	record-size 一条数据的大小,单位字节</span><br><span class="line">	num-records 总共测试多少条数据</span><br><span class="line">	--throughput 每秒10000条这里</span><br><span class="line"></span><br><span class="line">消费者压测,kafka-consumer-perf-test.sh</span><br><span class="line">    kafka-consumer-perf-test.sh --bootstrap-server xxx --topic xxx --message 1000000 --consumer.config xxx</span><br><span class="line">    --message总共需要消费的数据条数</span><br><span class="line">    max.poll.records一次性拉取数据条数500增加</span><br><span class="line">	fetch.max.bytes默认50MB增加</span><br><span class="line">	压测上面两个参数</span><br><span class="line">	</span><br></pre></td></tr></table></figure>

<h1 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h1><h5 id="producer源码"><a href="#producer源码" class="headerlink" title="producer源码"></a>producer源码</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">整体流程:</span><br><span class="line"></span><br><span class="line">A 创建生产者对象过程,创建了生产者对象,并开启了sender线程</span><br><span class="line"></span><br><span class="line">B 发送数据过程:</span><br><span class="line">1 main中的send方法</span><br><span class="line">	生产者KafkaProducer类调用send方法发送数据</span><br><span class="line">	数据是封装为ProducerRecord</span><br><span class="line">2 Interceptor拦截器</span><br><span class="line">	底层先调用拦截器ProducerInterceptors对象的onSend方法</span><br><span class="line">		onSend方法里面是将拦截器这个列表循环遍历,去执行每一个拦截器ProducerInterceptor接口的onSend方法的具体实现</span><br><span class="line">		最终将一个ProducerRecord转换成另一个ProducerRecord</span><br><span class="line">		然后再调用doSend发送</span><br><span class="line">3 Serializer序列化器</span><br><span class="line">	该方法里面会使用key序列化器和value序列化器进行序列化,必须传入,否则这里直接抛出异常了</span><br><span class="line">4 Partition分区器</span><br><span class="line">	执行分区器的方法partition,返回int类型分区编号</span><br><span class="line">		先判断是否在ProducerRecord中设置了分区编号,设置了直接使用,不会做任何正确性判断</span><br><span class="line">		再判断是否有自定义分区器</span><br><span class="line">		再判断key不为空而且不忽略key即参数partitioner.ignore,keys默认false,就根据key进行分区</span><br><span class="line">		再判断key为空,最终会根据分区负载情况粘性分区,即尽可能选择同一个分区,阈值batch.size=16kb</span><br><span class="line">5 RecordAccumulate数据缓冲区</span><br><span class="line">	大小 buffer.memory=32mb</span><br><span class="line">	max.request.size=1mb</span><br><span class="line">6 Sender线程</span><br><span class="line">	两个参数:batch.size=16kb 达到linger.ms默认0</span><br><span class="line">	sendProducerData发送数据</span><br><span class="line">	poll拉取发送后的结果</span><br><span class="line">7 NetworkClient</span><br><span class="line">	sender线程中客户端最终调用send方法发送数据,这个客户端的具体实现类就是NetworkClient</span><br><span class="line">	max.in.flight.requests.per.connection 默认5</span><br><span class="line">8 Selector</span><br><span class="line">	selector最终调用send方法发送数据</span><br><span class="line">	失败重试retries 默认int最大值</span><br><span class="line">	成功后清理数据缓冲区对应数据</span><br><span class="line">9 ACKS应答</span><br></pre></td></tr></table></figure>

<h5 id="consumer源码"><a href="#consumer源码" class="headerlink" title="consumer源码"></a>consumer源码</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">整体流程</span><br><span class="line">A 初始化流程,即创建KafkaConsumer对象</span><br><span class="line">1 创建客户端对象ConsumerNetworkClient</span><br><span class="line">	初始化相关参数</span><br><span class="line">		连接重试时间reconnect.backoff.ms 50ms</span><br><span class="line">		最大连接重试时间reconnect.backoff.max.ms 1s</span><br><span class="line">		发送缓存send.buffer 128kb</span><br><span class="line">		接收缓存receive.buffer 64kb</span><br><span class="line">		客户端请求服务器超时时间request.timeout.ms 30s</span><br><span class="line">2 消费者分区分配策略 三种</span><br><span class="line">	range</span><br><span class="line">	round robin</span><br><span class="line">	sticky</span><br><span class="line">3 创建消费者组coordinator</span><br><span class="line">    消费者和coordinator会保持3s心跳</span><br><span class="line">        这两个参数触发,会触发消费者再平衡</span><br><span class="line">        session.timeout.ms=45s 心跳超时</span><br><span class="line">        max.poll.interval.ms=5分钟 消费者处理时间超时</span><br><span class="line">	ConsumerCoordinator对象</span><br><span class="line">	自动提交offset时间auto.commit.interval.ms 默认5s</span><br><span class="line">4 创建Fetcher对象,抓取数据</span><br><span class="line">    fetch.min.bytes每批次最小抓取字节数默认1字节</span><br><span class="line">	fetch.max.wait.ms一批次的数据最小值都未达到的超时时间默认500ms</span><br><span class="line">	fetch.max.bytes最大50mb</span><br><span class="line">	max.partition.fetch.bytes 默认1mb即单条日志最大上限</span><br><span class="line">	max.poll.records默认500条</span><br><span class="line"></span><br><span class="line">B 订阅主题,即main线程调用subscribe方法</span><br><span class="line">1 注册监听器,实现负载均衡</span><br><span class="line">	registerRebalanceListener</span><br><span class="line">2 订阅主题</span><br><span class="line"></span><br><span class="line">拉取数据,即main线程调用到了poll方法</span><br><span class="line">1 消费者组选择coordinator,即消费者组初始化</span><br><span class="line">	updateAssignmentMetadataIfNeeded</span><br><span class="line">	组id的hashcode对50取模(__consumer_offsets的分区数量是50默认)</span><br><span class="line">2 调用pollForFetches抓取数据</span><br><span class="line">3 发送消费请求sendFetches该方法比较核心</span><br><span class="line">    fetch.min.bytes每批次最小抓取字节数默认1字节</span><br><span class="line">	fetch.max.wait.ms一批次的数据最小值都未达到的超时时间默认500ms</span><br><span class="line">	fetch.max.bytes最大50mb</span><br><span class="line">3 客户端发送调用send</span><br><span class="line">4 发送结果设置了监听器,成功的回调函数onSuccess</span><br><span class="line">	成功的数据batchs和offset都放在completedFetches队列中</span><br><span class="line">5 消费者调用fetchRecords再次拉取数据</span><br><span class="line">	每批次从completedFetches队列中拉取的</span><br><span class="line">	参数max.poll.records默认500条</span><br><span class="line">	调用completedFetches.poll真正拉取数据</span><br><span class="line">6 反序列化</span><br><span class="line">7 拦截器intercepts调用onConsume处理</span><br><span class="line">	拦截器链</span><br><span class="line">8 用户处理数据阶段</span><br><span class="line"></span><br><span class="line">C 提交offset,即main调用commitAsync</span><br><span class="line">1 协调器coordinator调用commitOffsetsAsync</span><br><span class="line">	(异步提交)</span><br><span class="line">	sendOffsetCommitRequest发送提交的请求</span><br><span class="line">	结果注册监听器addListener</span><br><span class="line">		onSuccess回调函数进行处理</span><br></pre></td></tr></table></figure>

<h5 id="broker源码"><a href="#broker源码" class="headerlink" title="broker源码"></a>broker源码</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">broker源码</span><br><span class="line">KafkaServer类</span><br><span class="line">	startup方法</span><br><span class="line">		执行initZkClient启动</span><br><span class="line">		执行logManager.startup启动	</span><br><span class="line">		执行clientToControllerChannelManager.start()启动 网络客户端,可以接收请求</span><br><span class="line">		new SocketServer,创建网络服务器,可以发送请求,默认9092</span><br><span class="line">		调用replicaManager.startup()启动 用来管理底层文件和对象映射</span><br><span class="line">		调用kafkaController.startup()启动</span><br><span class="line">		创建AutoTopicCreationManager,即自动创建topic如果不存在</span><br><span class="line">		new KafkaApis对象,将请求的不同类型转换成具体要执行的功能</span><br><span class="line">			</span><br></pre></td></tr></table></figure>



      
    </div>
    <footer class="article-footer">
      <a data-url="https://nfsp412.github.io/asuka.github.io/2024/10/24/bigdata005/" data-id="cm3ae7icy00090gurcbof6ht4" data-title="Kafka学习笔记" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/asuka.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag">大数据</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/asuka.github.io/2024/10/23/bigdata006/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">后一篇</strong>
      <div class="article-nav-title">HBase学习笔记</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/asuka.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag">大数据</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/asuka.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" style="font-size: 10px;">大数据</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/asuka.github.io/archives/2024/10/">十月 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/asuka.github.io/2024/10/24/bigdata005/">Kafka学习笔记</a>
          </li>
        
          <li>
            <a href="/asuka.github.io/2024/10/23/bigdata006/">HBase学习笔记</a>
          </li>
        
          <li>
            <a href="/asuka.github.io/2024/10/21/bigdata004/">Flink学习笔记</a>
          </li>
        
          <li>
            <a href="/asuka.github.io/2024/10/21/bigdata007/">Clickhouse学习笔记</a>
          </li>
        
          <li>
            <a href="/asuka.github.io/2024/10/16/bigdata003/">Spark学习笔记</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 asuka<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/asuka.github.io/" class="mobile-nav-link">Home</a>
  
    <a href="/asuka.github.io/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/asuka.github.io/js/jquery-3.6.4.min.js"></script>



  
<script src="/asuka.github.io/fancybox/jquery.fancybox.min.js"></script>




<script src="/asuka.github.io/js/script.js"></script>





  </div>
</body>
</html>