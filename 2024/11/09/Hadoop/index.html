<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hadoop笔记整理 | Asuka的个人笔记</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="面试题hadoop相关命令行命令1234hadoop fs 命令 或者 hdfs dfs 命令上传：put下载：get其他：ls，cat，mkdir，cp，mv，rm，du  hadoop jar jar包路径 输入路径 输出路径。用来提交mr程序job hdfs oiv查看fsimage hdfs oev查看edits 设置副本数量：hadoop fs -setrep 3 filename 也能">
<meta property="og:type" content="article">
<meta property="og:title" content="Hadoop笔记整理">
<meta property="og:url" content="https://nfsp412.github.io/asuka.github.io/2024/11/09/Hadoop/index.html">
<meta property="og:site_name" content="Asuka的个人笔记">
<meta property="og:description" content="面试题hadoop相关命令行命令1234hadoop fs 命令 或者 hdfs dfs 命令上传：put下载：get其他：ls，cat，mkdir，cp，mv，rm，du  hadoop jar jar包路径 输入路径 输出路径。用来提交mr程序job hdfs oiv查看fsimage hdfs oev查看edits 设置副本数量：hadoop fs -setrep 3 filename 也能">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-11-09T14:30:48.000Z">
<meta property="article:modified_time" content="2024-11-09T14:42:56.106Z">
<meta property="article:author" content="asuka">
<meta property="article:tag" content="大数据">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/asuka.github.io/atom.xml" title="Asuka的个人笔记" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/asuka.github.io/favicon.png">
  
  
  
<link rel="stylesheet" href="/asuka.github.io/css/style.css">

  
    
<link rel="stylesheet" href="/asuka.github.io/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/asuka.github.io/" id="logo">Asuka的个人笔记</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/asuka.github.io/">Home</a>
        
          <a class="main-nav-link" href="/asuka.github.io/archives">Archives</a>
        
          <a class="main-nav-link" href="/asuka.github.io/%20%7C%7C%20fas%20fas%20fa-home">首页</a>
        
          <a class="main-nav-link" href="/asuka.github.io/categories/%20%7C%7C%20fas%20fa-folder-open">分类</a>
        
          <a class="main-nav-link" href="/asuka.github.io/archives/%20%7C%7C%20fas%20fa-archive">时间轴</a>
        
          <a class="main-nav-link" href="/asuka.github.io/tags/%20%7C%7C%20fas%20fa-tags">标签</a>
        
          <a class="main-nav-link" href="/asuka.github.io/link/%20%7C%7C%20fas%20fa-link">友情链接</a>
        
          <a class="main-nav-link" href="/asuka.github.io/about/%20%7C%7C%20fas%20fa-heart">关于我</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/asuka.github.io/atom.xml" title="RSS 订阅"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="搜索"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://nfsp412.github.io/asuka.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-Hadoop" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/asuka.github.io/2024/11/09/Hadoop/" class="article-date">
  <time class="dt-published" datetime="2024-11-09T14:30:48.000Z" itemprop="datePublished">2024-11-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Hadoop笔记整理
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h1><h5 id="hadoop相关命令行命令"><a href="#hadoop相关命令行命令" class="headerlink" title="hadoop相关命令行命令"></a>hadoop相关命令行命令</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs 命令 或者 hdfs dfs 命令</span><br><span class="line">上传：put</span><br><span class="line">下载：get</span><br><span class="line">其他：ls，cat，mkdir，cp，mv，rm，du</span><br></pre></td></tr></table></figure>

<p>hadoop jar jar包路径 输入路径 输出路径。用来提交mr程序job</p>
<p>hdfs oiv查看fsimage</p>
<p>hdfs oev查看edits</p>
<p>设置副本数量：hadoop fs -setrep 3 filename</p>
<p>也能够通过API操作，使用hadoop-client包，通过FileSystem调用get方法传入username，URI地址，conf配置三个参数，获取对象，通过该对象调用相关方法，最后调用close方法关闭资源，具体略</p>
<p>yarn application -list列出所有任务</p>
<p>yarn application -list -appStates根据具体状态列出任务</p>
<p>yarn application -kill appID，杀死任务</p>
<p>yarn logs -applicationId ApplicationId，查询任务的日志，没有页面的时候很常用</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn logs -applicationId xxx | less</span><br></pre></td></tr></table></figure>

<p>yarn node -list -all列出所有的NM节点</p>
<p>yarn queue -status QueueName查询队列情况</p>
<p>hdfs diskbalancer -plan hadoop103生成均衡计划，新加入的磁盘</p>
<p>hdfs diskbalancer -execute hadoop103.plan.json执行均衡计划，新加入的磁盘</p>
<p>hdfs dfsadmin -refreshNodes刷新NN</p>
<p>start-balancer.sh -threshold 10开启数据均衡，新加入的服务器节点</p>
<p>hdfs haadmin -transitionToStandby nn2名字 –forcemanual 强制转换成standby节点,另一个变成了active节点</p>
<h5 id="参数优先级"><a href="#参数优先级" class="headerlink" title="参数优先级?"></a>参数优先级?</h5><p>参数优先级排序：（1）客户端代码中设置的值（configuration.set(“key”,”value”)） &gt;（2）ClassPath下的用户自定义配置文件（resources目录下配置文件） &gt;（3）然后是服务器的自定义配置（xxx-site.xml） &gt;（4）服务器的默认配置（xxx-default.xml）</p>
<h5 id="如何保证数据完整性"><a href="#如何保证数据完整性" class="headerlink" title="如何保证数据完整性?"></a>如何保证数据完整性?</h5><p>DN读取数据块时，会计算checksum校验和，然后和创建时的校验和对比，相同则认为是数据完整的</p>
<p>读写数据时也是按照512字节的chunk加上4字节的checksum校验和，凑够64kb以packet形式发送</p>
<p>校验算法：crc，md5，sha1等</p>
<h5 id="Fsimage文件中没有记录数据块所对应DataNode服务器信息，如何定位-为什么？"><a href="#Fsimage文件中没有记录数据块所对应DataNode服务器信息，如何定位-为什么？" class="headerlink" title="Fsimage文件中没有记录数据块所对应DataNode服务器信息，如何定位?为什么？"></a>Fsimage文件中没有记录数据块所对应DataNode服务器信息，如何定位?为什么？</h5><p>因为在集群启动后，要求DataNode上报数据块信息，并间隔一段时间后再次上报,所以没有记录</p>
<h5 id="NameNode如何确定下次开机启动的时候合并哪些Edits？"><a href="#NameNode如何确定下次开机启动的时候合并哪些Edits？" class="headerlink" title="NameNode如何确定下次开机启动的时候合并哪些Edits？"></a>NameNode如何确定下次开机启动的时候合并哪些Edits？</h5><p>fsimage文件选择最新序号,加上edits文件从该序号之后的文件即为需要合并的文件</p>
<p>但是注意的是,namenode中还存在inprogress最新操作文件,这个是snn中没有的,丢失是可能发生的</p>
<h5 id="maptask并行度和什么有关系"><a href="#maptask并行度和什么有关系" class="headerlink" title="maptask并行度和什么有关系?"></a>maptask并行度和什么有关系?</h5><p>数据切片是MapReduce程序计算输入数据的单位，一个切片会对应启动一个MapTask</p>
<p>默认切片大小为block块大小,即默认128MB,,注意IDEA调试的时候默认是32MB块</p>
<p>是针对每一个文件进行切片计算</p>
<p>注意判断条件是128MB的1.1倍,大于就切片128MB,小于就不切了,一整个作为一片</p>
<h5 id="分区-分文件-数和reducetask个数的关系"><a href="#分区-分文件-数和reducetask个数的关系" class="headerlink" title="分区(分文件)数和reducetask个数的关系"></a>分区(分文件)数和reducetask个数的关系</h5><p>如果reducetask个数(默认1)大于分区数,那么多产生空part-r-000xx文件</p>
<p>如果大于1小于分区数,会抛出异常,因为有一些数据没地方去</p>
<p>如果为1,那么最终只有一个part-r-00000</p>
<p>分区编号从0开始,在自定义分区器时需要注意</p>
<h5 id="reduce方法调用次数"><a href="#reduce方法调用次数" class="headerlink" title="reduce方法调用次数?"></a>reduce方法调用次数?</h5><p>1 和分组key数量有关,注意如果是bean类型,且实现排序接口,那么会按照排序属性字段分组</p>
<p>注意区分reduce方法调用次数和reducetask个数的区别</p>
<h5 id="reducetask和maptask个数"><a href="#reducetask和maptask个数" class="headerlink" title="reducetask和maptask个数?"></a>reducetask和maptask个数?</h5><p>1 reducetask个数是手动指定的,默认1</p>
<p>2 maptask个数看切片个数</p>
<h5 id="如果数据不均衡（某个节点数据少，其他节点数据多），怎么处理？"><a href="#如果数据不均衡（某个节点数据少，其他节点数据多），怎么处理？" class="headerlink" title="如果数据不均衡（某个节点数据少，其他节点数据多），怎么处理？"></a>如果数据不均衡（某个节点数据少，其他节点数据多），怎么处理？</h5><p>1 节点间数据均衡,使用sbin&#x2F;start-balancer.sh -threshold 10命令,10代表差异不超过10%</p>
<p>停止命令sbin&#x2F;stop-balancer.sh</p>
<p>尽量不在nn节点进行</p>
<h5 id="小文件怎么处理"><a href="#小文件怎么处理" class="headerlink" title="小文件怎么处理?"></a>小文件怎么处理?</h5><p>1 使用CombineTextInputFormat代替TextInputFormat类,去读取文件,设置合理的切片值,默认是4MB</p>
<p>2 使用hadoop archive -archiveName input.har -p  &#x2F;small   &#x2F;output_small,实际上是一个mr任务,进行合并小文件,NN认为是一个文件,实际上还是多个小文件</p>
<p>3 使用jvm重用,即开启uber模式参数</p>
<h5 id="mr优化的参数有哪些"><a href="#mr优化的参数有哪些" class="headerlink" title="mr优化的参数有哪些?"></a>mr优化的参数有哪些?</h5><p>1 解决数据倾斜,使用自定义的分区类</p>
<p>2 相关参数的设置,例如环形缓冲区增大,溢出阈值,merge合并次数增加,reduce拉取的并行度增加,maptask和reducetask的cpu和内存的适当增加</p>
<p>3 三个阶段都考虑使用压缩</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">增加环形缓冲区大小从100mb到更多</span><br><span class="line">mapreduce.task.io.sort.mb = 200</span><br><span class="line"></span><br><span class="line">增大阈值从80到更多</span><br><span class="line">mapreduce.map.sort.spill.percent = 90</span><br><span class="line"></span><br><span class="line">增加Merge次数从10到更多</span><br><span class="line">mapreduce.task.io.sort.factor = 20</span><br><span class="line"></span><br><span class="line">内存,cpu</span><br><span class="line">mapreduce.map.memory.mb</span><br><span class="line">mapreduce.map.cpu.vcores</span><br><span class="line">mapreduce.reduce.memory.mb</span><br><span class="line">mapreduce.reduce.cpu.vcores</span><br><span class="line"></span><br><span class="line">异常重试默认4次到更多</span><br><span class="line">mapreduce.map.maxattempts = 4</span><br><span class="line"></span><br><span class="line">reduce主动拉取的并行度,默认5到更多</span><br><span class="line">mapreduce.reduce.shuffle.Parallelcopies = 10</span><br><span class="line"></span><br><span class="line">拉取后先内存后磁盘,buffer大小,默认占比0.7到更多</span><br><span class="line">mapreduce.reduce.shuffle.input.buffer.percent = 0.8</span><br><span class="line"></span><br><span class="line">buffer数据占比开始溢出默认0.66到更多</span><br><span class="line">mapreduce.reduce.shuffle.merge.percent = 0.75</span><br><span class="line"></span><br><span class="line">reduce重试次数默认4次到更多</span><br><span class="line">mapreduce.reduce.maxattempts = 4</span><br></pre></td></tr></table></figure>

<h5 id="mr程序的数据倾斜怎么处理"><a href="#mr程序的数据倾斜怎么处理" class="headerlink" title="mr程序的数据倾斜怎么处理?"></a>mr程序的数据倾斜怎么处理?</h5><p>详见hive数据倾斜</p>
<h5 id="namenode启动源码有哪些部分"><a href="#namenode启动源码有哪些部分" class="headerlink" title="namenode启动源码有哪些部分?"></a>namenode启动源码有哪些部分?</h5><p>NameNode类的main方法</p>
<p>1 启动9870端口服务,使用HttpServer2类</p>
<p>2 加载fsimage和edits log文件</p>
<p>3 初始化nn的rpc服务端,使用NameNodeRpcServer类</p>
<p>4 nn启动时的资源检查,心跳超时判断,超时10min+3s时间,安全模式检查,阈值是0.999,即1000块中只能有一个块失败</p>
<h5 id="datanode启动源码有哪些部分"><a href="#datanode启动源码有哪些部分" class="headerlink" title="datanode启动源码有哪些部分?"></a>datanode启动源码有哪些部分?</h5><p>DataNode类的main方法</p>
<p>1 初始化DataXceiverServer,这个对象和hdfs的上传有关</p>
<p>2 初始化http服务,使用HttpServer2类</p>
<p>3 初始化dn的rpc服务器,即创建了一个rpc客户端</p>
<p>4 dn向nn注册,发送心跳	</p>
<h5 id="hdfs上传源码有哪些"><a href="#hdfs上传源码有哪些" class="headerlink" title="hdfs上传源码有哪些?"></a>hdfs上传源码有哪些?</h5><p>1 创建过程源码,以create方法为例,实际上调用的是NameNodeRpcServer的create方法</p>
<p>该方法创建DFSOutputStream输出流对象,相关参数是block 128mb,packet 64kb,chunk 512bytes,checksum 4bytes</p>
<p>该对象调用start方法,实际上是DataStreamer线程的start方法</p>
<p>创建了dataQueue队列和ackQueue队列</p>
<p>2 写入过程源码,以write方法为例,实际上最终也是调用到了 DFSOutputStream输出流对象的writeChunk方法,</p>
<p>先按照512+4bytes写入packet,等packet64kb满了以后,向dataQueue队列添加,唤醒DataStreamer线程</p>
<p>DataStreamer线程会调用setPipeline创建管道,机架感知,获取存储位置信息</p>
<p>创建Sender线程调用send方法使用socket方式发送packet,发送一个packet就从dataQueue队列移除,然后添加到ackQueue队列,</p>
<p>同时启动了ResponseProcessor线程用来接收ack响应,存储成功后再从ackQueue队列移除该packet,不成功就把这个packet再次添加到dataQueue队列</p>
<h5 id="yarn提交任务源码有哪些"><a href="#yarn提交任务源码有哪些" class="headerlink" title="yarn提交任务源码有哪些?"></a>yarn提交任务源码有哪些?</h5><p>1 在自定义的mr程序中,Driver驱动类中,job执行waitForCompletion方法的过程为例</p>
<p>2 客户端向rm提交job,调用到submit方法,最终获取jobid,指定提交路径,即.staging路径,然后进入切片环节,即默认TextFileInputFormat类,然后生成job.split切片信息和job.xml配置信息还有jar包,准备工作做好进入真正提交任务流程</p>
<p>3 rm启动am过程,上面提交完任务后,会拼接一个shell命令,rm执行这个命令就启动了am,在yarn环境下,对应的是MRAppMaster对象</p>
<p>4 启动MRAppMaster类,执行main方法,会创建一个dispatcher调度器,然后把任务job放在yarn的queue队列中</p>
<p>5 YarnChild类是队列中任务执行的类,执行该类main方法,创建Task对象后执行run方法,该Task对象有MapTask和ReduceTask两个实现类</p>
<p>6 MapTask分为map阶段和sort阶段,最终调用到了我们自定义程序的Mapper的map方法</p>
<p>7 ReduceTask分为copy阶段,sort阶段,reduce阶段,同样是最终调用到了我们自定义程序的Reducer的reduce方法</p>
<h5 id="Shuffle工作机制是什么"><a href="#Shuffle工作机制是什么" class="headerlink" title="Shuffle工作机制是什么?"></a>Shuffle工作机制是什么?</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1 map方法对于数据进行一行一行的处理，得到的结果是kv对，调用collect方法，放入环形缓冲区</span><br><span class="line">2 放入环形缓冲区之前会打上分区标记，调用getPartitioner方法</span><br><span class="line">3 到达一定比例后开始溢写到磁盘，在溢写过程中会进行快排，即sortAndSpill</span><br><span class="line">4 这个环节可以预先进行map端聚合，即归约环节，减少进入reduce阶段的数据量；也可以进行压缩</span><br><span class="line">5 多个溢写的文件会进行多轮递归的归并排序，每一轮合并10个文件。最终形成一个file.out数据文件和file.out.index索引文件，即mergeParts方法</span><br><span class="line"></span><br><span class="line">6 reduce阶段的reducetask主动去拉取自己分区的数据，即copy环节</span><br><span class="line">7 由于可能来自于多个maptask文件，所以会优先内存其次磁盘的进行归并排序，sort环节</span><br><span class="line">8 合并以后shuffle过程就结束了，会针对每一个key的所有的数据执行reduce方法</span><br></pre></td></tr></table></figure>

<h5 id="几次排序"><a href="#几次排序" class="headerlink" title="几次排序?"></a>几次排序?</h5><p>maptask和reducetask都会对key进行排序，是默认行为，所以key必须实现了排序</p>
<p>默认是字典序，快排</p>
<p>maptask两次排序；快排时先按照分区编号排序，再按照编号内部的key排序，</p>
<p>reducetask一次排序</p>
<h5 id="调度策略对比"><a href="#调度策略对比" class="headerlink" title="调度策略对比?"></a>调度策略对比?</h5><p>CDH默认公平调度器</p>
<p>Apache默认容量调度器</p>
<p>容量调度器和公平调度器的对比：</p>
<p>相同点：都支持多队列，每个队列内部使用FIFO；每个队列可以设置资源的下限和上限；某个队列资源有剩余，可以暂时共享给其他队列；支持多用户和多程序运行</p>
<p>不同点：对于队列资源的分配方式不同，容量调度器优先选择资源利用率低的队列，而公平调度器优先选择对资源的缺额比例大的队列；容量调度器队列内部的资源分配使用FIFO，DRF，而公平调度器额外还能使用FAIR</p>
<h1 id="常用包"><a href="#常用包" class="headerlink" title="常用包"></a>常用包</h1><h5 id="commons-beanutils"><a href="#commons-beanutils" class="headerlink" title="commons-beanutils"></a>commons-beanutils</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//复制bean类</span></span><br><span class="line">BeanUtils.copyProperties(tmpOrderBean, value);</span><br></pre></td></tr></table></figure>

<h5 id="commons-lang"><a href="#commons-lang" class="headerlink" title="commons-lang"></a>commons-lang</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//判空</span></span><br><span class="line">StringUtils.isNotEmpty(line = reader.readLine());</span><br></pre></td></tr></table></figure>






      
    </div>
    <footer class="article-footer">
      <a data-url="https://nfsp412.github.io/asuka.github.io/2024/11/09/Hadoop/" data-id="cm3aa09no00010surgdctegf5" data-title="Hadoop笔记整理" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/asuka.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag">大数据</a></li></ul>

    </footer>
  </div>
  
    
  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/asuka.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag">大数据</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/asuka.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" style="font-size: 10px;">大数据</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/asuka.github.io/archives/2024/11/">十一月 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/asuka.github.io/2024/11/09/Hadoop/">Hadoop笔记整理</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 asuka<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/asuka.github.io/" class="mobile-nav-link">Home</a>
  
    <a href="/asuka.github.io/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/asuka.github.io/%20%7C%7C%20fas%20fas%20fa-home" class="mobile-nav-link">首页</a>
  
    <a href="/asuka.github.io/categories/%20%7C%7C%20fas%20fa-folder-open" class="mobile-nav-link">分类</a>
  
    <a href="/asuka.github.io/archives/%20%7C%7C%20fas%20fa-archive" class="mobile-nav-link">时间轴</a>
  
    <a href="/asuka.github.io/tags/%20%7C%7C%20fas%20fa-tags" class="mobile-nav-link">标签</a>
  
    <a href="/asuka.github.io/link/%20%7C%7C%20fas%20fa-link" class="mobile-nav-link">友情链接</a>
  
    <a href="/asuka.github.io/about/%20%7C%7C%20fas%20fa-heart" class="mobile-nav-link">关于我</a>
  
</nav>
    


<script src="/asuka.github.io/js/jquery-3.6.4.min.js"></script>



  
<script src="/asuka.github.io/fancybox/jquery.fancybox.min.js"></script>




<script src="/asuka.github.io/js/script.js"></script>





  </div>
</body>
</html>